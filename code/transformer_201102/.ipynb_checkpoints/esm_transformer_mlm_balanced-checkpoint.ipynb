{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import esm\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "from esm.constants import proteinseq_toks\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from esm.modules import TransformerLayer, PositionalEmbedding  # noqa\n",
    "from esm.model import ProteinBertModel\n",
    "\n",
    "# model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm1_t34_670M_UR50S\")\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ych_util import prepare_mlm_mask\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>seq</th>\n",
       "      <th>pfamA_acc</th>\n",
       "      <th>clan_x</th>\n",
       "      <th>pfamA_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>318809</td>\n",
       "      <td>A0A1V0A8E6_9ACTN/194-355</td>\n",
       "      <td>A0A1V0A8E6_9ACTN/194-355 A0A1V0A8E6.1 PF13304....</td>\n",
       "      <td>TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALAL...</td>\n",
       "      <td>PF13304</td>\n",
       "      <td>p_loop_gtpase</td>\n",
       "      <td>AAA_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>1483573</td>\n",
       "      <td>A0A1G7KN25_9PROT/48-166</td>\n",
       "      <td>A0A1G7KN25_9PROT/48-166 A0A1G7KN25.1 PF01926.2...</td>\n",
       "      <td>EVAFVGRSNVGKSSLVNALTGRKTLARTSNTPGRTQEVIFFDLGGR...</td>\n",
       "      <td>PF01926</td>\n",
       "      <td>p_loop_gtpase</td>\n",
       "      <td>MMR_HSR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>368351</td>\n",
       "      <td>R5DZV7_9FIRM/322-526</td>\n",
       "      <td>R5DZV7_9FIRM/322-526 R5DZV7.1 PF13604.7;AAA_30;</td>\n",
       "      <td>ELDEIQKEAVKKTVQNGLVVITGGPGTGKTTTINTIIRYFQMEGLD...</td>\n",
       "      <td>PF13604</td>\n",
       "      <td>p_loop_gtpase</td>\n",
       "      <td>AAA_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>683920</td>\n",
       "      <td>A0A2B8ATP4_9ACTN/40-200</td>\n",
       "      <td>A0A2B8ATP4_9ACTN/40-200 A0A2B8ATP4.1 PF00005.2...</td>\n",
       "      <td>VNGVDYSVDAGETLAVLGESGSGKSVTAQAVMGILDMPPGRIPHGE...</td>\n",
       "      <td>PF00005</td>\n",
       "      <td>p_loop_gtpase</td>\n",
       "      <td>ABC_tran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14189</th>\n",
       "      <td>187106</td>\n",
       "      <td>A0A0G4IPH7_PLABS/263-393</td>\n",
       "      <td>A0A0G4IPH7_PLABS/263-393 A0A0G4IPH7.1 PF03953....</td>\n",
       "      <td>PRIHFPLCALAPVISAEIAYHEQLSVAEITNSVFEPANQMVKCDPR...</td>\n",
       "      <td>PF03953</td>\n",
       "      <td>tubulin_c</td>\n",
       "      <td>Tubulin_C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                        id  \\\n",
       "8049       318809  A0A1V0A8E6_9ACTN/194-355   \n",
       "7421      1483573   A0A1G7KN25_9PROT/48-166   \n",
       "7270       368351      R5DZV7_9FIRM/322-526   \n",
       "6993       683920   A0A2B8ATP4_9ACTN/40-200   \n",
       "14189      187106  A0A0G4IPH7_PLABS/263-393   \n",
       "\n",
       "                                             description  \\\n",
       "8049   A0A1V0A8E6_9ACTN/194-355 A0A1V0A8E6.1 PF13304....   \n",
       "7421   A0A1G7KN25_9PROT/48-166 A0A1G7KN25.1 PF01926.2...   \n",
       "7270     R5DZV7_9FIRM/322-526 R5DZV7.1 PF13604.7;AAA_30;   \n",
       "6993   A0A2B8ATP4_9ACTN/40-200 A0A2B8ATP4.1 PF00005.2...   \n",
       "14189  A0A0G4IPH7_PLABS/263-393 A0A0G4IPH7.1 PF03953....   \n",
       "\n",
       "                                                     seq pfamA_acc  \\\n",
       "8049   TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALAL...   PF13304   \n",
       "7421   EVAFVGRSNVGKSSLVNALTGRKTLARTSNTPGRTQEVIFFDLGGR...   PF01926   \n",
       "7270   ELDEIQKEAVKKTVQNGLVVITGGPGTGKTTTINTIIRYFQMEGLD...   PF13604   \n",
       "6993   VNGVDYSVDAGETLAVLGESGSGKSVTAQAVMGILDMPPGRIPHGE...   PF00005   \n",
       "14189  PRIHFPLCALAPVISAEIAYHEQLSVAEITNSVFEPANQMVKCDPR...   PF03953   \n",
       "\n",
       "              clan_x pfamA_name  \n",
       "8049   p_loop_gtpase     AAA_21  \n",
       "7421   p_loop_gtpase   MMR_HSR1  \n",
       "7270   p_loop_gtpase     AAA_30  \n",
       "6993   p_loop_gtpase   ABC_tran  \n",
       "14189      tubulin_c  Tubulin_C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfamA_balanced = pd.read_csv(\"../../data/esm/pfamA_motors_balanced.csv\")\n",
    "pfamA_balanced = pfamA_balanced.sample(frac = 1)\n",
    "pfamA_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = esm.Alphabet.from_dict(proteinseq_toks)\n",
    "# model_name = \"esm1_t34_670M_UR50S\"\n",
    "model_name = \"esm1_t12_85M_UR50S\"\n",
    "url = f\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"cuda\")\n",
    "    model_data = torch.hub.load_state_dict_from_url(url, progress=False)\n",
    "else:\n",
    "    model_data = torch.hub.load_state_dict_from_url(url, progress=False, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pra = lambda s: ''.join(s.split('decoder_')[1:] if 'decoder' in s else s)\n",
    "prs = lambda s: ''.join(s.split('decoder.')[1:] if 'decoder' in s else s)\n",
    "model_args = {pra(arg[0]): arg[1] for arg in vars(model_data[\"args\"]).items()}\n",
    "model_state = {prs(arg[0]): arg[1] for arg in model_data[\"model\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = esm.ProteinBertModel(\n",
    "        Namespace(**model_args), len(alphabet), padding_idx=alphabet.padding_idx\n",
    "    )\n",
    "\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"../../data/esm1_t12_85M_UR50S_balanced_201102.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinBertModel(\n",
       "  (embed_tokens): Embedding(35, 768, padding_idx=1)\n",
       "  (embed_positions): PositionalEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0001 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A1V0A8E6_9ACTN/194-355']\n",
      "['TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALALLRLADLGIDDVLIDQCEVAHSDGPRTQRRIRLVHQTAHEKAPLDFAAESAGTRTWFHLIGPVLAALKAGSLLLFDELDASLHPTLCVQLLRLFQDPAMNPKGAQLVFTSHDTSLLN']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 0.0\n",
      "Loss 2.1437\n",
      "time elapsed 0.2020\n",
      "['A0A2N5Y3G4_9GAMM/13-174']\n",
      "['VIKVIGVGGGGGNAVKHMIENAVEGVDFICANTDAQALSDISSKTVLQLGGDITKGLGAGANPEIGRAAALEDRERIADALRGADMVFITAGMGGGTGTGGAPVVAEVAREMGILTVAVVTRPFAFEGKKRLAIAQEGVRELQQHVDSLITIPNEKLLEVLG']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 10000.0\n",
      "Loss 1.3429\n",
      "time elapsed 2207.5310\n",
      "['A0A1V0A8E6_9ACTN/194-355']\n",
      "['TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALALLRLADLGIDDVLIDQCEVAHSDGPRTQRRIRLVHQTAHEKAPLDFAAESAGTRTWFHLIGPVLAALKAGSLLLFDELDASLHPTLCVQLLRLFQDPAMNPKGAQLVFTSHDTSLLN']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 0.0\n",
      "Loss 2.5217\n",
      "time elapsed 4261.9109\n",
      "['A0A2N5Y3G4_9GAMM/13-174']\n",
      "['VIKVIGVGGGGGNAVKHMIENAVEGVDFICANTDAQALSDISSKTVLQLGGDITKGLGAGANPEIGRAAALEDRERIADALRGADMVFITAGMGGGTGTGGAPVVAEVAREMGILTVAVVTRPFAFEGKKRLAIAQEGVRELQQHVDSLITIPNEKLLEVLG']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 10000.0\n",
      "Loss 0.9721\n",
      "time elapsed 6814.2369\n",
      "['A0A1V0A8E6_9ACTN/194-355']\n",
      "['TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALALLRLADLGIDDVLIDQCEVAHSDGPRTQRRIRLVHQTAHEKAPLDFAAESAGTRTWFHLIGPVLAALKAGSLLLFDELDASLHPTLCVQLLRLFQDPAMNPKGAQLVFTSHDTSLLN']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 0.0\n",
      "Loss 2.6199\n",
      "time elapsed 8863.0887\n",
      "['A0A2N5Y3G4_9GAMM/13-174']\n",
      "['VIKVIGVGGGGGNAVKHMIENAVEGVDFICANTDAQALSDISSKTVLQLGGDITKGLGAGANPEIGRAAALEDRERIADALRGADMVFITAGMGGGTGTGGAPVVAEVAREMGILTVAVVTRPFAFEGKKRLAIAQEGVRELQQHVDSLITIPNEKLLEVLG']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 10000.0\n",
      "Loss 1.1343\n",
      "time elapsed 11410.7000\n",
      "['A0A1V0A8E6_9ACTN/194-355']\n",
      "['TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALALLRLADLGIDDVLIDQCEVAHSDGPRTQRRIRLVHQTAHEKAPLDFAAESAGTRTWFHLIGPVLAALKAGSLLLFDELDASLHPTLCVQLLRLFQDPAMNPKGAQLVFTSHDTSLLN']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 0.0\n",
      "Loss 2.8044\n",
      "time elapsed 13242.9931\n",
      "['A0A2N5Y3G4_9GAMM/13-174']\n",
      "['VIKVIGVGGGGGNAVKHMIENAVEGVDFICANTDAQALSDISSKTVLQLGGDITKGLGAGANPEIGRAAALEDRERIADALRGADMVFITAGMGGGTGTGGAPVVAEVAREMGILTVAVVTRPFAFEGKKRLAIAQEGVRELQQHVDSLITIPNEKLLEVLG']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 10000.0\n",
      "Loss 0.6637\n",
      "time elapsed 15054.1381\n",
      "['A0A1V0A8E6_9ACTN/194-355']\n",
      "['TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALALLRLADLGIDDVLIDQCEVAHSDGPRTQRRIRLVHQTAHEKAPLDFAAESAGTRTWFHLIGPVLAALKAGSLLLFDELDASLHPTLCVQLLRLFQDPAMNPKGAQLVFTSHDTSLLN']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 0.0\n",
      "Loss 2.6110\n",
      "time elapsed 16505.1196\n",
      "['A0A2N5Y3G4_9GAMM/13-174']\n",
      "['VIKVIGVGGGGGNAVKHMIENAVEGVDFICANTDAQALSDISSKTVLQLGGDITKGLGAGANPEIGRAAALEDRERIADALRGADMVFITAGMGGGTGTGGAPVVAEVAREMGILTVAVVTRPFAFEGKKRLAIAQEGVRELQQHVDSLITIPNEKLLEVLG']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 10000.0\n",
      "Loss 0.9589\n",
      "time elapsed 18313.3232\n",
      "['A0A1V0A8E6_9ACTN/194-355']\n",
      "['TPHPHHRTACSTMRWFERGPGGDQLSLSDLEEDRSFEADRAQALALLRLADLGIDDVLIDQCEVAHSDGPRTQRRIRLVHQTAHEKAPLDFAAESAGTRTWFHLIGPVLAALKAGSLLLFDELDASLHPTLCVQLLRLFQDPAMNPKGAQLVFTSHDTSLLN']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 0.0\n",
      "Loss 2.6960\n",
      "time elapsed 19672.2145\n",
      "['A0A2N5Y3G4_9GAMM/13-174']\n",
      "['VIKVIGVGGGGGNAVKHMIENAVEGVDFICANTDAQALSDISSKTVLQLGGDITKGLGAGANPEIGRAAALEDRERIADALRGADMVFITAGMGGGTGTGGAPVVAEVAREMGILTVAVVTRPFAFEGKKRLAIAQEGVRELQQHVDSLITIPNEKLLEVLG']\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 163, 35])\n",
      "torch.Size([24, 35])\n",
      "torch.Size([24])\n",
      "At Epoch: 10000.0\n",
      "Loss 0.9120\n",
      "time elapsed 21116.4287\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 10000\n",
    "for j in range(10):\n",
    "    for i in range(pfamA_balanced.shape[0]):\n",
    "        if len(pfamA_balanced.iloc[i,3])>1024:\n",
    "            continue\n",
    "        data = [(pfamA_balanced.iloc[i,1], pfamA_balanced.iloc[i,3])]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        true_aa,target_ind,masked_batch_tokens = prepare_mlm_mask(alphabet,batch_tokens)\n",
    "        optimizer.zero_grad()\n",
    "        results = model(masked_batch_tokens.to('cuda'), repr_layers=[34])   \n",
    "        pred = results[\"logits\"].squeeze(0)[target_ind,:]   \n",
    "        target = true_aa.squeeze(0)\n",
    "        loss = criterion(pred.cpu(),target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(batch_labels)\n",
    "            print(batch_strs)\n",
    "            print(batch_tokens.size())\n",
    "            print(masked_batch_tokens.size())\n",
    "            print(results[\"logits\"].size())\n",
    "            print(pred.size())\n",
    "            print(target.size())\n",
    "            print(f\"At Epoch: %.1f\"% i)\n",
    "            print(f\"Loss %.4f\"% loss)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"time elapsed %.4f\"% elapsed)\n",
    "            torch.save(model.state_dict(), \"../../data/esm1_t12_85M_UR50S_balanced_201102.pt\")\n",
    "    #     loss_vector.append(loss)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/esm1_t12_85M_UR50S_balanced_201102.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
