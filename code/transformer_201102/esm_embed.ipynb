{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cyu7/.cache/torch/hub/facebookresearch_esm_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm1_t34_670M_UR50S\")\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>seq</th>\n",
       "      <th>pfamA_acc</th>\n",
       "      <th>clan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167044</td>\n",
       "      <td>U5C9Z2_9BACT/7-195</td>\n",
       "      <td>U5C9Z2_9BACT/7-195 U5C9Z2.1 PF00814.26;TsaD;</td>\n",
       "      <td>HQDNVHARSLMGLVRNVFEQAGLEKTALDAVAVSSGPGSYTGLRIG...</td>\n",
       "      <td>PF00814</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125588</td>\n",
       "      <td>A0A2C9ZZA3_9GAMM/2-187</td>\n",
       "      <td>A0A2C9ZZA3_9GAMM/2-187 A0A2C9ZZA3.1 PF03309.15...</td>\n",
       "      <td>ILLIDAGNTAIKLAVSDEHNQPTLIKQVQLCWSKISLVLYSCVKHS...</td>\n",
       "      <td>PF03309</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161982</td>\n",
       "      <td>W6LR88_9GAMM/14-234</td>\n",
       "      <td>W6LR88_9GAMM/14-234 W6LR88.1 PF05134.14;T2SSL;</td>\n",
       "      <td>DQAEWLLADGTVQHGTLTELVPHVIGARLLLIAPGERLTLHRVPLP...</td>\n",
       "      <td>PF05134</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22085</td>\n",
       "      <td>A0A2H3ZDD2_PHODC/25-338</td>\n",
       "      <td>A0A2H3ZDD2_PHODC/25-338 A0A2H3ZDD2.1 PF01869.2...</td>\n",
       "      <td>LGLDGGTTSTVCVCLPAAMPLASALPDPLPILSRAVAGCSNHNSVG...</td>\n",
       "      <td>PF01869</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10493</td>\n",
       "      <td>Q75BT7_ASHGO/309-399</td>\n",
       "      <td>Q75BT7_ASHGO/309-399 Q75BT7.1 PF00022.20;Actin;</td>\n",
       "      <td>AGLPSATPEQIHAALLTNVVIIGGTSLLQGLEQRLVNDLSLQFPQY...</td>\n",
       "      <td>PF00022</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       id  \\\n",
       "0      167044       U5C9Z2_9BACT/7-195   \n",
       "1      125588   A0A2C9ZZA3_9GAMM/2-187   \n",
       "2      161982      W6LR88_9GAMM/14-234   \n",
       "3       22085  A0A2H3ZDD2_PHODC/25-338   \n",
       "4       10493     Q75BT7_ASHGO/309-399   \n",
       "\n",
       "                                         description  \\\n",
       "0       U5C9Z2_9BACT/7-195 U5C9Z2.1 PF00814.26;TsaD;   \n",
       "1  A0A2C9ZZA3_9GAMM/2-187 A0A2C9ZZA3.1 PF03309.15...   \n",
       "2     W6LR88_9GAMM/14-234 W6LR88.1 PF05134.14;T2SSL;   \n",
       "3  A0A2H3ZDD2_PHODC/25-338 A0A2H3ZDD2.1 PF01869.2...   \n",
       "4    Q75BT7_ASHGO/309-399 Q75BT7.1 PF00022.20;Actin;   \n",
       "\n",
       "                                                 seq pfamA_acc        clan  \n",
       "0  HQDNVHARSLMGLVRNVFEQAGLEKTALDAVAVSSGPGSYTGLRIG...   PF00814  actin_like  \n",
       "1  ILLIDAGNTAIKLAVSDEHNQPTLIKQVQLCWSKISLVLYSCVKHS...   PF03309  actin_like  \n",
       "2  DQAEWLLADGTVQHGTLTELVPHVIGARLLLIAPGERLTLHRVPLP...   PF05134  actin_like  \n",
       "3  LGLDGGTTSTVCVCLPAAMPLASALPDPLPILSRAVAGCSNHNSVG...   PF01869  actin_like  \n",
       "4  AGLPSATPEQIHAALLTNVVIIGGTSLLQGLEQRLVNDLSLQFPQY...   PF00022  actin_like  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfamA_balanced = pd.read_csv(\"../../data/pfamA_motors_balanced.csv\")\n",
    "pfamA_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>seq</th>\n",
       "      <th>pfamA_acc</th>\n",
       "      <th>pfamA_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2966648</td>\n",
       "      <td>A0A1Y3GBD4_9EURY/5-68 A0A1Y3GBD4.1 PF01918.22;...</td>\n",
       "      <td>NVVYVGNKEVMSYVLAVTTQFNEGSDEVVIKARGRAISTAVDTAEV...</td>\n",
       "      <td>PF01918</td>\n",
       "      <td>Alba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2966649</td>\n",
       "      <td>G0V5M1_NAUCC/40-110 G0V5M1.1 PF01918.22;Alba;</td>\n",
       "      <td>KRVCITKNANIKHTVDKLQGLLESTEGFVCLEAHGPHLQKMLSILE...</td>\n",
       "      <td>PF01918</td>\n",
       "      <td>Alba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2966650</td>\n",
       "      <td>A0A2G2W1S6_CAPBA/19-84 A0A2G2W1S6.1 PF01918.22...</td>\n",
       "      <td>NEIRVTAQGLIRNYISYATTLLQDQRTNEVVLKAMGQAISKTVAIA...</td>\n",
       "      <td>PF01918</td>\n",
       "      <td>Alba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2966651</td>\n",
       "      <td>G2QUI4_THITE/71-147 G2QUI4.1 PF01918.22;Alba;</td>\n",
       "      <td>KPMSVMPSTSIGKHVDRALEHLGRFSAWDTSVLPGVVLLCAKSSAS...</td>\n",
       "      <td>PF01918</td>\n",
       "      <td>Alba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2966652</td>\n",
       "      <td>A0A0E0K979_ORYPU/20-85 A0A0E0K979.1 PF01918.22...</td>\n",
       "      <td>NEIRITTQGLIRNYVTYATSLLQEKRVKEIVLKAMGQAISKTVAIA...</td>\n",
       "      <td>PF01918</td>\n",
       "      <td>Alba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0     2966648  A0A1Y3GBD4_9EURY/5-68 A0A1Y3GBD4.1 PF01918.22;...   \n",
       "1     2966649      G0V5M1_NAUCC/40-110 G0V5M1.1 PF01918.22;Alba;   \n",
       "2     2966650  A0A2G2W1S6_CAPBA/19-84 A0A2G2W1S6.1 PF01918.22...   \n",
       "3     2966651      G2QUI4_THITE/71-147 G2QUI4.1 PF01918.22;Alba;   \n",
       "4     2966652  A0A0E0K979_ORYPU/20-85 A0A0E0K979.1 PF01918.22...   \n",
       "\n",
       "                                                 seq pfamA_acc pfamA_name  \n",
       "0  NVVYVGNKEVMSYVLAVTTQFNEGSDEVVIKARGRAISTAVDTAEV...   PF01918       Alba  \n",
       "1  KRVCITKNANIKHTVDKLQGLLESTEGFVCLEAHGPHLQKMLSILE...   PF01918       Alba  \n",
       "2  NEIRVTAQGLIRNYISYATTLLQDQRTNEVVLKAMGQAISKTVAIA...   PF01918       Alba  \n",
       "3  KPMSVMPSTSIGKHVDRALEHLGRFSAWDTSVLPGVVLLCAKSSAS...   PF01918       Alba  \n",
       "4  NEIRITTQGLIRNYVTYATSLLQEKRVKEIVLKAMGQAISKTVAIA...   PF01918       Alba  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfamA_random = pd.read_csv(\"../../data/pfamA_random_201027.csv\")\n",
    "pfamA_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinBertModel(\n",
       "  (embed_tokens): Embedding(35, 1280, padding_idx=1)\n",
       "  (embed_positions): PositionalEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (12): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (13): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (14): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (15): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (16): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (17): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (18): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (19): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (20): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (21): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (22): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (23): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (24): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (25): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (26): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (27): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (28): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (29): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (30): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (31): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (32): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "    (33): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): BertLayerNorm()\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): BertLayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (two protein sequences)\n",
    "# data = [(\"protein1\", \"HQDNVHARSLMGLVRNVFEQAGLEKTALDAVAVSSGPG\"), (\"protein2\", \"DAGNTAIKLAVSDEHNQPTLIKQVQLCWSKISLVLYSCVKHS\")]\n",
    "\n",
    "\n",
    "# print(batch_labels)\n",
    "# print(batch_strs)\n",
    "# print(batch_tokens)\n",
    "# Extract per-residue embeddings (on CPU)\n",
    "def esm_embed(df):\n",
    "    sequence_embeddings = []\n",
    "    for i in range(pfamA_balanced.shape[0]):\n",
    "        if (i%10)==0:\n",
    "            print(i)\n",
    "        data = [(pfamA_balanced.iloc[i,1], pfamA_balanced.iloc[i,3])]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        print(batch_labels)\n",
    "        print(batch_strs)\n",
    "        print(batch_tokens)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens.to('cuda'), repr_layers=[34])\n",
    "            # last layer\n",
    "            token_embeddings = results[\"representations\"][34]\n",
    "            seq = pfamA_balanced.iloc[i,3]\n",
    "            sequence_embeddings.append(token_embeddings[0, 1:len(seq) + 1].mean(0).cpu().detach().numpy())\n",
    "        break\n",
    "#     print(np.array(sequence_embeddings).shape)\n",
    "#     np.save(\"../../data/balanced_esm.npy\", np.array(sequence_embeddings))\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['U5C9Z2_9BACT/7-195']\n",
      "['HQDNVHARSLMGLVRNVFEQAGLEKTALDAVAVSSGPGSYTGLRIGVSVAKGLAYALDKPVIGVGTLEALAFRAIPFSDSTDTIIPMLDARRMEVYALVMDGLGDTLISPQPFILEDNPFMEYLEKGKVFFLGDGVPKSKEILSHPNSRFVPLFNSSQSIGELAYKKFLKADFESLAYFEPNYIKEFRI']\n",
      "tensor([[32, 21, 16, 13, 17,  7, 21,  5, 10,  8,  4, 20,  6,  4,  7, 10, 17,  7,\n",
      "         18,  9, 16,  5,  6,  4,  9, 15, 11,  5,  4, 13,  5,  7,  5,  7,  8,  8,\n",
      "          6, 14,  6,  8, 19, 11,  6,  4, 10, 12,  6,  7,  8,  7,  5, 15,  6,  4,\n",
      "          5, 19,  5,  4, 13, 15, 14,  7, 12,  6,  7,  6, 11,  4,  9,  5,  4,  5,\n",
      "         18, 10,  5, 12, 14, 18,  8, 13,  8, 11, 13, 11, 12, 12, 14, 20,  4, 13,\n",
      "          5, 10, 10, 20,  9,  7, 19,  5,  4,  7, 20, 13,  6,  4,  6, 13, 11,  4,\n",
      "         12,  8, 14, 16, 14, 18, 12,  4,  9, 13, 17, 14, 18, 20,  9, 19,  4,  9,\n",
      "         15,  6, 15,  7, 18, 18,  4,  6, 13,  6,  7, 14, 15,  8, 15,  9, 12,  4,\n",
      "          8, 21, 14, 17,  8, 10, 18,  7, 14,  4, 18, 17,  8,  8, 16,  8, 12,  6,\n",
      "          9,  4,  5, 19, 15, 15, 18,  4, 15,  5, 13, 18,  9,  8,  4,  5, 19, 18,\n",
      "          9, 14, 17, 19, 12, 15,  9, 18, 10, 12]])\n"
     ]
    }
   ],
   "source": [
    "esm_embed(pfamA_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokens[:,3] = 34\n",
    "seq_len = batch_tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_seqind = torch.randperm(seq_len)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,   4,   5,  44,  65, 112, 171,  56, 168,  34, 164, 131,\n",
       "        180,  54, 126, 181, 127, 187,  79, 138,  97,  80, 201, 161,  39,  60,\n",
       "        128,   2, 191, 103, 175,  81, 147,  67, 207, 116,  53, 134, 137, 122,\n",
       "        177,  92, 141,  40,  62,  28, 148,  90, 117, 104, 178,  11,  48, 105,\n",
       "        196, 163, 153,  12,  52,  36,  46, 158,  86,  99, 139, 188,  41, 114,\n",
       "        197,  83, 157, 162, 144,   7,  72,  31,  87, 179,  64, 101, 146,  21,\n",
       "        169, 150,  76, 120,  57, 142, 130,  37,  35, 125,  70,   8, 193,  18,\n",
       "         95,  29,  58, 106,  55, 119,   6, 143, 111,  75,  47,  24,  45,   9,\n",
       "        136, 185, 182, 189, 184, 167, 155,   1,  93,  91, 151,  27, 121, 170,\n",
       "        165, 102,  51, 132, 107, 109, 124,  23, 154, 123,  50,   4, 200, 115,\n",
       "         43,  59, 110,  88,  10,   3,  19,  32,  74, 108, 166,  68, 183,  71,\n",
       "         82,  38, 133, 186,  77, 195,  73,  20,  16,  89, 129,  61,  85, 190,\n",
       "        156, 174,  84, 118, 113,  98, 194, 202, 145,  17,  15, 149,  66,  13,\n",
       "        176,  96, 172,  26,  25,  30, 100,  78, 192, 203,  42,  33,  14, 205,\n",
       "        204, 198,  63, 152,  69,   5,  49,  94, 199, 140, 160])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_seqind[0:5] = torch.tensor([1,2,3,4,5])\n",
    "shuffled_seqind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_num = int(seq_len * 0.15)\n",
    "random_num = int(0.1*masked_num)\n",
    "keep_num = int(0.1*masked_num)\n",
    "masked_num = masked_num - random_num - keep_num\n",
    "# generate #masked_num indices from range 1 to batch_tokens.size(1)\n",
    "shuffled_seqind = torch.randperm(seq_len)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_id = shuffled_seqind[0:masked_num].size()\n",
    "random_id = shuffled_seqind[masked_num:masked_num+masked_num].size()\n",
    "mask_id = shuffled_seqind[masked_num+masked_num:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1,26,(1,random_num)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
