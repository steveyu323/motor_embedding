{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "> The following code adopted from transformer_encoder_ver2 (a smaller embedding and FDN with the hope of getting less concentrated features). It further exapanded by applying a masked language model. \n",
    "\n",
    "> In the previous model, a mask is applied so that at each word, it sees only the word prior to it. (When predicting #2 word, it sees only #1 word, and all rest are masked to -INF), so that the model follows an auto-regressive manner\n",
    "\n",
    "> In the MLM setting, certain proportion of the sentense is randomly masked (15% in BERT), and they are masked throughout the training process. The loss is only on those positions's correctness. In the TransformerEncoderLayer, the mask pass in should be changed and should mask the position of mask to -INF while keeping all the rest to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179519      179519\n",
      "1414859    1414859\n",
      "12920        12920\n",
      "1415258    1415258\n",
      "13385        13385\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(18000, 6)\n",
      "13493    180756\n",
      "1539     166414\n",
      "2688     131988\n",
      "1691      37094\n",
      "188      130155\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(59149, 6)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "pfamA_motors = pd.read_csv(\"../../data/pfamA_motors.csv\")\n",
    "df_dev = pd.read_csv(\"../../data/df_dev.csv\")\n",
    "motor_toolkit = pd.read_csv(\"../../data/motor_tookits.csv\")\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors.groupby('clan').apply(lambda _df: _df.sample(4500,random_state=1))\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.apply(lambda x: x.reset_index(drop = True))\n",
    "\n",
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "\n",
    "pfamA_target = pfamA_motors.loc[pfamA_motors[\"pfamA_acc\"].isin(pfamA_target_name),:]\n",
    "\n",
    "\n",
    "# shuffle pfamA_target and pfamA_motors_balanced\n",
    "pfamA_target = pfamA_target.sample(frac = 1)\n",
    "pfamA_target_ind = pfamA_target.iloc[:,0]\n",
    "print(pfamA_target_ind[0:5])\n",
    "print(pfamA_motors_balanced.shape)\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.sample(frac = 1) \n",
    "pfamA_motors_balanced_ind = pfamA_motors_balanced.iloc[:,0]\n",
    "print(pfamA_motors_balanced_ind[0:5])\n",
    "print(pfamA_target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20,  2,  7,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# def prepare_labels(seq):\n",
    "#     idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_eval(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "prepare_sequence('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding module injects some information about the relative or absolute position of\n",
    "    the tokens in the sequence. The positional encodings have the same dimension as the embeddings \n",
    "    so that the two can be summed. Here, we use sine and cosine functions of different frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         print(\"x.size() : \", x.size())\n",
    "#         print(\"self.pe.size() :\", self.pe[:x.size(0),:,:].size())\n",
    "        x = torch.add(x ,Variable(self.pe[:x.size(0),:,:], requires_grad=False))\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout,activation='gelu')\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def _generate_square_mlm_mask(self, sz, mask_frac=0.15):\n",
    "        # 0's are the masked position\n",
    "        zeros_num = int(sz * mask_frac)\n",
    "        ones_num = sz - zeros_num\n",
    "        lm_mask = torch.cat([torch.zeros(zeros_num), torch.ones(ones_num)])\n",
    "        lm_mask = lm_mask[torch.randperm(sz)]\n",
    "        masked_ind = lm_mask.eq(0)\n",
    "        lm_mask = lm_mask.repeat(sz, 1)\n",
    "        mask = lm_mask.float().masked_fill(lm_mask == 0, float('-inf')).masked_fill(lm_mask == 1, float(0.0))\n",
    "        return mask,masked_ind\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if model.training:\n",
    "            device = src.device\n",
    "            mask,masked_ind = self._generate_square_mlm_mask(src.size(0))\n",
    "            mask = mask.to(device)\n",
    "            self.src_mask = mask\n",
    "            self.src_mask_ind = masked_ind\n",
    "            \n",
    "#         print(\"src.device: \", src.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "#         print(\"self.encoder(src) size: \", src.size())\n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"elf.pos_encoder(src) size: \", src.size())\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "#         print(\"output size: \", output.size())\n",
    "        output = self.decoder(output)\n",
    "        return output, self.src_mask_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(aminoacid_list) + 1 # the size of vocabulary\n",
    "emsize = 12 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 12 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(21, 12)\n",
       "  (decoder): Linear(in_features=12, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train() # Turn on the train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "torch.Size([20])\n",
      "torch.Size([20, 21])\n",
      "At Epoch: 0.0\n",
      "Loss 2.9990\n",
      "time elapsed 0.0545\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "    print(len(seq))\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output,mask_ind = model(sentence_in)\n",
    "#     print(mask_ind)\n",
    "    targets = sentence_in[mask_ind]\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    print(targets.squeeze(1).size())\n",
    "    print(output[mask_ind].squeeze(1).size())\n",
    "\n",
    "    loss = criterion(output[mask_ind].squeeze(1), targets.squeeze(1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_201025.pt\")\n",
    "#     loss_vector.append(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.7891\n",
      "time elapsed 0.0447\n",
      "At Epoch: 1000.0\n",
      "Loss 0.0325\n",
      "time elapsed 20.6619\n",
      "At Epoch: 2000.0\n",
      "Loss 0.0132\n",
      "time elapsed 40.8814\n",
      "At Epoch: 3000.0\n",
      "Loss 0.2113\n",
      "time elapsed 60.8712\n",
      "At Epoch: 4000.0\n",
      "Loss 0.0011\n",
      "time elapsed 80.6062\n",
      "At Epoch: 5000.0\n",
      "Loss 0.0002\n",
      "time elapsed 100.3380\n",
      "At Epoch: 6000.0\n",
      "Loss 0.0116\n",
      "time elapsed 119.9446\n",
      "At Epoch: 7000.0\n",
      "Loss 0.0004\n",
      "time elapsed 139.4941\n",
      "At Epoch: 8000.0\n",
      "Loss 0.0005\n",
      "time elapsed 158.8835\n",
      "At Epoch: 9000.0\n",
      "Loss 0.0002\n",
      "time elapsed 177.8303\n",
      "At Epoch: 10000.0\n",
      "Loss 0.0009\n",
      "time elapsed 197.3433\n",
      "At Epoch: 11000.0\n",
      "Loss 0.0004\n",
      "time elapsed 217.1073\n",
      "At Epoch: 12000.0\n",
      "Loss 0.0002\n",
      "time elapsed 236.8729\n",
      "At Epoch: 13000.0\n",
      "Loss 0.0001\n",
      "time elapsed 256.5046\n",
      "At Epoch: 14000.0\n",
      "Loss 0.2801\n",
      "time elapsed 276.4871\n",
      "At Epoch: 15000.0\n",
      "Loss 0.0349\n",
      "time elapsed 296.3113\n",
      "At Epoch: 16000.0\n",
      "Loss 0.0001\n",
      "time elapsed 316.0182\n",
      "At Epoch: 17000.0\n",
      "Loss 0.0001\n",
      "time elapsed 335.5370\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "#     print(len(seq))\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "#     print(sentence_in.size())\n",
    "    optimizer.zero_grad()\n",
    "    output,mask_ind = model(sentence_in)\n",
    "#     print(mask_ind)\n",
    "    targets = sentence_in[mask_ind]\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "#     print(targets.squeeze(1).size())\n",
    "#     print(output[mask_ind].squeeze(1).size())\n",
    "\n",
    "    loss = criterion(output[mask_ind].squeeze(1), targets.squeeze(1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "        torch.save(model.state_dict(), \"../../data/transformer_encoder_mlm_201025.pt\")\n",
    "#     loss_vector.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/mini_transformer_encoder_mlm_balanced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed weight updates using the entire pfam_motor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 0.0004\n",
      "time elapsed 0.0398\n",
      "At Epoch: 1000.0\n",
      "Loss 0.0180\n",
      "time elapsed 19.9810\n",
      "At Epoch: 2000.0\n",
      "Loss 0.3997\n",
      "time elapsed 39.8189\n",
      "At Epoch: 3000.0\n",
      "Loss 0.0003\n",
      "time elapsed 59.7628\n",
      "At Epoch: 4000.0\n",
      "Loss 0.0957\n",
      "time elapsed 79.6739\n",
      "At Epoch: 5000.0\n",
      "Loss 0.0000\n",
      "time elapsed 99.5836\n",
      "At Epoch: 6000.0\n",
      "Loss 0.0004\n",
      "time elapsed 119.5286\n",
      "At Epoch: 7000.0\n",
      "Loss 0.0001\n",
      "time elapsed 139.5472\n",
      "At Epoch: 8000.0\n",
      "Loss 0.1972\n",
      "time elapsed 159.5110\n",
      "At Epoch: 9000.0\n",
      "Loss 0.0060\n",
      "time elapsed 179.5563\n",
      "At Epoch: 10000.0\n",
      "Loss 0.0038\n",
      "time elapsed 199.4711\n",
      "At Epoch: 11000.0\n",
      "Loss 0.0000\n",
      "time elapsed 219.6897\n",
      "At Epoch: 12000.0\n",
      "Loss 0.0001\n",
      "time elapsed 239.8744\n",
      "At Epoch: 13000.0\n",
      "Loss 0.0002\n",
      "time elapsed 259.8663\n",
      "At Epoch: 14000.0\n",
      "Loss 0.0014\n",
      "time elapsed 279.3699\n",
      "At Epoch: 15000.0\n",
      "Loss 0.0005\n",
      "time elapsed 300.5702\n",
      "At Epoch: 16000.0\n",
      "Loss 0.0007\n",
      "time elapsed 321.4982\n",
      "At Epoch: 17000.0\n",
      "Loss 0.0000\n",
      "time elapsed 341.3100\n",
      "At Epoch: 18000.0\n",
      "Loss 0.0000\n",
      "time elapsed 360.9946\n",
      "At Epoch: 19000.0\n",
      "Loss 0.0696\n",
      "time elapsed 380.2019\n",
      "At Epoch: 20000.0\n",
      "Loss 0.0003\n",
      "time elapsed 399.7678\n",
      "At Epoch: 21000.0\n",
      "Loss 0.0006\n",
      "time elapsed 418.6420\n",
      "At Epoch: 22000.0\n",
      "Loss 0.0026\n",
      "time elapsed 437.4579\n",
      "At Epoch: 23000.0\n",
      "Loss 0.0000\n",
      "time elapsed 456.4931\n",
      "At Epoch: 24000.0\n",
      "Loss 0.0002\n",
      "time elapsed 475.6142\n",
      "At Epoch: 25000.0\n",
      "Loss 0.0000\n",
      "time elapsed 494.8597\n",
      "At Epoch: 26000.0\n",
      "Loss 0.0000\n",
      "time elapsed 514.8452\n",
      "At Epoch: 27000.0\n",
      "Loss 0.0004\n",
      "time elapsed 534.8567\n",
      "At Epoch: 28000.0\n",
      "Loss 0.0030\n",
      "time elapsed 554.8715\n",
      "At Epoch: 29000.0\n",
      "Loss 0.0001\n",
      "time elapsed 576.0052\n",
      "At Epoch: 30000.0\n",
      "Loss 0.0001\n",
      "time elapsed 595.5033\n",
      "At Epoch: 31000.0\n",
      "Loss 0.0001\n",
      "time elapsed 615.2527\n",
      "At Epoch: 32000.0\n",
      "Loss 0.0026\n",
      "time elapsed 635.7172\n",
      "At Epoch: 33000.0\n",
      "Loss 0.0000\n",
      "time elapsed 655.2698\n",
      "At Epoch: 34000.0\n",
      "Loss 0.0000\n",
      "time elapsed 675.1459\n",
      "At Epoch: 35000.0\n",
      "Loss 0.0000\n",
      "time elapsed 694.2505\n",
      "At Epoch: 36000.0\n",
      "Loss 0.8177\n",
      "time elapsed 713.2897\n",
      "At Epoch: 37000.0\n",
      "Loss 0.1759\n",
      "time elapsed 733.6247\n",
      "At Epoch: 38000.0\n",
      "Loss 0.0474\n",
      "time elapsed 753.6539\n",
      "At Epoch: 39000.0\n",
      "Loss 0.0002\n",
      "time elapsed 773.5960\n",
      "At Epoch: 40000.0\n",
      "Loss 0.0005\n",
      "time elapsed 793.6064\n",
      "At Epoch: 41000.0\n",
      "Loss 0.0741\n",
      "time elapsed 813.5946\n",
      "At Epoch: 42000.0\n",
      "Loss 0.0030\n",
      "time elapsed 833.5506\n",
      "At Epoch: 43000.0\n",
      "Loss 0.0001\n",
      "time elapsed 853.3963\n",
      "At Epoch: 44000.0\n",
      "Loss 0.0051\n",
      "time elapsed 873.2939\n",
      "At Epoch: 45000.0\n",
      "Loss 0.0012\n",
      "time elapsed 894.5904\n",
      "At Epoch: 46000.0\n",
      "Loss 0.1820\n",
      "time elapsed 914.8665\n",
      "At Epoch: 47000.0\n",
      "Loss 0.0000\n",
      "time elapsed 935.0407\n",
      "At Epoch: 48000.0\n",
      "Loss 0.0000\n",
      "time elapsed 954.9849\n",
      "At Epoch: 49000.0\n",
      "Loss 0.0000\n",
      "time elapsed 974.9297\n",
      "At Epoch: 50000.0\n",
      "Loss 0.0059\n",
      "time elapsed 995.6417\n",
      "At Epoch: 51000.0\n",
      "Loss 0.0000\n",
      "time elapsed 1015.5499\n",
      "At Epoch: 52000.0\n",
      "Loss 0.0519\n",
      "time elapsed 1035.5710\n",
      "At Epoch: 53000.0\n",
      "Loss 0.0000\n",
      "time elapsed 1055.5737\n",
      "At Epoch: 54000.0\n",
      "Loss 0.0003\n",
      "time elapsed 1075.6092\n",
      "At Epoch: 55000.0\n",
      "Loss 0.0121\n",
      "time elapsed 1095.5459\n",
      "At Epoch: 56000.0\n",
      "Loss 0.0009\n",
      "time elapsed 1115.4242\n",
      "At Epoch: 57000.0\n",
      "Loss 0.0002\n",
      "time elapsed 1135.3681\n",
      "At Epoch: 58000.0\n",
      "Loss 0.0001\n",
      "time elapsed 1155.3149\n",
      "At Epoch: 59000.0\n",
      "Loss 0.0006\n",
      "time elapsed 1175.3191\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_target.shape[0]): \n",
    "    seq = pfamA_target.iloc[epoch, 3]\n",
    "#     print(len(seq))\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "#     print(sentence_in.size())\n",
    "    optimizer.zero_grad()\n",
    "    output,mask_ind = model(sentence_in)\n",
    "#     print(mask_ind)\n",
    "    targets = sentence_in[mask_ind]\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "#     print(targets.squeeze(1).size())\n",
    "#     print(output[mask_ind].squeeze(1).size())\n",
    "\n",
    "    loss = criterion(output[mask_ind].squeeze(1), targets.squeeze(1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_mlm_201025.pt\")\n",
    "#     loss_vector.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/mini_transformer_encoder_mlm_balanced_target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
