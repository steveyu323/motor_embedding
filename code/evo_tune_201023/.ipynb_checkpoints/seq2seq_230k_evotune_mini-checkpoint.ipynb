{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "# import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179519      179519\n",
      "1414859    1414859\n",
      "12920        12920\n",
      "1415258    1415258\n",
      "13385        13385\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(18000, 6)\n",
      "13493    180756\n",
      "1539     166414\n",
      "2688     131988\n",
      "1691      37094\n",
      "188      130155\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(59149, 6)\n"
     ]
    }
   ],
   "source": [
    "pfamA_motors = pd.read_csv(\"../../data/pfamA_motors.csv\")\n",
    "df_dev = pd.read_csv(\"../../data/df_dev.csv\")\n",
    "motor_toolkit = pd.read_csv(\"../../data/motor_tookits.csv\")\n",
    "pfamA_motors_balanced = pfamA_motors.groupby('clan').apply(lambda _df: _df.sample(4500,random_state=1))\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.apply(lambda x: x.reset_index(drop = True))\n",
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "\n",
    "pfamA_target = pfamA_motors.loc[pfamA_motors[\"pfamA_acc\"].isin(pfamA_target_name),:]\n",
    "\n",
    "# shuffle pfamA_target and pfamA_motors_balanced\n",
    "pfamA_target = pfamA_target.sample(frac = 1)\n",
    "pfamA_target_ind = pfamA_target.iloc[:,0]\n",
    "print(pfamA_target_ind[0:5])\n",
    "print(pfamA_motors_balanced.shape)\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.sample(frac = 1) \n",
    "pfamA_motors_balanced_ind = pfamA_motors_balanced.iloc[:,0]\n",
    "print(pfamA_motors_balanced_ind[0:5])\n",
    "print(pfamA_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[0:-1],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_labels(seq):\n",
    "    idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_eval(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "prepare_labels('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = len(aminoacid_list) + 1\n",
    "num_layers = 1\n",
    "hidden_size = 64\n",
    "output_size = len(aminoacid_list) + 1\n",
    "embedding_size= 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size, batch_first=False, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.log_softmax = nn.LogSoftmax(dim= 1)\n",
    "        self.batch_first = batch_first\n",
    "        self.aa_embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                           batch_first=batch_first, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # embed each aa to the embedded space\n",
    "        embedding_tensor = self.aa_embedding(seq)\n",
    "        #output of shape (seq_len, batch, num_directions * hidden_size):\n",
    "        outputs, hidden = self.rnn(embedding_tensor.view(len(seq), 1, -1))\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # both forward and backward direction\n",
    "        self.enc_units = hidden_size*2\n",
    "        self.dec_units = hidden_size*2\n",
    "        \n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units, bias=False)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units, bias=False)\n",
    "        self.vt = nn.Linear(self.enc_units, 1, bias=False)\n",
    "\n",
    "    def forward(self,encoder_outputs,decoder_state):\n",
    "        \n",
    "        \n",
    "        # encoder_outputs: (seq_len, batch_size, hidden_size*2)\n",
    "        encoder_transform = self.W1(encoder_outputs)\n",
    "#         print(\"encoder_transform.shape: \", encoder_transform.shape)\n",
    "\n",
    "        # (1 (unsqueezed),batch_size, hidden_size*2)\n",
    "        decoder_transform = self.W2(decoder_state)\n",
    "#         print(\"decoder_transform: \", decoder_transform.shape)\n",
    "        \n",
    "        combined_transform = encoder_transform + decoder_transform\n",
    "#         print(\"combined_transform.shape \", combined_transform.shape)\n",
    "        # 1st line of Eq.(3) in the paper\n",
    "        # (seq_len, batch_size = 1 , 1) => squeeze to (seq_len, batch_size)\n",
    "        u_i = self.vt(torch.tanh(combined_transform)).squeeze()\n",
    "#         print(\"u_i.shape \", u_i.shape)\n",
    "\n",
    "        # log-softmax for a better numerical stability\n",
    "        attention_weights = F.log_softmax(u_i, dim=0).view(-1, 1, 1)\n",
    "#         print(\"attention_weights.shape \", attention_weights.shape)\n",
    "        \n",
    "        #context_vector shape after sum == (batch,hidden*2)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        context_vector = torch.sum(context_vector, dim=0)\n",
    "#         print(\"context_vector.shape \", context_vector.shape)\n",
    "        return context_vector,attention_weights\n",
    "\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # hidden size refers to input hidden size, decoder hidden size should be 2*encoder hidden size since the decoder is unidirectional\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size, batch_first=False, bidirectional=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.log_softmax = nn.LogSoftmax(dim= 1)\n",
    "        self.batch_first = batch_first\n",
    "        self.context_size = hidden_size*2\n",
    "        self.aa_embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTMCell(input_size=embedding_size + self.context_size, hidden_size=hidden_size*2)\n",
    "        self.attn = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "    def forward(self, y, encoder_outputs,decoder_state,decoder_cell):\n",
    "        \n",
    "        context_vector,attention_weights = self.attn(encoder_outputs,decoder_state)\n",
    "#         print(\"context_vector.shape: \", context_vector.shape)\n",
    "#         print(\"attention_weights.shape: \", attention_weights.shape)\n",
    "        y_embedded = self.aa_embedding(y)\n",
    "#         print(\"y_embedded.shape: \", y_embedded.shape)\n",
    "        y_cat = torch.cat((context_vector.unsqueeze(1), y_embedded), -1)\n",
    "#         print(\"y_cat.shape: \", y_cat.shape)\n",
    "        hidden = self.rnn(y_cat.squeeze(1),(decoder_state,decoder_cell))\n",
    "        h_i,_ = hidden\n",
    "#         print(\"h_i.shape: \", h_i.shape)\n",
    "        decoded_space = self.fc(h_i)\n",
    "#         print(\"decoded_space.shape: \", decoded_space.shape)\n",
    "        decoded_scores = F.log_softmax(decoded_space, dim = 1)\n",
    "#         print(\"decoded_scores.shape: \", decoded_scores.shape)\n",
    "        decoded_aa = torch.argmax(decoded_scores)\n",
    "#         print(\"decoded_aa.shape: \", decoded_aa.shape)\n",
    "        return decoded_scores, decoded_aa, hidden, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size = input_size, \\\n",
    "                               embedding_size = embedding_size, \\\n",
    "                               hidden_size = hidden_size, \\\n",
    "                               num_layers = num_layers, \\\n",
    "                               output_size = output_size)\n",
    "decoder = Decoder(input_size = input_size, \\\n",
    "                               embedding_size = embedding_size, \\\n",
    "                               hidden_size = hidden_size, \\\n",
    "                               num_layers = num_layers, \\\n",
    "                               output_size = output_size)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "for m in encoder.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "for m in decoder.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()                \n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                       lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (log_softmax): LogSoftmax()\n",
       "  (aa_embedding): Embedding(21, 10)\n",
       "  (rnn): LSTM(10, 64, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"../../data/seq2seq_encoder_df_dev_201012_230k.pt\"))\n",
    "encoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (log_softmax): LogSoftmax()\n",
       "  (aa_embedding): Embedding(21, 10)\n",
       "  (rnn): LSTMCell(138, 128)\n",
       "  (attn): Attention(\n",
       "    (W1): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (W2): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (vt): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_state_dict(torch.load(\"../../data/seq2seq_decoder_df_dev_201012_230k.pt\"))\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on pfamA_motors_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.9367\n",
      "At Epoch: 1000.0\n",
      "Loss 3.0015\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8021\n",
      "At Epoch: 3000.0\n",
      "Loss 2.7523\n",
      "At Epoch: 4000.0\n",
      "Loss 2.6752\n",
      "At Epoch: 5000.0\n",
      "Loss 2.8581\n",
      "At Epoch: 6000.0\n",
      "Loss 2.8701\n"
     ]
    }
   ],
   "source": [
    "# loss_vector = []\n",
    "# running_loss = 0\n",
    "print_every = 1000\n",
    "\n",
    "# for epoch in np.arange(0, len(clan_training_data)): \n",
    "#     seq, clan = clan_training_data[epoch]\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "#     print(epoch)\n",
    "#     print(\"len(seq): \", len(seq))\n",
    "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "    # Tensors of word indices.\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "    sentence_in = sentence_in.to(device = device)\n",
    "    targets = targets.view(1,-1).to(device = device)\n",
    "#     print(\"targets.shape: \", targets.shape)\n",
    "    \n",
    "#     print(targets.shape)\n",
    "#     print(sentence_in.shape)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    seq_len = sentence_in.size(0)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(sentence_in)\n",
    "#     print(\"encoder_outputs.shape: \" , encoder_outputs.shape)\n",
    "    \n",
    "    encoder_h_n, encoder_c_n = encoder_hidden\n",
    "#     print(\"encoder_h_n.shape: \", encoder_h_n.shape)\n",
    "#     print(\"encoder_hidden_last: \", encoder_c_n.shape)\n",
    "    \n",
    "    # Lets use zeros as an intial input\n",
    "    y_0 = 0\n",
    "    # using zeros for initial decoder hidden and cell state \n",
    "    d_0 = Variable(torch.zeros(1, decoder.hidden_size*2))\n",
    "    dcell_0 = Variable(torch.zeros(1,  decoder.hidden_size*2))\n",
    "#     print(\"d_0.shape: \", d_0.shape)\n",
    "#     print(\"dcell_0.shape: \", dcell_0.shape)\n",
    "    \n",
    "    y_last = y_0\n",
    "    d_last, d_cell_last = d_0, dcell_0\n",
    "\n",
    "    \n",
    "#     print(\"seq_len: \",seq_len)\n",
    "    for di in range(seq_len):\n",
    "        decoded_scores, y_last, (d_last,d_cell_last), attention_weights = decoder(Variable(torch.LongTensor([[y_last]])).to(device), \\\n",
    "                                                                                 encoder_outputs.to(device), \\\n",
    "                                                                                 d_last.to(device),\\\n",
    "                                                                                 d_cell_last.to(device))\n",
    "        \n",
    "        loss += criterion(decoded_scores.to(device), targets[:,di])   \n",
    "    \n",
    "    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% (loss/seq_len))\n",
    "        torch.save(encoder.state_dict(), \"../../data/seq2seq_encoder_balanced.pt\")\n",
    "        torch.save(decoder.state_dict(), \"../../data/seq2seq_decoder_balanced.pt\")\n",
    "#     loss_vector.append(loss/seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"../../data/seq2seq_encoder_balanced.pt\")\n",
    "torch.save(decoder.state_dict(), \"../../data/seq2seq_decoder_balanced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_vector = []\n",
    "# running_loss = 0\n",
    "print_every = 1000\n",
    "\n",
    "# for epoch in np.arange(0, len(clan_training_data)): \n",
    "#     seq, clan = clan_training_data[epoch]\n",
    "\n",
    "for epoch in np.arange(0, pfamA_target.shape[0]): \n",
    "    seq = pfamA_target.iloc[epoch, 3]\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "#     print(epoch)\n",
    "#     print(\"len(seq): \", len(seq))\n",
    "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "    # Tensors of word indices.\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "    sentence_in = sentence_in.to(device = device)\n",
    "    targets = targets.view(1,-1).to(device = device)\n",
    "#     print(\"targets.shape: \", targets.shape)\n",
    "    \n",
    "#     print(targets.shape)\n",
    "#     print(sentence_in.shape)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    seq_len = sentence_in.size(0)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(sentence_in)\n",
    "#     print(\"encoder_outputs.shape: \" , encoder_outputs.shape)\n",
    "    \n",
    "    encoder_h_n, encoder_c_n = encoder_hidden\n",
    "#     print(\"encoder_h_n.shape: \", encoder_h_n.shape)\n",
    "#     print(\"encoder_hidden_last: \", encoder_c_n.shape)\n",
    "    \n",
    "    # Lets use zeros as an intial input\n",
    "    y_0 = 0\n",
    "    # using zeros for initial decoder hidden and cell state \n",
    "    d_0 = Variable(torch.zeros(1, decoder.hidden_size*2))\n",
    "    dcell_0 = Variable(torch.zeros(1,  decoder.hidden_size*2))\n",
    "#     print(\"d_0.shape: \", d_0.shape)\n",
    "#     print(\"dcell_0.shape: \", dcell_0.shape)\n",
    "    \n",
    "    y_last = y_0\n",
    "    d_last, d_cell_last = d_0, dcell_0\n",
    "\n",
    "    \n",
    "#     print(\"seq_len: \",seq_len)\n",
    "    for di in range(seq_len):\n",
    "        decoded_scores, y_last, (d_last,d_cell_last), attention_weights = decoder(Variable(torch.LongTensor([[y_last]])).to(device), \\\n",
    "                                                                                 encoder_outputs.to(device), \\\n",
    "                                                                                 d_last.to(device),\\\n",
    "                                                                                 d_cell_last.to(device))\n",
    "        \n",
    "        loss += criterion(decoded_scores.to(device), targets[:,di])   \n",
    "    \n",
    "    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% (loss/seq_len))\n",
    "        torch.save(encoder.state_dict(), \"../../data/seq2seq_encoder_balanced_target.pt\")\n",
    "        torch.save(decoder.state_dict(), \"../../data/seq2seq_decoder_balanced_target.pt\")\n",
    "#     loss_vector.append(loss/seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"../../data/seq2seq_encoder_balanced_target.pt\")\n",
    "torch.save(decoder.state_dict(), \"../../data/seq2seq_decoder_balanced_target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
