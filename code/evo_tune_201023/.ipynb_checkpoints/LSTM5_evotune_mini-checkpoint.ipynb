{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "# import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfamA_motors = pd.read_csv(\"../../data/pfamA_motors.csv\")\n",
    "df_dev = pd.read_csv(\"../../data/df_dev.csv\")\n",
    "motor_toolkit = pd.read_csv(\"../../data/motor_tookits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfamA_motors_balanced = pfamA_motors.groupby('clan').apply(lambda _df: _df.sample(4500,random_state=1))\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.apply(lambda x: x.reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "\n",
    "pfamA_target = pfamA_motors.loc[pfamA_motors[\"pfamA_acc\"].isin(pfamA_target_name),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179519      179519\n",
      "1414859    1414859\n",
      "12920        12920\n",
      "1415258    1415258\n",
      "13385        13385\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(18000, 6)\n",
      "13493    180756\n",
      "1539     166414\n",
      "2688     131988\n",
      "1691      37094\n",
      "188      130155\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(59149, 6)\n"
     ]
    }
   ],
   "source": [
    "# shuffle pfamA_target and pfamA_motors_balanced\n",
    "pfamA_target = pfamA_target.sample(frac = 1)\n",
    "pfamA_target_ind = pfamA_target.iloc[:,0]\n",
    "print(pfamA_target_ind[0:5])\n",
    "print(pfamA_motors_balanced.shape)\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.sample(frac = 1) \n",
    "pfamA_motors_balanced_ind = pfamA_motors_balanced.iloc[:,0]\n",
    "print(pfamA_motors_balanced_ind[0:5])\n",
    "print(pfamA_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>seq</th>\n",
       "      <th>pfamA_acc</th>\n",
       "      <th>clan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13493</th>\n",
       "      <td>180756</td>\n",
       "      <td>F7HLH3_MACMU/18-154</td>\n",
       "      <td>F7HLH3_MACMU/18-154 F7HLH3.2 PF00091.26;Tubulin;</td>\n",
       "      <td>SWSASTCTTTRPAVAGTCPAGAGNNWARGHYTEGAELMESVMDVVR...</td>\n",
       "      <td>PF00091</td>\n",
       "      <td>tubulin_binding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>166414</td>\n",
       "      <td>A0A0X8UZH9_9EURY/22-293</td>\n",
       "      <td>A0A0X8UZH9_9EURY/22-293 A0A0X8UZH9.1 PF00814.2...</td>\n",
       "      <td>AHVLSNIIDLFRPPQGGLHPREAANHHADAVAKTIVEAVETAGISL...</td>\n",
       "      <td>PF00814</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>131988</td>\n",
       "      <td>A0A1S6U8J5_9PROT/24-293</td>\n",
       "      <td>A0A1S6U8J5_9PROT/24-293 A0A1S6U8J5.1 PF02541.1...</td>\n",
       "      <td>CFVEKSFEAIVGSARGLRENMLISDEAKERIFNALKLAKEEFDFSL...</td>\n",
       "      <td>PF02541</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>37094</td>\n",
       "      <td>A0A0Q5VID2_9CAUL/261-451</td>\n",
       "      <td>A0A0Q5VID2_9CAUL/261-451 A0A0Q5VID2.1 PF02782....</td>\n",
       "      <td>AKITYGTGAFLVANVGDQPVVSTRRLLGTLGYDVRGTAAYALEGSI...</td>\n",
       "      <td>PF02782</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>130155</td>\n",
       "      <td>A0A2Z4UGU8_9RHIZ/111-409</td>\n",
       "      <td>A0A2Z4UGU8_9RHIZ/111-409 A0A2Z4UGU8.1 PF02541....</td>\n",
       "      <td>LVAKRSRDGFRVIDAYSRIVRLGEGLASTGQLSDDAMNRAAAALKI...</td>\n",
       "      <td>PF02541</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                        id  \\\n",
       "13493      180756       F7HLH3_MACMU/18-154   \n",
       "1539       166414   A0A0X8UZH9_9EURY/22-293   \n",
       "2688       131988   A0A1S6U8J5_9PROT/24-293   \n",
       "1691        37094  A0A0Q5VID2_9CAUL/261-451   \n",
       "188        130155  A0A2Z4UGU8_9RHIZ/111-409   \n",
       "\n",
       "                                             description  \\\n",
       "13493   F7HLH3_MACMU/18-154 F7HLH3.2 PF00091.26;Tubulin;   \n",
       "1539   A0A0X8UZH9_9EURY/22-293 A0A0X8UZH9.1 PF00814.2...   \n",
       "2688   A0A1S6U8J5_9PROT/24-293 A0A1S6U8J5.1 PF02541.1...   \n",
       "1691   A0A0Q5VID2_9CAUL/261-451 A0A0Q5VID2.1 PF02782....   \n",
       "188    A0A2Z4UGU8_9RHIZ/111-409 A0A2Z4UGU8.1 PF02541....   \n",
       "\n",
       "                                                     seq pfamA_acc  \\\n",
       "13493  SWSASTCTTTRPAVAGTCPAGAGNNWARGHYTEGAELMESVMDVVR...   PF00091   \n",
       "1539   AHVLSNIIDLFRPPQGGLHPREAANHHADAVAKTIVEAVETAGISL...   PF00814   \n",
       "2688   CFVEKSFEAIVGSARGLRENMLISDEAKERIFNALKLAKEEFDFSL...   PF02541   \n",
       "1691   AKITYGTGAFLVANVGDQPVVSTRRLLGTLGYDVRGTAAYALEGSI...   PF02782   \n",
       "188    LVAKRSRDGFRVIDAYSRIVRLGEGLASTGQLSDDAMNRAAAALKI...   PF02541   \n",
       "\n",
       "                  clan  \n",
       "13493  tubulin_binding  \n",
       "1539        actin_like  \n",
       "2688        actin_like  \n",
       "1691        actin_like  \n",
       "188         actin_like  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfamA_motors_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>seq</th>\n",
       "      <th>pfamA_acc</th>\n",
       "      <th>clan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179519</th>\n",
       "      <td>179519</td>\n",
       "      <td>A0A098S4B7_9BACT/12-174</td>\n",
       "      <td>A0A098S4B7_9BACT/12-174 A0A098S4B7.1 PF00091.2...</td>\n",
       "      <td>IIKVLGVGGGGSNAVTHMFRQGIVGVDFAICNTDSQAMELSPVTTR...</td>\n",
       "      <td>PF00091</td>\n",
       "      <td>tubulin_binding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414859</th>\n",
       "      <td>1414859</td>\n",
       "      <td>A0A0A1SN99_9HYPO/43-437</td>\n",
       "      <td>A0A0A1SN99_9HYPO/43-437 A0A0A1SN99.1 PF00225.2...</td>\n",
       "      <td>RASDEDSRTAVRVAIRIRPPLKPTDPGYELIPQRFQRSMVQTTSDT...</td>\n",
       "      <td>PF00225</td>\n",
       "      <td>p_loop_gtpase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12920</th>\n",
       "      <td>12920</td>\n",
       "      <td>M7C1E0_CHEMY/152-523</td>\n",
       "      <td>M7C1E0_CHEMY/152-523 M7C1E0.1 PF00022.20;Actin;</td>\n",
       "      <td>MGKVAVVIDNGSCFTRAGFAGEDKPKSVLKTTSMPPTCPAVMREIP...</td>\n",
       "      <td>PF00022</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415258</th>\n",
       "      <td>1415258</td>\n",
       "      <td>H9J8N3_BOMMO/16-329</td>\n",
       "      <td>H9J8N3_BOMMO/16-329 H9J8N3.1 PF00225.24;Kinesin;</td>\n",
       "      <td>NQTFAMDKRKKQVSLCEATSAASAPEDRKVGVTAPKMFAFDAIFSQ...</td>\n",
       "      <td>PF00225</td>\n",
       "      <td>p_loop_gtpase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13385</th>\n",
       "      <td>13385</td>\n",
       "      <td>A0A075AWM4_ROZAC/39-659</td>\n",
       "      <td>A0A075AWM4_ROZAC/39-659 A0A075AWM4.1 PF00022.2...</td>\n",
       "      <td>IDTSKVIVLHPGSETLKFGMATEGLPRTIPNVIARLDPTKGDTMEA...</td>\n",
       "      <td>PF00022</td>\n",
       "      <td>actin_like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                       id  \\\n",
       "179519       179519  A0A098S4B7_9BACT/12-174   \n",
       "1414859     1414859  A0A0A1SN99_9HYPO/43-437   \n",
       "12920         12920     M7C1E0_CHEMY/152-523   \n",
       "1415258     1415258      H9J8N3_BOMMO/16-329   \n",
       "13385         13385  A0A075AWM4_ROZAC/39-659   \n",
       "\n",
       "                                               description  \\\n",
       "179519   A0A098S4B7_9BACT/12-174 A0A098S4B7.1 PF00091.2...   \n",
       "1414859  A0A0A1SN99_9HYPO/43-437 A0A0A1SN99.1 PF00225.2...   \n",
       "12920      M7C1E0_CHEMY/152-523 M7C1E0.1 PF00022.20;Actin;   \n",
       "1415258   H9J8N3_BOMMO/16-329 H9J8N3.1 PF00225.24;Kinesin;   \n",
       "13385    A0A075AWM4_ROZAC/39-659 A0A075AWM4.1 PF00022.2...   \n",
       "\n",
       "                                                       seq pfamA_acc  \\\n",
       "179519   IIKVLGVGGGGSNAVTHMFRQGIVGVDFAICNTDSQAMELSPVTTR...   PF00091   \n",
       "1414859  RASDEDSRTAVRVAIRIRPPLKPTDPGYELIPQRFQRSMVQTTSDT...   PF00225   \n",
       "12920    MGKVAVVIDNGSCFTRAGFAGEDKPKSVLKTTSMPPTCPAVMREIP...   PF00022   \n",
       "1415258  NQTFAMDKRKKQVSLCEATSAASAPEDRKVGVTAPKMFAFDAIFSQ...   PF00225   \n",
       "13385    IDTSKVIVLHPGSETLKFGMATEGLPRTIPNVIARLDPTKGDTMEA...   PF00022   \n",
       "\n",
       "                    clan  \n",
       "179519   tubulin_binding  \n",
       "1414859    p_loop_gtpase  \n",
       "12920         actin_like  \n",
       "1415258    p_loop_gtpase  \n",
       "13385         actin_like  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfamA_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[0:-1],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_labels(seq):\n",
    "    idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_eval(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "prepare_labels('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = len(aminoacid_list) + 1\n",
    "num_layers = 1\n",
    "hidden_size = 128\n",
    "output_size = len(aminoacid_list) + 1\n",
    "embedding_size= 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create Bidirectional LSTM\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self,input_size, embedding_size, hidden_size, num_layers, output_size):\n",
    "        super(BRNN,self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.log_softmax = nn.LogSoftmax(dim= 1)\n",
    "        self.aa_embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_size, \n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, \n",
    "                            bidirectional = True)\n",
    "        #hidden_state: a forward and a backward state for each layer of LSTM\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "    \n",
    "    def aa_encoder(self, input): \n",
    "        \"Helper function to map single aminoacids to the embedding space.\"\n",
    "        projected = self.embedding(input)\n",
    "        return projected \n",
    "    \n",
    "\n",
    "    def forward(self,seq):\n",
    "        # embed each aa to the embedded space\n",
    "        embedding_tensor = self.aa_embedding(seq)\n",
    "\n",
    "        # initialization could be neglected as the default is 0 for h0 and c0\n",
    "        # initialize hidden state\n",
    "        # h0 = torch.zeros(self.num_layers*2,x.size(0),self.hidden_size).to(device)\n",
    "        # initialize cell_state\n",
    "        # c0 = torch.zeros(self.num_layers*2,x.size(0),self.hidden_size).to(device)\n",
    "\n",
    "        # shape(seq_len = len(sequence), batch_size = 1, input_size = -1)\n",
    "        # (5aa,1 sequence per batch, 10-dimension embedded vector)\n",
    "\n",
    "        #output of shape (seq_len, batch, num_directions * hidden_size):\n",
    "        out, (hn, cn) = self.lstm(embedding_tensor.view(len(seq), 1, -1))\n",
    "        # decoded_space = self.fc(out.view(len(seq), -1))\n",
    "        decoded_space = self.fc(out.view(len(seq), -1))\n",
    "        decoded_scores = F.log_softmax(decoded_space, dim=1)\n",
    "        return decoded_scores, hn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\n",
    "model = BRNN(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../../data/bidirectional_lstm_5_201008.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BRNN(\n",
       "  (log_softmax): LogSoftmax()\n",
       "  (aa_embedding): Embedding(21, 10)\n",
       "  (lstm): LSTM(10, 128, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed weight updates using motor_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.00\n",
      "Loss 0.02\n",
      "At Epoch: 1000.00\n",
      "Loss 0.01\n",
      "At Epoch: 2000.00\n",
      "Loss 0.01\n",
      "At Epoch: 3000.00\n",
      "Loss 0.04\n",
      "At Epoch: 4000.00\n",
      "Loss 0.01\n",
      "At Epoch: 5000.00\n",
      "Loss 0.01\n",
      "At Epoch: 6000.00\n",
      "Loss 0.01\n",
      "At Epoch: 7000.00\n",
      "Loss 0.01\n",
      "At Epoch: 8000.00\n",
      "Loss 0.02\n",
      "At Epoch: 9000.00\n",
      "Loss 0.02\n",
      "At Epoch: 10000.00\n",
      "Loss 0.03\n",
      "At Epoch: 11000.00\n",
      "Loss 0.01\n",
      "At Epoch: 12000.00\n",
      "Loss 0.01\n",
      "At Epoch: 13000.00\n",
      "Loss 0.01\n",
      "At Epoch: 14000.00\n",
      "Loss 0.01\n",
      "At Epoch: 15000.00\n",
      "Loss 0.03\n",
      "At Epoch: 16000.00\n",
      "Loss 0.02\n",
      "At Epoch: 17000.00\n",
      "Loss 0.02\n"
     ]
    }
   ],
   "source": [
    "#Train Network\n",
    "\n",
    "# loss_vector = []\n",
    "running_loss = 0\n",
    "print_every = 1000\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    \n",
    "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "    # Tensors of word indices.\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "    \n",
    "    sentence_in = sentence_in.to(device = device)\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    # Step 3. Run our forward pass.\n",
    "    model.zero_grad()\n",
    "    aa_scores, hn = model(sentence_in)\n",
    "\n",
    "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "    #  calling optimizer.step()\n",
    "\n",
    "    loss = loss_function(aa_scores, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "      print(f\"At Epoch: %.2f\"% epoch)\n",
    "      print(f\"Loss %.2f\"% loss)\n",
    "    # Print current loss    \n",
    "#     loss_vector.append(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/evotune_lstm_5_balanced.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed weight updates using the entire pfam_motor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.00\n",
      "Loss 0.01\n",
      "At Epoch: 1000.00\n",
      "Loss 0.00\n",
      "At Epoch: 2000.00\n",
      "Loss 0.02\n",
      "At Epoch: 3000.00\n",
      "Loss 0.02\n",
      "At Epoch: 4000.00\n",
      "Loss 0.00\n",
      "At Epoch: 5000.00\n",
      "Loss 0.01\n",
      "At Epoch: 6000.00\n",
      "Loss 0.00\n",
      "At Epoch: 7000.00\n",
      "Loss 0.01\n",
      "At Epoch: 8000.00\n",
      "Loss 0.01\n",
      "At Epoch: 9000.00\n",
      "Loss 0.00\n",
      "At Epoch: 10000.00\n",
      "Loss 0.01\n",
      "At Epoch: 11000.00\n",
      "Loss 0.00\n",
      "At Epoch: 12000.00\n",
      "Loss 0.04\n",
      "At Epoch: 13000.00\n",
      "Loss 0.00\n",
      "At Epoch: 14000.00\n",
      "Loss 0.06\n",
      "At Epoch: 15000.00\n",
      "Loss 0.04\n",
      "At Epoch: 16000.00\n",
      "Loss 0.08\n",
      "At Epoch: 17000.00\n",
      "Loss 0.04\n",
      "At Epoch: 18000.00\n",
      "Loss 0.00\n",
      "At Epoch: 19000.00\n",
      "Loss 0.00\n",
      "At Epoch: 20000.00\n",
      "Loss 0.00\n",
      "At Epoch: 21000.00\n",
      "Loss 0.03\n",
      "At Epoch: 22000.00\n",
      "Loss 0.03\n",
      "At Epoch: 23000.00\n",
      "Loss 0.00\n",
      "At Epoch: 24000.00\n",
      "Loss 0.01\n",
      "At Epoch: 25000.00\n",
      "Loss 0.00\n",
      "At Epoch: 26000.00\n",
      "Loss 0.01\n",
      "At Epoch: 27000.00\n",
      "Loss 0.01\n",
      "At Epoch: 28000.00\n",
      "Loss 0.01\n",
      "At Epoch: 29000.00\n",
      "Loss 0.00\n",
      "At Epoch: 30000.00\n",
      "Loss 0.01\n",
      "At Epoch: 31000.00\n",
      "Loss 0.02\n",
      "At Epoch: 32000.00\n",
      "Loss 0.00\n",
      "At Epoch: 33000.00\n",
      "Loss 0.01\n",
      "At Epoch: 34000.00\n",
      "Loss 0.01\n",
      "At Epoch: 35000.00\n",
      "Loss 0.02\n",
      "At Epoch: 36000.00\n",
      "Loss 0.00\n",
      "At Epoch: 37000.00\n",
      "Loss 0.01\n",
      "At Epoch: 38000.00\n",
      "Loss 0.01\n",
      "At Epoch: 39000.00\n",
      "Loss 0.01\n",
      "At Epoch: 40000.00\n",
      "Loss 0.01\n",
      "At Epoch: 41000.00\n",
      "Loss 0.00\n",
      "At Epoch: 42000.00\n",
      "Loss 0.00\n",
      "At Epoch: 43000.00\n",
      "Loss 0.01\n",
      "At Epoch: 44000.00\n",
      "Loss 0.01\n",
      "At Epoch: 45000.00\n",
      "Loss 0.00\n",
      "At Epoch: 46000.00\n",
      "Loss 0.01\n",
      "At Epoch: 47000.00\n",
      "Loss 0.02\n",
      "At Epoch: 48000.00\n",
      "Loss 0.02\n",
      "At Epoch: 49000.00\n",
      "Loss 0.01\n",
      "At Epoch: 50000.00\n",
      "Loss 0.01\n",
      "At Epoch: 51000.00\n",
      "Loss 0.01\n",
      "At Epoch: 52000.00\n",
      "Loss 0.00\n",
      "At Epoch: 53000.00\n",
      "Loss 0.00\n",
      "At Epoch: 54000.00\n",
      "Loss 0.00\n",
      "At Epoch: 55000.00\n",
      "Loss 0.00\n",
      "At Epoch: 56000.00\n",
      "Loss 0.04\n",
      "At Epoch: 57000.00\n",
      "Loss 0.00\n",
      "At Epoch: 58000.00\n",
      "Loss 0.00\n",
      "At Epoch: 59000.00\n",
      "Loss 0.01\n"
     ]
    }
   ],
   "source": [
    "#Train Network\n",
    "\n",
    "# loss_vector = []\n",
    "running_loss = 0\n",
    "print_every = 1000\n",
    "\n",
    "for epoch in np.arange(0, pfamA_target.shape[0]): \n",
    "    seq = pfamA_target.iloc[epoch, 3]\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    \n",
    "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "    # Tensors of word indices.\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "    \n",
    "    sentence_in = sentence_in.to(device = device)\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    # Step 3. Run our forward pass.\n",
    "    model.zero_grad()\n",
    "    aa_scores, hn = model(sentence_in)\n",
    "\n",
    "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "    #  calling optimizer.step()\n",
    "\n",
    "    loss = loss_function(aa_scores, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "      print(f\"At Epoch: %.2f\"% epoch)\n",
    "      print(f\"Loss %.2f\"% loss)\n",
    "    # Print current loss    \n",
    "#     loss_vector.append(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/evotune_lstm_5_balanced_target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
