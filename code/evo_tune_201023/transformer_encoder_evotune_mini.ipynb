{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179519      179519\n",
      "1414859    1414859\n",
      "12920        12920\n",
      "1415258    1415258\n",
      "13385        13385\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(18000, 6)\n",
      "13493    180756\n",
      "1539     166414\n",
      "2688     131988\n",
      "1691      37094\n",
      "188      130155\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(59149, 6)\n"
     ]
    }
   ],
   "source": [
    "pfamA_motors = pd.read_csv(\"../../data/pfamA_motors.csv\")\n",
    "df_dev = pd.read_csv(\"../../data/df_dev.csv\")\n",
    "motor_toolkit = pd.read_csv(\"../../data/motor_tookits.csv\")\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors.groupby('clan').apply(lambda _df: _df.sample(4500,random_state=1))\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.apply(lambda x: x.reset_index(drop = True))\n",
    "\n",
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "\n",
    "pfamA_target = pfamA_motors.loc[pfamA_motors[\"pfamA_acc\"].isin(pfamA_target_name),:]\n",
    "\n",
    "\n",
    "# shuffle pfamA_target and pfamA_motors_balanced\n",
    "pfamA_target = pfamA_target.sample(frac = 1)\n",
    "pfamA_target_ind = pfamA_target.iloc[:,0]\n",
    "print(pfamA_target_ind[0:5])\n",
    "print(pfamA_motors_balanced.shape)\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.sample(frac = 1) \n",
    "pfamA_motors_balanced_ind = pfamA_motors_balanced.iloc[:,0]\n",
    "print(pfamA_motors_balanced_ind[0:5])\n",
    "print(pfamA_target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[0:-1],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_labels(seq):\n",
    "    idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_eval(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "prepare_labels('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding module injects some information about the relative or absolute position of\n",
    "    the tokens in the sequence. The positional encodings have the same dimension as the embeddings \n",
    "    so that the two can be summed. Here, we use sine and cosine functions of different frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         print(\"x.size() : \", x.size())\n",
    "#         print(\"self.pe.size() :\", self.pe[:x.size(0),:,:].size())\n",
    "        x = torch.add(x ,Variable(self.pe[:x.size(0),:,:], requires_grad=False))\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.size(0):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "            self.src_mask = mask\n",
    "#         print(\"src.device: \", src.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "#         print(\"self.encoder(src) size: \", src.size())\n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"elf.pos_encoder(src) size: \", src.size())\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "#         print(\"output size: \", output.size())\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "ntokens = len(aminoacid_list) + 1 # the size of vocabulary\n",
    "emsize = 12 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 12 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(21, 12)\n",
       "  (decoder): Linear(in_features=12, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train() # Turn on the train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 3.0078\n",
      "time elapsed 0.0631\n",
      "At Epoch: 1000.0\n",
      "Loss 2.9869\n",
      "time elapsed 59.0081\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8501\n",
      "time elapsed 118.0856\n",
      "At Epoch: 3000.0\n",
      "Loss 2.7247\n",
      "time elapsed 176.6534\n",
      "At Epoch: 4000.0\n",
      "Loss 2.6230\n",
      "time elapsed 234.9085\n",
      "At Epoch: 5000.0\n",
      "Loss 2.8462\n",
      "time elapsed 293.7515\n",
      "At Epoch: 6000.0\n",
      "Loss 2.8804\n",
      "time elapsed 350.8426\n",
      "At Epoch: 7000.0\n",
      "Loss 2.8472\n",
      "time elapsed 407.4885\n",
      "At Epoch: 8000.0\n",
      "Loss 2.8552\n",
      "time elapsed 467.4006\n",
      "At Epoch: 9000.0\n",
      "Loss 2.7410\n",
      "time elapsed 527.1411\n",
      "At Epoch: 10000.0\n",
      "Loss 2.7857\n",
      "time elapsed 585.3429\n",
      "At Epoch: 11000.0\n",
      "Loss 2.6961\n",
      "time elapsed 642.6315\n",
      "At Epoch: 12000.0\n",
      "Loss 2.9467\n",
      "time elapsed 700.4089\n",
      "At Epoch: 13000.0\n",
      "Loss 2.7536\n",
      "time elapsed 757.9138\n",
      "At Epoch: 14000.0\n",
      "Loss 2.8088\n",
      "time elapsed 815.7969\n",
      "At Epoch: 15000.0\n",
      "Loss 2.8467\n",
      "time elapsed 874.5147\n",
      "At Epoch: 16000.0\n",
      "Loss 2.7928\n",
      "time elapsed 931.9800\n",
      "At Epoch: 17000.0\n",
      "Loss 2.9131\n",
      "time elapsed 989.7155\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "    \n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(sentence_in)\n",
    "    \n",
    "#     print(\"targets size: \", targets.size())\n",
    "    loss = criterion(output.view(-1, ntokens), targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_201025.pt\")\n",
    "#     loss_vector.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/mini_transformerencoder_balanced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.8283\n",
      "time elapsed 0.0449\n",
      "At Epoch: 1000.0\n",
      "Loss 2.9020\n",
      "time elapsed 70.9264\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8652\n",
      "time elapsed 142.2189\n",
      "At Epoch: 3000.0\n",
      "Loss 3.0131\n",
      "time elapsed 211.4416\n",
      "At Epoch: 4000.0\n",
      "Loss 2.8506\n",
      "time elapsed 283.2010\n",
      "At Epoch: 5000.0\n",
      "Loss 2.8749\n",
      "time elapsed 354.7739\n",
      "At Epoch: 6000.0\n",
      "Loss 2.8948\n",
      "time elapsed 424.6255\n",
      "At Epoch: 7000.0\n",
      "Loss 2.9095\n",
      "time elapsed 496.9839\n",
      "At Epoch: 8000.0\n",
      "Loss 2.9123\n",
      "time elapsed 567.9382\n",
      "At Epoch: 9000.0\n",
      "Loss 2.9087\n",
      "time elapsed 640.4164\n",
      "At Epoch: 10000.0\n",
      "Loss 2.9523\n",
      "time elapsed 712.1103\n",
      "At Epoch: 11000.0\n",
      "Loss 2.8718\n",
      "time elapsed 785.6764\n",
      "At Epoch: 12000.0\n",
      "Loss 2.9717\n",
      "time elapsed 858.2953\n",
      "At Epoch: 13000.0\n",
      "Loss 2.8197\n",
      "time elapsed 931.6730\n",
      "At Epoch: 14000.0\n",
      "Loss 2.8069\n",
      "time elapsed 1003.2191\n",
      "At Epoch: 15000.0\n",
      "Loss 2.8856\n",
      "time elapsed 1074.0346\n",
      "At Epoch: 16000.0\n",
      "Loss 2.9269\n",
      "time elapsed 1144.7228\n",
      "At Epoch: 17000.0\n",
      "Loss 2.7843\n",
      "time elapsed 1215.4855\n",
      "At Epoch: 18000.0\n",
      "Loss 2.8091\n",
      "time elapsed 1286.5096\n",
      "At Epoch: 19000.0\n",
      "Loss 2.8971\n",
      "time elapsed 1357.7200\n",
      "At Epoch: 20000.0\n",
      "Loss 2.9718\n",
      "time elapsed 1428.6084\n",
      "At Epoch: 21000.0\n",
      "Loss 2.8662\n",
      "time elapsed 1500.4349\n",
      "At Epoch: 22000.0\n",
      "Loss 2.8315\n",
      "time elapsed 1570.8596\n",
      "At Epoch: 23000.0\n",
      "Loss 2.8725\n",
      "time elapsed 1640.3621\n",
      "At Epoch: 24000.0\n",
      "Loss 2.8970\n",
      "time elapsed 1710.2859\n",
      "At Epoch: 25000.0\n",
      "Loss 2.9833\n",
      "time elapsed 1781.6462\n",
      "At Epoch: 26000.0\n",
      "Loss 2.8359\n",
      "time elapsed 1852.4087\n",
      "At Epoch: 27000.0\n",
      "Loss 2.9122\n",
      "time elapsed 1919.7979\n",
      "At Epoch: 28000.0\n",
      "Loss 2.8665\n",
      "time elapsed 1971.9492\n",
      "At Epoch: 29000.0\n",
      "Loss 2.8947\n",
      "time elapsed 2025.6547\n",
      "At Epoch: 30000.0\n",
      "Loss 2.8834\n",
      "time elapsed 2079.6459\n",
      "At Epoch: 31000.0\n",
      "Loss 2.8847\n",
      "time elapsed 2132.8055\n",
      "At Epoch: 32000.0\n",
      "Loss 2.8732\n",
      "time elapsed 2186.8153\n",
      "At Epoch: 33000.0\n",
      "Loss 2.8282\n",
      "time elapsed 2241.0949\n",
      "At Epoch: 34000.0\n",
      "Loss 2.8050\n",
      "time elapsed 2293.4643\n",
      "At Epoch: 35000.0\n",
      "Loss 2.8753\n",
      "time elapsed 2347.8888\n",
      "At Epoch: 36000.0\n",
      "Loss 2.9144\n",
      "time elapsed 2400.6956\n",
      "At Epoch: 37000.0\n",
      "Loss 2.9702\n",
      "time elapsed 2454.9261\n",
      "At Epoch: 38000.0\n",
      "Loss 2.9528\n",
      "time elapsed 2510.2639\n",
      "At Epoch: 39000.0\n",
      "Loss 2.8189\n",
      "time elapsed 2565.1147\n",
      "At Epoch: 40000.0\n",
      "Loss 2.7682\n",
      "time elapsed 2619.8280\n",
      "At Epoch: 41000.0\n",
      "Loss 2.9174\n",
      "time elapsed 2672.7613\n",
      "At Epoch: 42000.0\n",
      "Loss 2.7764\n",
      "time elapsed 2725.5157\n",
      "At Epoch: 43000.0\n",
      "Loss 2.9065\n",
      "time elapsed 2780.3920\n",
      "At Epoch: 44000.0\n",
      "Loss 2.8429\n",
      "time elapsed 2834.3931\n",
      "At Epoch: 45000.0\n",
      "Loss 2.8784\n",
      "time elapsed 2886.7607\n",
      "At Epoch: 46000.0\n",
      "Loss 2.7878\n",
      "time elapsed 2940.3051\n",
      "At Epoch: 47000.0\n",
      "Loss 2.9209\n",
      "time elapsed 2994.8507\n",
      "At Epoch: 48000.0\n",
      "Loss 2.7945\n",
      "time elapsed 3050.5908\n",
      "At Epoch: 49000.0\n",
      "Loss 2.8543\n",
      "time elapsed 3104.4715\n",
      "At Epoch: 50000.0\n",
      "Loss 2.8821\n",
      "time elapsed 3158.6844\n",
      "At Epoch: 51000.0\n",
      "Loss 2.8742\n",
      "time elapsed 3209.9032\n",
      "At Epoch: 52000.0\n",
      "Loss 2.9363\n",
      "time elapsed 3263.9240\n",
      "At Epoch: 53000.0\n",
      "Loss 2.9268\n",
      "time elapsed 3316.7977\n",
      "At Epoch: 54000.0\n",
      "Loss 2.9107\n",
      "time elapsed 3370.0062\n",
      "At Epoch: 55000.0\n",
      "Loss 2.9149\n",
      "time elapsed 3424.5844\n",
      "At Epoch: 56000.0\n",
      "Loss 2.9084\n",
      "time elapsed 3478.0063\n",
      "At Epoch: 57000.0\n",
      "Loss 2.9304\n",
      "time elapsed 3531.4380\n",
      "At Epoch: 58000.0\n",
      "Loss 2.9035\n",
      "time elapsed 3584.3857\n",
      "At Epoch: 59000.0\n",
      "Loss 2.9368\n",
      "time elapsed 3638.8192\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_target.shape[0]): \n",
    "    seq = pfamA_target.iloc[epoch, 3]\n",
    "    \n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(sentence_in)\n",
    "    \n",
    "#     print(\"targets size: \", targets.size())\n",
    "    loss = criterion(output.view(-1, ntokens), targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_201025.pt\")\n",
    "#     loss_vector.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/mini_transformerencoder_balanced_target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
