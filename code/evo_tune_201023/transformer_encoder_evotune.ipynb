{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179519      179519\n",
      "1414859    1414859\n",
      "12920        12920\n",
      "1415258    1415258\n",
      "13385        13385\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(18000, 6)\n",
      "13493    180756\n",
      "1539     166414\n",
      "2688     131988\n",
      "1691      37094\n",
      "188      130155\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(59149, 6)\n"
     ]
    }
   ],
   "source": [
    "pfamA_motors = pd.read_csv(\"../../data/pfamA_motors.csv\")\n",
    "df_dev = pd.read_csv(\"../../data/df_dev.csv\")\n",
    "motor_toolkit = pd.read_csv(\"../../data/motor_tookits.csv\")\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors.groupby('clan').apply(lambda _df: _df.sample(4500,random_state=1))\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.apply(lambda x: x.reset_index(drop = True))\n",
    "\n",
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "\n",
    "pfamA_target = pfamA_motors.loc[pfamA_motors[\"pfamA_acc\"].isin(pfamA_target_name),:]\n",
    "\n",
    "\n",
    "# shuffle pfamA_target and pfamA_motors_balanced\n",
    "pfamA_target = pfamA_target.sample(frac = 1)\n",
    "pfamA_target_ind = pfamA_target.iloc[:,0]\n",
    "print(pfamA_target_ind[0:5])\n",
    "print(pfamA_motors_balanced.shape)\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.sample(frac = 1) \n",
    "pfamA_motors_balanced_ind = pfamA_motors_balanced.iloc[:,0]\n",
    "print(pfamA_motors_balanced_ind[0:5])\n",
    "print(pfamA_target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[0:-1],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_labels(seq):\n",
    "    idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_eval(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "prepare_labels('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding module injects some information about the relative or absolute position of\n",
    "    the tokens in the sequence. The positional encodings have the same dimension as the embeddings \n",
    "    so that the two can be summed. Here, we use sine and cosine functions of different frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         print(\"x.size() : \", x.size())\n",
    "#         print(\"self.pe.size() :\", self.pe[:x.size(0),:,:].size())\n",
    "        x = torch.add(x ,Variable(self.pe[:x.size(0),:,:], requires_grad=False))\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.size(0):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "            self.src_mask = mask\n",
    "#         print(\"src.device: \", src.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "#         print(\"self.encoder(src) size: \", src.size())\n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"elf.pos_encoder(src) size: \", src.size())\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "#         print(\"output size: \", output.size())\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "ntokens = len(aminoacid_list) + 1 # the size of vocabulary\n",
    "emsize = 12 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 12 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trained weights on df_dev\n",
    "model.load_state_dict(torch.load(\"../../data/transformer_encoder_201025.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(21, 12)\n",
       "  (decoder): Linear(in_features=12, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train() # Turn on the train mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed weight updates using motor_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.9313\n",
      "time elapsed 0.0462\n",
      "At Epoch: 1000.0\n",
      "Loss 2.9778\n",
      "time elapsed 19.2431\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8527\n",
      "time elapsed 38.5000\n",
      "At Epoch: 3000.0\n",
      "Loss 2.7362\n",
      "time elapsed 57.9419\n",
      "At Epoch: 4000.0\n",
      "Loss 2.7080\n",
      "time elapsed 77.5878\n",
      "At Epoch: 5000.0\n",
      "Loss 2.8469\n",
      "time elapsed 97.5463\n",
      "At Epoch: 6000.0\n",
      "Loss 2.8770\n",
      "time elapsed 117.0020\n",
      "At Epoch: 7000.0\n",
      "Loss 2.8420\n",
      "time elapsed 136.6154\n",
      "At Epoch: 8000.0\n",
      "Loss 2.8524\n",
      "time elapsed 156.3122\n",
      "At Epoch: 9000.0\n",
      "Loss 2.7866\n",
      "time elapsed 175.9095\n",
      "At Epoch: 10000.0\n",
      "Loss 2.8042\n",
      "time elapsed 199.8341\n",
      "At Epoch: 11000.0\n",
      "Loss 2.7702\n",
      "time elapsed 238.7344\n",
      "At Epoch: 12000.0\n",
      "Loss 2.9439\n",
      "time elapsed 283.1732\n",
      "At Epoch: 13000.0\n",
      "Loss 2.7778\n",
      "time elapsed 327.0251\n",
      "At Epoch: 14000.0\n",
      "Loss 2.8029\n",
      "time elapsed 371.7587\n",
      "At Epoch: 15000.0\n",
      "Loss 2.8372\n",
      "time elapsed 417.0790\n",
      "At Epoch: 16000.0\n",
      "Loss 2.8199\n",
      "time elapsed 463.3373\n",
      "At Epoch: 17000.0\n",
      "Loss 2.9194\n",
      "time elapsed 510.2089\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "    \n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(sentence_in)\n",
    "    \n",
    "#     print(\"targets size: \", targets.size())\n",
    "    loss = criterion(output.view(-1, ntokens), targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_201025.pt\")\n",
    "#     loss_vector.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/evotune_transformerencoder_balanced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed weight updates using the entire pfam_motor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.8420\n",
      "time elapsed 0.0300\n",
      "At Epoch: 1000.0\n",
      "Loss 2.9051\n",
      "time elapsed 53.5168\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8652\n",
      "time elapsed 110.0196\n",
      "At Epoch: 3000.0\n",
      "Loss 2.9730\n",
      "time elapsed 164.7915\n",
      "At Epoch: 4000.0\n",
      "Loss 2.8525\n",
      "time elapsed 220.6136\n",
      "At Epoch: 5000.0\n",
      "Loss 2.8726\n",
      "time elapsed 275.8973\n",
      "At Epoch: 6000.0\n",
      "Loss 2.8937\n",
      "time elapsed 328.3026\n",
      "At Epoch: 7000.0\n",
      "Loss 2.9131\n",
      "time elapsed 381.6925\n",
      "At Epoch: 8000.0\n",
      "Loss 2.9119\n",
      "time elapsed 433.0885\n",
      "At Epoch: 9000.0\n",
      "Loss 2.9055\n",
      "time elapsed 486.7597\n",
      "At Epoch: 10000.0\n",
      "Loss 2.9505\n",
      "time elapsed 542.0930\n",
      "At Epoch: 11000.0\n",
      "Loss 2.8767\n",
      "time elapsed 596.7346\n",
      "At Epoch: 12000.0\n",
      "Loss 2.9729\n",
      "time elapsed 651.6664\n",
      "At Epoch: 13000.0\n",
      "Loss 2.8220\n",
      "time elapsed 704.6541\n",
      "At Epoch: 14000.0\n",
      "Loss 2.8093\n",
      "time elapsed 758.7925\n",
      "At Epoch: 15000.0\n",
      "Loss 2.8726\n",
      "time elapsed 780.7511\n",
      "At Epoch: 16000.0\n",
      "Loss 2.9226\n",
      "time elapsed 799.7534\n",
      "At Epoch: 17000.0\n",
      "Loss 2.7995\n",
      "time elapsed 818.7104\n",
      "At Epoch: 18000.0\n",
      "Loss 2.8208\n",
      "time elapsed 837.6963\n",
      "At Epoch: 19000.0\n",
      "Loss 2.8996\n",
      "time elapsed 856.5695\n",
      "At Epoch: 20000.0\n",
      "Loss 2.9344\n",
      "time elapsed 875.3155\n",
      "At Epoch: 21000.0\n",
      "Loss 2.8507\n",
      "time elapsed 894.0216\n",
      "At Epoch: 22000.0\n",
      "Loss 2.8296\n",
      "time elapsed 912.9167\n",
      "At Epoch: 23000.0\n",
      "Loss 2.8659\n",
      "time elapsed 931.8268\n",
      "At Epoch: 24000.0\n",
      "Loss 2.8971\n",
      "time elapsed 950.5472\n",
      "At Epoch: 25000.0\n",
      "Loss 2.9987\n",
      "time elapsed 969.5393\n",
      "At Epoch: 26000.0\n",
      "Loss 2.8276\n",
      "time elapsed 988.3727\n",
      "At Epoch: 27000.0\n",
      "Loss 2.9119\n",
      "time elapsed 1007.2470\n",
      "At Epoch: 28000.0\n",
      "Loss 2.8670\n",
      "time elapsed 1026.0612\n",
      "At Epoch: 29000.0\n",
      "Loss 2.9030\n",
      "time elapsed 1044.7567\n",
      "At Epoch: 30000.0\n",
      "Loss 2.8866\n",
      "time elapsed 1063.5052\n",
      "At Epoch: 31000.0\n",
      "Loss 2.8754\n",
      "time elapsed 1082.1544\n",
      "At Epoch: 32000.0\n",
      "Loss 2.8801\n",
      "time elapsed 1100.8125\n",
      "At Epoch: 33000.0\n",
      "Loss 2.8351\n",
      "time elapsed 1119.4526\n",
      "At Epoch: 34000.0\n",
      "Loss 2.8158\n",
      "time elapsed 1138.1276\n",
      "At Epoch: 35000.0\n",
      "Loss 2.8774\n",
      "time elapsed 1156.8358\n",
      "At Epoch: 36000.0\n",
      "Loss 2.9160\n",
      "time elapsed 1175.6165\n",
      "At Epoch: 37000.0\n",
      "Loss 2.9726\n",
      "time elapsed 1194.4700\n",
      "At Epoch: 38000.0\n",
      "Loss 2.9533\n",
      "time elapsed 1213.3509\n",
      "At Epoch: 39000.0\n",
      "Loss 2.8171\n",
      "time elapsed 1232.2419\n",
      "At Epoch: 40000.0\n",
      "Loss 2.7657\n",
      "time elapsed 1251.0633\n",
      "At Epoch: 41000.0\n",
      "Loss 2.9179\n",
      "time elapsed 1269.7473\n",
      "At Epoch: 42000.0\n",
      "Loss 2.7715\n",
      "time elapsed 1288.3995\n",
      "At Epoch: 43000.0\n",
      "Loss 2.9088\n",
      "time elapsed 1307.0536\n",
      "At Epoch: 44000.0\n",
      "Loss 2.8451\n",
      "time elapsed 1325.7071\n",
      "At Epoch: 45000.0\n",
      "Loss 2.8789\n",
      "time elapsed 1344.5948\n",
      "At Epoch: 46000.0\n",
      "Loss 2.7862\n",
      "time elapsed 1363.4877\n",
      "At Epoch: 47000.0\n",
      "Loss 2.9269\n",
      "time elapsed 1382.2789\n",
      "At Epoch: 48000.0\n",
      "Loss 2.7932\n",
      "time elapsed 1401.1098\n",
      "At Epoch: 49000.0\n",
      "Loss 2.8578\n",
      "time elapsed 1419.7837\n",
      "At Epoch: 50000.0\n",
      "Loss 2.8840\n",
      "time elapsed 1438.5795\n",
      "At Epoch: 51000.0\n",
      "Loss 2.8731\n",
      "time elapsed 1457.3342\n",
      "At Epoch: 52000.0\n",
      "Loss 2.9366\n",
      "time elapsed 1476.3099\n",
      "At Epoch: 53000.0\n",
      "Loss 2.9268\n",
      "time elapsed 1495.3007\n",
      "At Epoch: 54000.0\n",
      "Loss 2.9130\n",
      "time elapsed 1514.4526\n",
      "At Epoch: 55000.0\n",
      "Loss 2.9160\n",
      "time elapsed 1534.5350\n",
      "At Epoch: 56000.0\n",
      "Loss 2.9031\n",
      "time elapsed 1554.1357\n",
      "At Epoch: 57000.0\n",
      "Loss 2.9299\n",
      "time elapsed 1574.0187\n",
      "At Epoch: 58000.0\n",
      "Loss 2.9047\n",
      "time elapsed 1593.3675\n",
      "At Epoch: 59000.0\n",
      "Loss 2.9357\n",
      "time elapsed 1612.4034\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_target.shape[0]): \n",
    "    seq = pfamA_target.iloc[epoch, 3]\n",
    "    \n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(sentence_in)\n",
    "    \n",
    "#     print(\"targets size: \", targets.size())\n",
    "    loss = criterion(output.view(-1, ntokens), targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_201025.pt\")\n",
    "#     loss_vector.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/evotune_transformerencoder_balanced_target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
