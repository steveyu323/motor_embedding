{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "> The following code adopted from transformer_encoder_ver2 (a smaller embedding and FDN with the hope of getting less concentrated features). It further exapanded by applying a masked language model. \n",
    "\n",
    "> In the previous model, a mask is applied so that at each word, it sees only the word prior to it. (When predicting #2 word, it sees only #1 word, and all rest are masked to -INF), so that the model follows an auto-regressive manner\n",
    "\n",
    "> In the MLM setting, certain proportion of the sentense is randomly masked (15% in BERT), and they are masked throughout the training process. The loss is only on those positions's correctness. In the TransformerEncoderLayer, the mask pass in should be changed and should mask the position of mask to -INF while keeping all the rest to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179519      179519\n",
      "1414859    1414859\n",
      "12920        12920\n",
      "1415258    1415258\n",
      "13385        13385\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(18000, 6)\n",
      "13493    180756\n",
      "1539     166414\n",
      "2688     131988\n",
      "1691      37094\n",
      "188      130155\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "(59149, 6)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "pfamA_motors = pd.read_csv(\"../../data/pfamA_motors.csv\")\n",
    "df_dev = pd.read_csv(\"../../data/df_dev.csv\")\n",
    "motor_toolkit = pd.read_csv(\"../../data/motor_tookits.csv\")\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors.groupby('clan').apply(lambda _df: _df.sample(4500,random_state=1))\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.apply(lambda x: x.reset_index(drop = True))\n",
    "\n",
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "\n",
    "pfamA_target = pfamA_motors.loc[pfamA_motors[\"pfamA_acc\"].isin(pfamA_target_name),:]\n",
    "\n",
    "\n",
    "# shuffle pfamA_target and pfamA_motors_balanced\n",
    "pfamA_target = pfamA_target.sample(frac = 1)\n",
    "pfamA_target_ind = pfamA_target.iloc[:,0]\n",
    "print(pfamA_target_ind[0:5])\n",
    "print(pfamA_motors_balanced.shape)\n",
    "\n",
    "pfamA_motors_balanced = pfamA_motors_balanced.sample(frac = 1) \n",
    "pfamA_motors_balanced_ind = pfamA_motors_balanced.iloc[:,0]\n",
    "print(pfamA_motors_balanced_ind[0:5])\n",
    "print(pfamA_target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20,  2,  7,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# def prepare_labels(seq):\n",
    "#     idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_eval(seq):\n",
    "    idxs = word_to_index(seq[:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "prepare_sequence('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding module injects some information about the relative or absolute position of\n",
    "    the tokens in the sequence. The positional encodings have the same dimension as the embeddings \n",
    "    so that the two can be summed. Here, we use sine and cosine functions of different frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         print(\"x.size() : \", x.size())\n",
    "#         print(\"self.pe.size() :\", self.pe[:x.size(0),:,:].size())\n",
    "        x = torch.add(x ,Variable(self.pe[:x.size(0),:,:], requires_grad=False))\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout,activation='gelu')\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def _generate_square_mlm_mask(self, sz, mask_frac=0.15):\n",
    "        # 0's are the masked position\n",
    "        zeros_num = int(sz * mask_frac)\n",
    "        ones_num = sz - zeros_num\n",
    "        lm_mask = torch.cat([torch.zeros(zeros_num), torch.ones(ones_num)])\n",
    "        lm_mask = lm_mask[torch.randperm(sz)]\n",
    "        masked_ind = lm_mask.eq(0)\n",
    "        lm_mask = lm_mask.repeat(sz, 1)\n",
    "        mask = lm_mask.float().masked_fill(lm_mask == 0, float('-inf')).masked_fill(lm_mask == 1, float(0.0))\n",
    "        return mask,masked_ind\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if model.training:\n",
    "            device = src.device\n",
    "            mask,masked_ind = self._generate_square_mlm_mask(src.size(0))\n",
    "            mask = mask.to(device)\n",
    "            self.src_mask = mask\n",
    "            self.src_mask_ind = masked_ind\n",
    "            \n",
    "#         print(\"src.device: \", src.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "#         print(\"self.encoder(src) size: \", src.size())\n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"elf.pos_encoder(src) size: \", src.size())\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "#         print(\"output size: \", output.size())\n",
    "        output = self.decoder(output)\n",
    "        return output, self.src_mask_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(aminoacid_list) + 1 # the size of vocabulary\n",
    "emsize = 12 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 12 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=12, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=12, bias=True)\n",
       "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(21, 12)\n",
       "  (decoder): Linear(in_features=12, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trained weights on df_dev\n",
    "model.load_state_dict(torch.load(\"../../data/201025/transformer_encoder_mlm_201025.pt\"))\n",
    "\n",
    "model.to(device)\n",
    "model.train() # Turn on the train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "torch.Size([20])\n",
      "torch.Size([20, 21])\n",
      "At Epoch: 0.0\n",
      "Loss 2.9410\n",
      "time elapsed 0.0358\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "    print(len(seq))\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output,mask_ind = model(sentence_in)\n",
    "#     print(mask_ind)\n",
    "    targets = sentence_in[mask_ind]\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "    print(targets.squeeze(1).size())\n",
    "    print(output[mask_ind].squeeze(1).size())\n",
    "\n",
    "    loss = criterion(output[mask_ind].squeeze(1), targets.squeeze(1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_201025.pt\")\n",
    "#     loss_vector.append(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.7067\n",
      "time elapsed 0.0246\n",
      "At Epoch: 1000.0\n",
      "Loss 2.9695\n",
      "time elapsed 21.2788\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8660\n",
      "time elapsed 42.4753\n",
      "At Epoch: 3000.0\n",
      "Loss 2.7525\n",
      "time elapsed 64.5147\n",
      "At Epoch: 4000.0\n",
      "Loss 2.7379\n",
      "time elapsed 86.3857\n",
      "At Epoch: 5000.0\n",
      "Loss 3.2984\n",
      "time elapsed 108.5590\n",
      "At Epoch: 6000.0\n",
      "Loss 2.9141\n",
      "time elapsed 130.7678\n",
      "At Epoch: 7000.0\n",
      "Loss 2.9873\n",
      "time elapsed 152.5417\n",
      "At Epoch: 8000.0\n",
      "Loss 3.0124\n",
      "time elapsed 174.4811\n",
      "At Epoch: 9000.0\n",
      "Loss 3.0226\n",
      "time elapsed 196.7575\n",
      "At Epoch: 10000.0\n",
      "Loss 2.7386\n",
      "time elapsed 218.5400\n",
      "At Epoch: 11000.0\n",
      "Loss 3.0671\n",
      "time elapsed 240.2965\n",
      "At Epoch: 12000.0\n",
      "Loss 3.0510\n",
      "time elapsed 262.0725\n",
      "At Epoch: 13000.0\n",
      "Loss 2.7768\n",
      "time elapsed 283.8791\n",
      "At Epoch: 14000.0\n",
      "Loss 2.8323\n",
      "time elapsed 305.5910\n",
      "At Epoch: 15000.0\n",
      "Loss 3.3245\n",
      "time elapsed 325.2611\n",
      "At Epoch: 16000.0\n",
      "Loss 3.0961\n",
      "time elapsed 344.8438\n",
      "At Epoch: 17000.0\n",
      "Loss 2.9534\n",
      "time elapsed 364.3605\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_motors_balanced.shape[0]): \n",
    "    seq = pfamA_motors_balanced.iloc[epoch, 3]\n",
    "#     print(len(seq))\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "#     print(sentence_in.size())\n",
    "    optimizer.zero_grad()\n",
    "    output,mask_ind = model(sentence_in)\n",
    "#     print(mask_ind)\n",
    "    targets = sentence_in[mask_ind]\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "#     print(targets.squeeze(1).size())\n",
    "#     print(output[mask_ind].squeeze(1).size())\n",
    "\n",
    "    loss = criterion(output[mask_ind].squeeze(1), targets.squeeze(1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_mlm_201025.pt\")\n",
    "#     loss_vector.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/201025/evotune_transformer_encoder_mlm_balanced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed weight updates using the entire pfam_motor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch: 0.0\n",
      "Loss 2.7140\n",
      "time elapsed 0.0274\n",
      "At Epoch: 1000.0\n",
      "Loss 2.9097\n",
      "time elapsed 21.9227\n",
      "At Epoch: 2000.0\n",
      "Loss 2.8480\n",
      "time elapsed 43.6692\n",
      "At Epoch: 3000.0\n",
      "Loss 3.1404\n",
      "time elapsed 65.3573\n",
      "At Epoch: 4000.0\n",
      "Loss 2.8393\n",
      "time elapsed 86.2651\n",
      "At Epoch: 5000.0\n",
      "Loss 2.8582\n",
      "time elapsed 108.4593\n",
      "At Epoch: 6000.0\n",
      "Loss 2.9257\n",
      "time elapsed 130.4809\n",
      "At Epoch: 7000.0\n",
      "Loss 3.0498\n",
      "time elapsed 152.5034\n",
      "At Epoch: 8000.0\n",
      "Loss 2.8899\n",
      "time elapsed 172.4534\n",
      "At Epoch: 9000.0\n",
      "Loss 2.8906\n",
      "time elapsed 194.4951\n",
      "At Epoch: 10000.0\n",
      "Loss 3.0192\n",
      "time elapsed 216.5674\n",
      "At Epoch: 11000.0\n",
      "Loss 2.9504\n",
      "time elapsed 237.8860\n",
      "At Epoch: 12000.0\n",
      "Loss 3.0292\n",
      "time elapsed 259.0785\n",
      "At Epoch: 13000.0\n",
      "Loss 2.7552\n",
      "time elapsed 281.0612\n",
      "At Epoch: 14000.0\n",
      "Loss 2.8100\n",
      "time elapsed 303.3530\n",
      "At Epoch: 15000.0\n",
      "Loss 2.5044\n",
      "time elapsed 325.3086\n",
      "At Epoch: 16000.0\n",
      "Loss 2.9062\n",
      "time elapsed 347.1907\n",
      "At Epoch: 17000.0\n",
      "Loss 2.7586\n",
      "time elapsed 369.2472\n",
      "At Epoch: 18000.0\n",
      "Loss 2.6688\n",
      "time elapsed 390.2338\n",
      "At Epoch: 19000.0\n",
      "Loss 2.9740\n",
      "time elapsed 411.4527\n",
      "At Epoch: 20000.0\n",
      "Loss 3.1020\n",
      "time elapsed 432.5937\n",
      "At Epoch: 21000.0\n",
      "Loss 2.9392\n",
      "time elapsed 453.6268\n",
      "At Epoch: 22000.0\n",
      "Loss 2.8523\n",
      "time elapsed 474.6757\n",
      "At Epoch: 23000.0\n",
      "Loss 3.0021\n",
      "time elapsed 495.8195\n",
      "At Epoch: 24000.0\n",
      "Loss 3.0805\n",
      "time elapsed 517.4716\n",
      "At Epoch: 25000.0\n",
      "Loss 2.9050\n",
      "time elapsed 539.6333\n",
      "At Epoch: 26000.0\n",
      "Loss 2.9102\n",
      "time elapsed 561.7667\n",
      "At Epoch: 27000.0\n",
      "Loss 2.8915\n",
      "time elapsed 583.5761\n",
      "At Epoch: 28000.0\n",
      "Loss 2.8671\n",
      "time elapsed 605.3760\n",
      "At Epoch: 29000.0\n",
      "Loss 2.9103\n",
      "time elapsed 626.5617\n",
      "At Epoch: 30000.0\n",
      "Loss 2.9577\n",
      "time elapsed 648.7179\n",
      "At Epoch: 31000.0\n",
      "Loss 2.9760\n",
      "time elapsed 670.1427\n",
      "At Epoch: 32000.0\n",
      "Loss 2.9898\n",
      "time elapsed 690.8565\n",
      "At Epoch: 33000.0\n",
      "Loss 2.9711\n",
      "time elapsed 711.9372\n",
      "At Epoch: 34000.0\n",
      "Loss 2.8728\n",
      "time elapsed 732.9498\n",
      "At Epoch: 35000.0\n",
      "Loss 2.9145\n",
      "time elapsed 753.9164\n",
      "At Epoch: 36000.0\n",
      "Loss 2.9364\n",
      "time elapsed 774.8700\n",
      "At Epoch: 37000.0\n",
      "Loss 3.2134\n",
      "time elapsed 795.8710\n",
      "At Epoch: 38000.0\n",
      "Loss 2.9287\n",
      "time elapsed 816.8169\n",
      "At Epoch: 39000.0\n",
      "Loss 2.9056\n",
      "time elapsed 837.8654\n",
      "At Epoch: 40000.0\n",
      "Loss 2.8824\n",
      "time elapsed 858.8210\n",
      "At Epoch: 41000.0\n",
      "Loss 3.1005\n",
      "time elapsed 880.7580\n",
      "At Epoch: 42000.0\n",
      "Loss 2.8571\n",
      "time elapsed 902.9349\n",
      "At Epoch: 43000.0\n",
      "Loss 2.9526\n",
      "time elapsed 925.9726\n",
      "At Epoch: 44000.0\n",
      "Loss 2.8038\n",
      "time elapsed 945.2703\n",
      "At Epoch: 45000.0\n",
      "Loss 2.9391\n",
      "time elapsed 966.8394\n",
      "At Epoch: 46000.0\n",
      "Loss 2.6897\n",
      "time elapsed 988.7634\n",
      "At Epoch: 47000.0\n",
      "Loss 2.8321\n",
      "time elapsed 1010.2805\n",
      "At Epoch: 48000.0\n",
      "Loss 2.7132\n",
      "time elapsed 1031.4124\n",
      "At Epoch: 49000.0\n",
      "Loss 3.1332\n",
      "time elapsed 1052.3964\n",
      "At Epoch: 50000.0\n",
      "Loss 3.0065\n",
      "time elapsed 1073.3693\n",
      "At Epoch: 51000.0\n",
      "Loss 2.8646\n",
      "time elapsed 1094.2970\n",
      "At Epoch: 52000.0\n",
      "Loss 2.9610\n",
      "time elapsed 1115.2873\n",
      "At Epoch: 53000.0\n",
      "Loss 2.9442\n",
      "time elapsed 1136.3033\n",
      "At Epoch: 54000.0\n",
      "Loss 2.9331\n",
      "time elapsed 1157.7534\n",
      "At Epoch: 55000.0\n",
      "Loss 2.9188\n",
      "time elapsed 1178.7325\n",
      "At Epoch: 56000.0\n",
      "Loss 2.9761\n",
      "time elapsed 1200.3566\n",
      "At Epoch: 57000.0\n",
      "Loss 2.9242\n",
      "time elapsed 1220.7859\n",
      "At Epoch: 58000.0\n",
      "Loss 2.9202\n",
      "time elapsed 1242.7209\n",
      "At Epoch: 59000.0\n",
      "Loss 2.9086\n",
      "time elapsed 1264.4303\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_every = 1000\n",
    "# loss_vector = []\n",
    "\n",
    "for epoch in np.arange(0, pfamA_target.shape[0]): \n",
    "    seq = pfamA_target.iloc[epoch, 3]\n",
    "#     print(len(seq))\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "#     sentence_in = sentence_in.to(device = device)\n",
    "    sentence_in = sentence_in.unsqueeze(1).to(device = device)\n",
    "#     print(sentence_in.size())\n",
    "    optimizer.zero_grad()\n",
    "    output,mask_ind = model(sentence_in)\n",
    "#     print(mask_ind)\n",
    "    targets = sentence_in[mask_ind]\n",
    "    targets = targets.to(device = device)\n",
    "    \n",
    "#     print(targets.squeeze(1).size())\n",
    "#     print(output[mask_ind].squeeze(1).size())\n",
    "\n",
    "    loss = criterion(output[mask_ind].squeeze(1), targets.squeeze(1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"time elapsed %.4f\"% elapsed)\n",
    "#         torch.save(model.state_dict(), \"../../data/transformer_encoder_mlm_201025.pt\")\n",
    "#     loss_vector.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/201025/evotune_transformer_encoder_mlm_balanced_target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
