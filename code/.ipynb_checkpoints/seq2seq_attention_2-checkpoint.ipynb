{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description\n",
    "- Apply a Sequence to Sequence model with a Encoder RNN and a Decoder RNN. Parameters related to attention are learned and weighted input hidden states are summed for a final hidden state input for each step of decoding\n",
    "- train on Uniref59=0_01\n",
    "\n",
    "## Math and model formulation rerference:\n",
    "- https://arxiv.org/abs/1506.03134\n",
    "- https://arxiv.org/pdf/1409.0473.pdf\n",
    "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "## code reference:\n",
    "- https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "- https://github.com/omarsar/pytorch_neural_machine_translation_attention/blob/master/NMT_in_PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# seed = 7\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pfamA_motors = pd.read_csv(\"../data/pfamA_motors.csv\")\n",
    "# df_dev = pd.read_csv(\"../data/df_dev.csv\")\n",
    "# pfamA_motors = pfamA_motors.iloc[:,1:]\n",
    "# clan_train_dat = pfamA_motors.groupby(\"clan\").head(4000)\n",
    "# clan_train_dat = clan_train_dat.sample(frac=1).reset_index(drop=True)\n",
    "# clan_test_dat = pfamA_motors.loc[~pfamA_motors[\"id\"].isin(clan_train_dat[\"id\"]),:].groupby(\"clan\").head(400)\n",
    "uniref50_01 = pd.read_csv(\"../data/uniref50_01.tsv\",sep = \"\\t\",header=None)\n",
    "uniref50_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clan_train_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tup(dat):\n",
    "    data = []\n",
    "    for i in range(dat.shape[0]):\n",
    "        row = dat.iloc[i,:]\n",
    "        tup = (row[\"seq\"],row[\"clan\"])\n",
    "        data.append(tup)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clan_training_data = df_to_tup(clan_train_dat)\n",
    "clan_test_data = df_to_tup(clan_test_dat)\n",
    "for seq,clan in clan_training_data:\n",
    "    print(seq)\n",
    "    print(clan)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminoacid_list = [\n",
    "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "clan_list = [\"actin_like\",\"tubulin_c\",\"tubulin_binding\",\"p_loop_gtpase\"]\n",
    "        \n",
    "aa_to_ix = dict(zip(aminoacid_list, np.arange(1, 21)))\n",
    "clan_to_ix = dict(zip(clan_list, np.arange(0, 4)))\n",
    "\n",
    "def word_to_index(seq,to_ix):\n",
    "    \"Returns a list of indices (integers) from a list of words.\"\n",
    "    return [to_ix.get(word, 0) for word in seq]\n",
    "\n",
    "ix_to_aa = dict(zip(np.arange(1, 21), aminoacid_list))\n",
    "ix_to_clan = dict(zip(np.arange(0, 4), clan_list))\n",
    "\n",
    "def index_to_word(ixs,ix_to): \n",
    "    \"Returns a list of words, given a list of their corresponding indices.\"\n",
    "    return [ix_to.get(ix, 'X') for ix in ixs]\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = word_to_index(seq[0:-1],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_labels(seq):\n",
    "    idxs = word_to_index(seq[1:],aa_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_labels('YCHXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size, batch_first=False, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.log_softmax = nn.LogSoftmax(dim= 1)\n",
    "        self.batch_first = batch_first\n",
    "        self.aa_embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                           batch_first=batch_first, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # embed each aa to the embedded space\n",
    "        embedding_tensor = self.aa_embedding(seq)\n",
    "        #output of shape (seq_len, batch, num_directions * hidden_size):\n",
    "        outputs, hidden = self.rnn(embedding_tensor.view(len(seq), 1, -1))\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # both forward and backward direction\n",
    "        self.enc_units = hidden_size*2\n",
    "        self.dec_units = hidden_size*2\n",
    "        \n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units, bias=False)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units, bias=False)\n",
    "        self.vt = nn.Linear(self.enc_units, 1, bias=False)\n",
    "\n",
    "    def forward(self,encoder_outputs,decoder_state):\n",
    "        \n",
    "        \n",
    "        # encoder_outputs: (seq_len, batch_size, hidden_size*2)\n",
    "        encoder_transform = self.W1(encoder_outputs)\n",
    "#         print(\"encoder_transform.shape: \", encoder_transform.shape)\n",
    "\n",
    "        # (1 (unsqueezed),batch_size, hidden_size*2)\n",
    "        decoder_transform = self.W2(decoder_state)\n",
    "#         print(\"decoder_transform: \", decoder_transform.shape)\n",
    "        \n",
    "        combined_transform = encoder_transform + decoder_transform\n",
    "#         print(\"combined_transform.shape \", combined_transform.shape)\n",
    "        # 1st line of Eq.(3) in the paper\n",
    "        # (seq_len, batch_size = 1 , 1) => squeeze to (seq_len, batch_size)\n",
    "        u_i = self.vt(torch.tanh(combined_transform)).squeeze()\n",
    "#         print(\"u_i.shape \", u_i.shape)\n",
    "\n",
    "        # log-softmax for a better numerical stability\n",
    "        attention_weights = F.log_softmax(u_i, dim=0).view(-1, 1, 1)\n",
    "#         print(\"attention_weights.shape \", attention_weights.shape)\n",
    "        \n",
    "        #context_vector shape after sum == (batch,hidden*2)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        context_vector = torch.sum(context_vector, dim=0)\n",
    "#         print(\"context_vector.shape \", context_vector.shape)\n",
    "        return context_vector,attention_weights\n",
    "\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # hidden size refers to input hidden size, decoder hidden size should be 2*encoder hidden size since the decoder is unidirectional\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size, batch_first=False, bidirectional=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.log_softmax = nn.LogSoftmax(dim= 1)\n",
    "        self.batch_first = batch_first\n",
    "        self.context_size = hidden_size*2\n",
    "        self.aa_embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTMCell(input_size=embedding_size + self.context_size, hidden_size=hidden_size*2)\n",
    "        self.attn = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "    def forward(self, y, encoder_outputs,decoder_state,decoder_cell):\n",
    "        \n",
    "        context_vector,attention_weights = self.attn(encoder_outputs,decoder_state)\n",
    "#         print(\"context_vector.shape: \", context_vector.shape)\n",
    "#         print(\"attention_weights.shape: \", attention_weights.shape)\n",
    "        y_embedded = self.aa_embedding(y)\n",
    "#         print(\"y_embedded.shape: \", y_embedded.shape)\n",
    "        y_cat = torch.cat((context_vector.unsqueeze(1), y_embedded), -1)\n",
    "#         print(\"y_cat.shape: \", y_cat.shape)\n",
    "        hidden = self.rnn(y_cat.squeeze(1),(decoder_state,decoder_cell))\n",
    "        h_i,_ = hidden\n",
    "#         print(\"h_i.shape: \", h_i.shape)\n",
    "        decoded_space = self.fc(h_i)\n",
    "#         print(\"decoded_space.shape: \", decoded_space.shape)\n",
    "        decoded_scores = F.log_softmax(decoded_space, dim = 1)\n",
    "#         print(\"decoded_scores.shape: \", decoded_scores.shape)\n",
    "        decoded_aa = torch.argmax(decoded_scores)\n",
    "#         print(\"decoded_aa.shape: \", decoded_aa.shape)\n",
    "        return decoded_scores, decoded_aa, hidden, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = len(aminoacid_list) + 1\n",
    "num_layers = 1\n",
    "hidden_size = 64\n",
    "output_size = len(aminoacid_list) + 1\n",
    "embedding_size= 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size = input_size, \\\n",
    "                               embedding_size = embedding_size, \\\n",
    "                               hidden_size = hidden_size, \\\n",
    "                               num_layers = num_layers, \\\n",
    "                               output_size = output_size)\n",
    "decoder = Decoder(input_size = input_size, \\\n",
    "                               embedding_size = embedding_size, \\\n",
    "                               hidden_size = hidden_size, \\\n",
    "                               num_layers = num_layers, \\\n",
    "                               output_size = output_size)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "for m in encoder.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "for m in decoder.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()                \n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                       lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trail test of Model by running 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_vector = []\n",
    "running_loss = 0\n",
    "print_every = 100\n",
    "\n",
    "for epoch in np.arange(0, uniref50_01.shape[0]): \n",
    "    seq = uniref50_01.iloc[epoch, 1]\n",
    "    if(len(seq)>4000):\n",
    "        continue\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    print(epoch)\n",
    "    print(\"len(seq): \", len(seq))\n",
    "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "    # Tensors of word indices.\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "    sentence_in = sentence_in.to(device = device)\n",
    "    targets = targets.view(1,-1).to(device = device)\n",
    "    print(\"targets.shape: \", targets.shape)\n",
    "    \n",
    "    print(targets.shape)\n",
    "    print(sentence_in.shape)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    seq_len = sentence_in.size(0)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(sentence_in)\n",
    "    print(\"encoder_outputs.shape: \" , encoder_outputs.shape)\n",
    "    \n",
    "    encoder_h_n, encoder_c_n = encoder_hidden\n",
    "    print(\"encoder_h_n.shape: \", encoder_h_n.shape)\n",
    "    print(\"encoder_hidden_last: \", encoder_c_n.shape)\n",
    "    \n",
    "    # Lets use zeros as an intial input\n",
    "    y_0 = 0\n",
    "    # using zeros for initial decoder hidden and cell state \n",
    "    d_0 = Variable(torch.zeros(1, decoder.hidden_size*2))\n",
    "    dcell_0 = Variable(torch.zeros(1,  decoder.hidden_size*2))\n",
    "    print(\"d_0.shape: \", d_0.shape)\n",
    "    print(\"dcell_0.shape: \", dcell_0.shape)\n",
    "    \n",
    "    y_last = y_0\n",
    "    d_last, d_cell_last = d_0, dcell_0\n",
    "\n",
    "    \n",
    "    print(\"seq_len: \",seq_len)\n",
    "    for di in range(seq_len):\n",
    "        decoded_scores, y_last, (d_last,d_cell_last), attention_weights = decoder(Variable(torch.LongTensor([[y_last]])).to(device), \\\n",
    "                                                                                 encoder_outputs.to(device), \\\n",
    "                                                                                 d_last.to(device),\\\n",
    "                                                                                 d_cell_last.to(device))\n",
    "        \n",
    "        loss += criterion(decoded_scores.to(device), targets[:,di])   \n",
    "    \n",
    "    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% (loss/seq_len))\n",
    "#     loss_vector.append(loss/seq_len)\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model on Uniref50_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_vector = []\n",
    "# running_loss = 0\n",
    "print_every = 1000\n",
    "\n",
    "for epoch in np.arange(0, uniref50_01.shape[0]): \n",
    "    seq = uniref50_01.iloc[epoch, 1]\n",
    "    if(len(seq)>4000):\n",
    "        continue\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "#     print(epoch)\n",
    "#     print(\"len(seq): \", len(seq))\n",
    "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "    # Tensors of word indices.\n",
    "    sentence_in = prepare_sequence(seq)\n",
    "    targets = prepare_labels(seq)\n",
    "    sentence_in = sentence_in.to(device = device)\n",
    "    targets = targets.view(1,-1).to(device = device)\n",
    "#     print(\"targets.shape: \", targets.shape)\n",
    "    \n",
    "#     print(targets.shape)\n",
    "#     print(sentence_in.shape)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    seq_len = sentence_in.size(0)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(sentence_in)\n",
    "#     print(\"encoder_outputs.shape: \" , encoder_outputs.shape)\n",
    "    \n",
    "    encoder_h_n, encoder_c_n = encoder_hidden\n",
    "#     print(\"encoder_h_n.shape: \", encoder_h_n.shape)\n",
    "#     print(\"encoder_hidden_last: \", encoder_c_n.shape)\n",
    "    \n",
    "    # Lets use zeros as an intial input\n",
    "    y_0 = 0\n",
    "    # using zeros for initial decoder hidden and cell state \n",
    "    d_0 = Variable(torch.zeros(1, decoder.hidden_size*2))\n",
    "    dcell_0 = Variable(torch.zeros(1,  decoder.hidden_size*2))\n",
    "#     print(\"d_0.shape: \", d_0.shape)\n",
    "#     print(\"dcell_0.shape: \", dcell_0.shape)\n",
    "    \n",
    "    y_last = y_0\n",
    "    d_last, d_cell_last = d_0, dcell_0\n",
    "\n",
    "    \n",
    "#     print(\"seq_len: \",seq_len)\n",
    "    for di in range(seq_len):\n",
    "        decoded_scores, y_last, (d_last,d_cell_last), attention_weights = decoder(Variable(torch.LongTensor([[y_last]])).to(device), \\\n",
    "                                                                                 encoder_outputs.to(device), \\\n",
    "                                                                                 d_last.to(device),\\\n",
    "                                                                                 d_cell_last.to(device))\n",
    "        \n",
    "        loss += criterion(decoded_scores.to(device), targets[:,di])   \n",
    "    \n",
    "    assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"At Epoch: %.1f\"% epoch)\n",
    "        print(f\"Loss %.4f\"% (loss/seq_len))\n",
    "#     loss_vector.append(loss/seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"../data/seq2seq_encoder_uniref_201012.pt\")\n",
    "torch.save(decoder.state_dict(), \"../data/seq2seq_decoder_uniref_201012.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
