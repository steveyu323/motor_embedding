{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm \n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "from esm.constants import proteinseq_toks\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from esm.modules import TransformerLayer, PositionalEmbedding  # noqa\n",
    "from esm.model import ProteinBertModel\n",
    "import esm\n",
    "import time\n",
    "\n",
    "import tape\n",
    "from tape import ProteinBertModel, TAPETokenizer,UniRepModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt_embed = np.load(\"../../out/201120/pdt_motor_t34.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt_motor = pd.read_csv(\"../../data/thermo/pdt_motor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501461, 1280)\n",
      "(501461, 6)\n"
     ]
    }
   ],
   "source": [
    "print(pdt_embed.shape)\n",
    "print(pdt_motor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfamA_target_name = [\"PF00349\",\"PF00022\",\"PF03727\",\"PF06723\",\\\n",
    "                       \"PF14450\",\"PF03953\",\"PF12327\",\"PF00091\",\"PF10644\",\\\n",
    "                      \"PF13809\",\"PF14881\",\"PF00063\",\"PF00225\",\"PF03028\"]\n",
    "pdt_motor_target = pdt_motor.loc[pdt_motor[\"pfam_id\"].isin(pfamA_target_name),:]\n",
    "pdt_embed_target = pdt_embed[pdt_motor[\"pfam_id\"].isin(pfamA_target_name),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7443, 1280)\n",
      "(7443, 6)\n",
      "584\n"
     ]
    }
   ],
   "source": [
    "print(pdt_embed_target.shape)\n",
    "print(pdt_motor_target.shape)\n",
    "print(sum(pdt_motor_target[\"is_thermophilic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>pfam_id</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clan</th>\n",
       "      <th>is_thermophilic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actin_like</th>\n",
       "      <th>0</th>\n",
       "      <td>50106</td>\n",
       "      <td>50106</td>\n",
       "      <td>50106</td>\n",
       "      <td>50106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3620</td>\n",
       "      <td>3620</td>\n",
       "      <td>3620</td>\n",
       "      <td>3620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">p_loop_gtpase</th>\n",
       "      <th>0</th>\n",
       "      <td>419235</td>\n",
       "      <td>419235</td>\n",
       "      <td>419235</td>\n",
       "      <td>419235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24467</td>\n",
       "      <td>24467</td>\n",
       "      <td>24467</td>\n",
       "      <td>24467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tubulin_binding</th>\n",
       "      <th>0</th>\n",
       "      <td>3685</td>\n",
       "      <td>3685</td>\n",
       "      <td>3685</td>\n",
       "      <td>3685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tubulin_c</th>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 uniprot_id  pfam_id   token     seq\n",
       "clan            is_thermophilic                                     \n",
       "actin_like      0                     50106    50106   50106   50106\n",
       "                1                      3620     3620    3620    3620\n",
       "p_loop_gtpase   0                    419235   419235  419235  419235\n",
       "                1                     24467    24467   24467   24467\n",
       "tubulin_binding 0                      3685     3685    3685    3685\n",
       "                1                       278      278     278     278\n",
       "tubulin_c       0                        68       68      68      68\n",
       "                1                         2        2       2       2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdt_motor.groupby([\"clan\",\"is_thermophilic\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>pfam_id</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clan</th>\n",
       "      <th>is_thermophilic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actin_like</th>\n",
       "      <th>0</th>\n",
       "      <td>3104</td>\n",
       "      <td>3104</td>\n",
       "      <td>3104</td>\n",
       "      <td>3104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_loop_gtpase</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tubulin_binding</th>\n",
       "      <th>0</th>\n",
       "      <td>3685</td>\n",
       "      <td>3685</td>\n",
       "      <td>3685</td>\n",
       "      <td>3685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tubulin_c</th>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 uniprot_id  pfam_id  token   seq\n",
       "clan            is_thermophilic                                  \n",
       "actin_like      0                      3104     3104   3104  3104\n",
       "                1                       304      304    304   304\n",
       "p_loop_gtpase   0                         2        2      2     2\n",
       "tubulin_binding 0                      3685     3685   3685  3685\n",
       "                1                       278      278    278   278\n",
       "tubulin_c       0                        68       68     68    68\n",
       "                1                         2        2      2     2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdt_motor_target.groupby([\"clan\",\"is_thermophilic\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "      <th>clan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pfam_id</th>\n",
       "      <th>is_thermophilic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PF00004</th>\n",
       "      <th>0</th>\n",
       "      <td>8644</td>\n",
       "      <td>8644</td>\n",
       "      <td>8644</td>\n",
       "      <td>8644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>744</td>\n",
       "      <td>744</td>\n",
       "      <td>744</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PF00005</th>\n",
       "      <th>0</th>\n",
       "      <td>189641</td>\n",
       "      <td>189641</td>\n",
       "      <td>189641</td>\n",
       "      <td>189641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9619</td>\n",
       "      <td>9619</td>\n",
       "      <td>9619</td>\n",
       "      <td>9619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00006</th>\n",
       "      <th>0</th>\n",
       "      <td>12904</td>\n",
       "      <td>12904</td>\n",
       "      <td>12904</td>\n",
       "      <td>12904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF14532</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF16203</th>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PF16575</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF16813</th>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         uniprot_id   token     seq    clan\n",
       "pfam_id is_thermophilic                                    \n",
       "PF00004 0                      8644    8644    8644    8644\n",
       "        1                       744     744     744     744\n",
       "PF00005 0                    189641  189641  189641  189641\n",
       "        1                      9619    9619    9619    9619\n",
       "PF00006 0                     12904   12904   12904   12904\n",
       "...                             ...     ...     ...     ...\n",
       "PF14532 1                         2       2       2       2\n",
       "PF16203 0                        10      10      10      10\n",
       "PF16575 0                         7       7       7       7\n",
       "        1                        39      39      39      39\n",
       "PF16813 0                        38      38      38      38\n",
       "\n",
       "[277 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdt_motor.loc[pdt_motor[\"clan\"]==\"p_loop_gtpase\",:].groupby([\"pfam_id\",\"is_thermophilic\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try create a balanced training set by sampling the same number of min(thermophilic, non-thermophilic) of a family. For now do no sample from a family is it does not contain one of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "854\n",
      "17\n",
      "102\n",
      "612\n",
      "128\n",
      "584\n",
      "46\n",
      "153\n",
      "9619\n",
      "402\n",
      "317\n",
      "675\n",
      "744\n",
      "174\n",
      "132\n",
      "1\n",
      "1032\n",
      "1453\n",
      "194\n",
      "35\n",
      "185\n",
      "2\n",
      "243\n",
      "131\n",
      "199\n",
      "4\n",
      "159\n",
      "183\n",
      "27\n",
      "38\n",
      "27\n",
      "598\n",
      "230\n",
      "309\n",
      "320\n",
      "412\n",
      "97\n",
      "276\n",
      "230\n",
      "8\n",
      "92\n",
      "10\n",
      "175\n",
      "60\n",
      "42\n",
      "438\n",
      "114\n",
      "21\n",
      "164\n",
      "76\n",
      "298\n",
      "112\n",
      "62\n",
      "184\n",
      "276\n",
      "294\n",
      "269\n",
      "10\n",
      "5\n",
      "267\n",
      "410\n",
      "113\n",
      "156\n",
      "14\n",
      "134\n",
      "61\n",
      "29\n",
      "14\n",
      "17\n",
      "174\n",
      "1\n",
      "144\n",
      "98\n",
      "46\n",
      "79\n",
      "2\n",
      "3\n",
      "227\n",
      "90\n",
      "209\n",
      "0\n",
      "86\n",
      "77\n",
      "92\n",
      "78\n",
      "8\n",
      "3\n",
      "7\n",
      "183\n",
      "0\n",
      "72\n",
      "154\n",
      "19\n",
      "0\n",
      "174\n",
      "3\n",
      "7\n",
      "36\n",
      "142\n",
      "15\n",
      "4\n",
      "57\n",
      "93\n",
      "6\n",
      "2\n",
      "24\n",
      "20\n",
      "110\n",
      "35\n",
      "0\n",
      "0\n",
      "67\n",
      "5\n",
      "14\n",
      "2\n",
      "12\n",
      "15\n",
      "3\n",
      "16\n",
      "8\n",
      "5\n",
      "32\n",
      "0\n",
      "3\n",
      "3\n",
      "34\n",
      "2\n",
      "1\n",
      "17\n",
      "25\n",
      "6\n",
      "1\n",
      "12\n",
      "0\n",
      "59\n",
      "6\n",
      "0\n",
      "1\n",
      "0\n",
      "32\n",
      "8\n",
      "53\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "60\n",
      "2\n",
      "0\n",
      "13\n",
      "0\n",
      "7\n",
      "0\n",
      "1\n",
      "4\n",
      "5\n",
      "1\n",
      "7\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "18\n",
      "0\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "thermo_sampled = pd.DataFrame()\n",
    "for pfam_id in pdt_motor[\"pfam_id\"].unique():\n",
    "    curr_dat = pdt_motor.loc[pdt_motor[\"pfam_id\"] == pfam_id,:]\n",
    "    is_thermo = curr_dat.loc[curr_dat[\"is_thermophilic\"]==1,:]\n",
    "    not_thermo = curr_dat.loc[curr_dat[\"is_thermophilic\"]==0,:]\n",
    "    if (not_thermo.shape[0]>=is_thermo.shape[0]):\n",
    "        print(is_thermo.shape[0])\n",
    "        #sample #is_thermo.shape[0] entries from not_thermo uniformly\n",
    "        thermo_sampled = thermo_sampled.append(is_thermo)\n",
    "        tmp = not_thermo.sample(n = is_thermo.shape[0])\n",
    "    else:\n",
    "        #sample #not_thermo.shape[0] entries from is_thermo uniformly\n",
    "        print(not_thermo.shape[0])\n",
    "        thermo_sampled = thermo_sampled.append(not_thermo)\n",
    "        tmp = is_thermo.sample(n = not_thermo.shape[0])\n",
    "    thermo_sampled = thermo_sampled.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>pfam_id</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clan</th>\n",
       "      <th>is_thermophilic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actin_like</th>\n",
       "      <th>0</th>\n",
       "      <td>3604</td>\n",
       "      <td>3604</td>\n",
       "      <td>3604</td>\n",
       "      <td>3604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3604</td>\n",
       "      <td>3604</td>\n",
       "      <td>3604</td>\n",
       "      <td>3604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">p_loop_gtpase</th>\n",
       "      <th>0</th>\n",
       "      <td>24382</td>\n",
       "      <td>24382</td>\n",
       "      <td>24382</td>\n",
       "      <td>24382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24382</td>\n",
       "      <td>24382</td>\n",
       "      <td>24382</td>\n",
       "      <td>24382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tubulin_binding</th>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tubulin_c</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 uniprot_id  pfam_id  token    seq\n",
       "clan            is_thermophilic                                   \n",
       "actin_like      0                      3604     3604   3604   3604\n",
       "                1                      3604     3604   3604   3604\n",
       "p_loop_gtpase   0                     24382    24382  24382  24382\n",
       "                1                     24382    24382  24382  24382\n",
       "tubulin_binding 0                       278      278    278    278\n",
       "                1                       278      278    278    278\n",
       "tubulin_c       0                         2        2      2      2\n",
       "                1                         2        2      2      2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thermo_sampled.groupby([\"clan\",\"is_thermophilic\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_sampled_embed = pdt_embed[thermo_sampled.index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the hidden dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(thermo_sampled_embed)\n",
    "thermo_sampled_embed_scaled = scaler.transform(thermo_sampled_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6755429.5, 4681645.5, 4044263.2, 2901785. , 2774242. , 2547687.5,\n",
       "       2290476.8, 1964802.9, 1819529.6, 1658939.1], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, v = np.linalg.svd(thermo_sampled_embed_scaled.T@thermo_sampled_embed_scaled)\n",
    "s[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ratio = np.cumsum(s)/sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9640889"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ratio[270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1280)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = thermo_sampled_embed_scaled.T@thermo_sampled_embed_scaled\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1280)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = np.cov(thermo_sampled_embed_scaled.T)\n",
    "sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([119.49956083,  82.81554047,  71.54063013,  51.33086417,\n",
       "        49.07470398,  45.06708606,  40.51717862,  34.75620251,\n",
       "        32.18640523,  29.3456542 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, v = np.linalg.svd(sigma)\n",
    "s[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ratio = np.cumsum(s)/sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.910473017770574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ratio[125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_sampled_embed_scaled_reduced = pca.fit_transform(thermo_sampled_embed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09335782, 0.15805678, 0.21394718, 0.25404894, 0.29238817,\n",
       "       0.3275964 , 0.35925007, 0.38640305, 0.41154838, 0.43447438,\n",
       "       0.45580924, 0.47338852, 0.48982868, 0.5051827 , 0.5198387 ,\n",
       "       0.53395027, 0.547616  , 0.55886745, 0.570102  , 0.58107036,\n",
       "       0.59194773, 0.6024172 , 0.61216956, 0.62163585, 0.63056934,\n",
       "       0.6393483 , 0.6479902 , 0.65614367, 0.663938  , 0.67124474,\n",
       "       0.678297  , 0.6850373 , 0.6916274 , 0.6979462 , 0.7040716 ,\n",
       "       0.7100208 , 0.7156064 , 0.7210687 , 0.7263722 , 0.7314695 ,\n",
       "       0.73644066, 0.74123645, 0.7458298 , 0.75030845, 0.75461894,\n",
       "       0.758809  , 0.7629481 , 0.76699454, 0.7708458 , 0.7745878 ,\n",
       "       0.77823716, 0.78178173, 0.78527474, 0.788654  , 0.7919747 ,\n",
       "       0.7951846 , 0.7983063 , 0.80136865, 0.8042768 , 0.8071294 ,\n",
       "       0.80995667, 0.81269544, 0.81539494, 0.8180541 , 0.82064956,\n",
       "       0.8231842 , 0.8256487 , 0.82809585, 0.83040196, 0.8326741 ,\n",
       "       0.83490556, 0.83710647, 0.8392701 , 0.84141505, 0.84348905,\n",
       "       0.84549683, 0.8474908 , 0.8494401 , 0.8513514 , 0.8532077 ,\n",
       "       0.85502857, 0.85681623, 0.85856086, 0.86027664, 0.8619296 ,\n",
       "       0.8635713 , 0.86517036, 0.8667232 , 0.86826867, 0.8697848 ,\n",
       "       0.8712783 , 0.8727509 , 0.8741964 , 0.8756217 , 0.87701416,\n",
       "       0.87838215, 0.87973404, 0.881058  , 0.88238007, 0.8836775 ,\n",
       "       0.88494843, 0.88619804, 0.88742167, 0.8886291 , 0.8898238 ,\n",
       "       0.89099973, 0.8921691 , 0.8932873 , 0.894401  , 0.89547217,\n",
       "       0.89652145, 0.89754933, 0.89856184, 0.89955664, 0.9005412 ,\n",
       "       0.90150934, 0.90246826, 0.9034109 , 0.9043433 , 0.9052648 ,\n",
       "       0.9061639 , 0.90704656, 0.9079163 , 0.9087624 , 0.90960175],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56532, 125)\n",
      "(56532,)\n"
     ]
    }
   ],
   "source": [
    "X = thermo_sampled_embed_scaled_reduced\n",
    "y = thermo_sampled[\"is_thermophilic\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying thermophilic using logistic regression with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(cv=5, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8061377907490935"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083582089552239"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying thermophilic using softSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyu7/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761437258153676"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7658087910144159"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying thermophilic using kNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5,weights = \"uniform\")\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.883338861249309"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8266560537719997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8396568497390997\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=5,weights = \"distance\")\n",
    "neigh.fit(X_train, y_train)\n",
    "print(neigh.score(X_train, y_train))\n",
    "print(neigh.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8375342708056955\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=9,weights = \"distance\")\n",
    "neigh.fit(X_train, y_train)\n",
    "print(neigh.score(X_train, y_train))\n",
    "print(neigh.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermoDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dat,label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dat (ndarray): ndarray with the X data\n",
    "            label: an pdSeries with the 0/1 label of the X data \n",
    "        \"\"\"\n",
    "        self.X = dat\n",
    "        self.y = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        embed = self.X[idx,:]\n",
    "        is_thermo = self.y.iloc[idx]\n",
    "        sample = {'X': embed, 'y': is_thermo}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = thermo_sampled_embed_scaled_reduced\n",
    "y = thermo_sampled[\"is_thermophilic\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_dataset_train = ThermoDataset(X_train,y_train)\n",
    "train_loader = DataLoader(thermo_dataset_train, batch_size=100,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([100, 125]) torch.Size([100])\n",
      "1 torch.Size([100, 125]) torch.Size([100])\n",
      "2 torch.Size([100, 125]) torch.Size([100])\n",
      "3 torch.Size([100, 125]) torch.Size([100])\n",
      "4 torch.Size([100, 125]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['X'].size(),\n",
    "          sample_batched['y'].size())\n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ThermoClassifier_75(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThermoClassifier_75, self).__init__()\n",
    "        self.fc1 = nn.Linear(125, 80)\n",
    "        self.fc2 = nn.Linear(80, 50)\n",
    "        self.fc3 = nn.Linear(50, 30)\n",
    "        self.fc4 = nn.Linear(30, 10)\n",
    "        self.fc5 = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ThermoClassifier_75().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [200/453], Loss: 0.3864\n",
      "Epoch [1/200], Step [400/453], Loss: 0.4660\n",
      "Epoch [2/200], Step [200/453], Loss: 0.3220\n",
      "Epoch [2/200], Step [400/453], Loss: 0.5007\n",
      "Epoch [3/200], Step [200/453], Loss: 0.3201\n",
      "Epoch [3/200], Step [400/453], Loss: 0.4220\n",
      "Epoch [4/200], Step [200/453], Loss: 0.3263\n",
      "Epoch [4/200], Step [400/453], Loss: 0.3155\n",
      "Epoch [5/200], Step [200/453], Loss: 0.3927\n",
      "Epoch [5/200], Step [400/453], Loss: 0.3021\n",
      "Epoch [6/200], Step [200/453], Loss: 0.2756\n",
      "Epoch [6/200], Step [400/453], Loss: 0.3609\n",
      "Epoch [7/200], Step [200/453], Loss: 0.2634\n",
      "Epoch [7/200], Step [400/453], Loss: 0.3270\n",
      "Epoch [8/200], Step [200/453], Loss: 0.3333\n",
      "Epoch [8/200], Step [400/453], Loss: 0.3471\n",
      "Epoch [9/200], Step [200/453], Loss: 0.3245\n",
      "Epoch [9/200], Step [400/453], Loss: 0.3226\n",
      "Epoch [10/200], Step [200/453], Loss: 0.2857\n",
      "Epoch [10/200], Step [400/453], Loss: 0.3910\n",
      "Epoch [11/200], Step [200/453], Loss: 0.3386\n",
      "Epoch [11/200], Step [400/453], Loss: 0.3090\n",
      "Epoch [12/200], Step [200/453], Loss: 0.2711\n",
      "Epoch [12/200], Step [400/453], Loss: 0.3801\n",
      "Epoch [13/200], Step [200/453], Loss: 0.2934\n",
      "Epoch [13/200], Step [400/453], Loss: 0.3172\n",
      "Epoch [14/200], Step [200/453], Loss: 0.3614\n",
      "Epoch [14/200], Step [400/453], Loss: 0.2441\n",
      "Epoch [15/200], Step [200/453], Loss: 0.3402\n",
      "Epoch [15/200], Step [400/453], Loss: 0.3242\n",
      "Epoch [16/200], Step [200/453], Loss: 0.2745\n",
      "Epoch [16/200], Step [400/453], Loss: 0.2080\n",
      "Epoch [17/200], Step [200/453], Loss: 0.2323\n",
      "Epoch [17/200], Step [400/453], Loss: 0.2367\n",
      "Epoch [18/200], Step [200/453], Loss: 0.2582\n",
      "Epoch [18/200], Step [400/453], Loss: 0.2365\n",
      "Epoch [19/200], Step [200/453], Loss: 0.2091\n",
      "Epoch [19/200], Step [400/453], Loss: 0.2425\n",
      "Epoch [20/200], Step [200/453], Loss: 0.1997\n",
      "Epoch [20/200], Step [400/453], Loss: 0.2699\n",
      "Epoch [21/200], Step [200/453], Loss: 0.3069\n",
      "Epoch [21/200], Step [400/453], Loss: 0.2350\n",
      "Epoch [22/200], Step [200/453], Loss: 0.2535\n",
      "Epoch [22/200], Step [400/453], Loss: 0.2104\n",
      "Epoch [23/200], Step [200/453], Loss: 0.2066\n",
      "Epoch [23/200], Step [400/453], Loss: 0.2663\n",
      "Epoch [24/200], Step [200/453], Loss: 0.1179\n",
      "Epoch [24/200], Step [400/453], Loss: 0.2482\n",
      "Epoch [25/200], Step [200/453], Loss: 0.2794\n",
      "Epoch [25/200], Step [400/453], Loss: 0.2243\n",
      "Epoch [26/200], Step [200/453], Loss: 0.2075\n",
      "Epoch [26/200], Step [400/453], Loss: 0.2052\n",
      "Epoch [27/200], Step [200/453], Loss: 0.1588\n",
      "Epoch [27/200], Step [400/453], Loss: 0.2367\n",
      "Epoch [28/200], Step [200/453], Loss: 0.1268\n",
      "Epoch [28/200], Step [400/453], Loss: 0.1657\n",
      "Epoch [29/200], Step [200/453], Loss: 0.1276\n",
      "Epoch [29/200], Step [400/453], Loss: 0.1805\n",
      "Epoch [30/200], Step [200/453], Loss: 0.1960\n",
      "Epoch [30/200], Step [400/453], Loss: 0.2352\n",
      "Epoch [31/200], Step [200/453], Loss: 0.1340\n",
      "Epoch [31/200], Step [400/453], Loss: 0.2537\n",
      "Epoch [32/200], Step [200/453], Loss: 0.2348\n",
      "Epoch [32/200], Step [400/453], Loss: 0.3647\n",
      "Epoch [33/200], Step [200/453], Loss: 0.1076\n",
      "Epoch [33/200], Step [400/453], Loss: 0.1470\n",
      "Epoch [34/200], Step [200/453], Loss: 0.0962\n",
      "Epoch [34/200], Step [400/453], Loss: 0.2050\n",
      "Epoch [35/200], Step [200/453], Loss: 0.1989\n",
      "Epoch [35/200], Step [400/453], Loss: 0.1534\n",
      "Epoch [36/200], Step [200/453], Loss: 0.2085\n",
      "Epoch [36/200], Step [400/453], Loss: 0.1545\n",
      "Epoch [37/200], Step [200/453], Loss: 0.2294\n",
      "Epoch [37/200], Step [400/453], Loss: 0.1960\n",
      "Epoch [38/200], Step [200/453], Loss: 0.2138\n",
      "Epoch [38/200], Step [400/453], Loss: 0.1923\n",
      "Epoch [39/200], Step [200/453], Loss: 0.1178\n",
      "Epoch [39/200], Step [400/453], Loss: 0.1149\n",
      "Epoch [40/200], Step [200/453], Loss: 0.0770\n",
      "Epoch [40/200], Step [400/453], Loss: 0.1198\n",
      "Epoch [41/200], Step [200/453], Loss: 0.2045\n",
      "Epoch [41/200], Step [400/453], Loss: 0.1647\n",
      "Epoch [42/200], Step [200/453], Loss: 0.0424\n",
      "Epoch [42/200], Step [400/453], Loss: 0.1102\n",
      "Epoch [43/200], Step [200/453], Loss: 0.1374\n",
      "Epoch [43/200], Step [400/453], Loss: 0.1574\n",
      "Epoch [44/200], Step [200/453], Loss: 0.0846\n",
      "Epoch [44/200], Step [400/453], Loss: 0.0831\n",
      "Epoch [45/200], Step [200/453], Loss: 0.1090\n",
      "Epoch [45/200], Step [400/453], Loss: 0.1486\n",
      "Epoch [46/200], Step [200/453], Loss: 0.1194\n",
      "Epoch [46/200], Step [400/453], Loss: 0.1719\n",
      "Epoch [47/200], Step [200/453], Loss: 0.2207\n",
      "Epoch [47/200], Step [400/453], Loss: 0.1290\n",
      "Epoch [48/200], Step [200/453], Loss: 0.1575\n",
      "Epoch [48/200], Step [400/453], Loss: 0.1205\n",
      "Epoch [49/200], Step [200/453], Loss: 0.1330\n",
      "Epoch [49/200], Step [400/453], Loss: 0.0956\n",
      "Epoch [50/200], Step [200/453], Loss: 0.1414\n",
      "Epoch [50/200], Step [400/453], Loss: 0.1374\n",
      "Epoch [51/200], Step [200/453], Loss: 0.0755\n",
      "Epoch [51/200], Step [400/453], Loss: 0.1175\n",
      "Epoch [52/200], Step [200/453], Loss: 0.1575\n",
      "Epoch [52/200], Step [400/453], Loss: 0.1735\n",
      "Epoch [53/200], Step [200/453], Loss: 0.1053\n",
      "Epoch [53/200], Step [400/453], Loss: 0.0851\n",
      "Epoch [54/200], Step [200/453], Loss: 0.1814\n",
      "Epoch [54/200], Step [400/453], Loss: 0.1200\n",
      "Epoch [55/200], Step [200/453], Loss: 0.1174\n",
      "Epoch [55/200], Step [400/453], Loss: 0.1428\n",
      "Epoch [56/200], Step [200/453], Loss: 0.1326\n",
      "Epoch [56/200], Step [400/453], Loss: 0.0892\n",
      "Epoch [57/200], Step [200/453], Loss: 0.1113\n",
      "Epoch [57/200], Step [400/453], Loss: 0.2166\n",
      "Epoch [58/200], Step [200/453], Loss: 0.1197\n",
      "Epoch [58/200], Step [400/453], Loss: 0.1530\n",
      "Epoch [59/200], Step [200/453], Loss: 0.1843\n",
      "Epoch [59/200], Step [400/453], Loss: 0.1256\n",
      "Epoch [60/200], Step [200/453], Loss: 0.0988\n",
      "Epoch [60/200], Step [400/453], Loss: 0.0943\n",
      "Epoch [61/200], Step [200/453], Loss: 0.1241\n",
      "Epoch [61/200], Step [400/453], Loss: 0.1058\n",
      "Epoch [62/200], Step [200/453], Loss: 0.0766\n",
      "Epoch [62/200], Step [400/453], Loss: 0.1537\n",
      "Epoch [63/200], Step [200/453], Loss: 0.1078\n",
      "Epoch [63/200], Step [400/453], Loss: 0.1864\n",
      "Epoch [64/200], Step [200/453], Loss: 0.0872\n",
      "Epoch [64/200], Step [400/453], Loss: 0.1945\n",
      "Epoch [65/200], Step [200/453], Loss: 0.1387\n",
      "Epoch [65/200], Step [400/453], Loss: 0.0810\n",
      "Epoch [66/200], Step [200/453], Loss: 0.1102\n",
      "Epoch [66/200], Step [400/453], Loss: 0.1544\n",
      "Epoch [67/200], Step [200/453], Loss: 0.1145\n",
      "Epoch [67/200], Step [400/453], Loss: 0.1089\n",
      "Epoch [68/200], Step [200/453], Loss: 0.1561\n",
      "Epoch [68/200], Step [400/453], Loss: 0.1185\n",
      "Epoch [69/200], Step [200/453], Loss: 0.0422\n",
      "Epoch [69/200], Step [400/453], Loss: 0.0753\n",
      "Epoch [70/200], Step [200/453], Loss: 0.0591\n",
      "Epoch [70/200], Step [400/453], Loss: 0.1409\n",
      "Epoch [71/200], Step [200/453], Loss: 0.0355\n",
      "Epoch [71/200], Step [400/453], Loss: 0.1293\n",
      "Epoch [72/200], Step [200/453], Loss: 0.1010\n",
      "Epoch [72/200], Step [400/453], Loss: 0.1443\n",
      "Epoch [73/200], Step [200/453], Loss: 0.0970\n",
      "Epoch [73/200], Step [400/453], Loss: 0.0823\n",
      "Epoch [74/200], Step [200/453], Loss: 0.0637\n",
      "Epoch [74/200], Step [400/453], Loss: 0.0692\n",
      "Epoch [75/200], Step [200/453], Loss: 0.2775\n",
      "Epoch [75/200], Step [400/453], Loss: 0.0859\n",
      "Epoch [76/200], Step [200/453], Loss: 0.1217\n",
      "Epoch [76/200], Step [400/453], Loss: 0.1147\n",
      "Epoch [77/200], Step [200/453], Loss: 0.0661\n",
      "Epoch [77/200], Step [400/453], Loss: 0.1529\n",
      "Epoch [78/200], Step [200/453], Loss: 0.0379\n",
      "Epoch [78/200], Step [400/453], Loss: 0.0838\n",
      "Epoch [79/200], Step [200/453], Loss: 0.1050\n",
      "Epoch [79/200], Step [400/453], Loss: 0.0806\n",
      "Epoch [80/200], Step [200/453], Loss: 0.1068\n",
      "Epoch [80/200], Step [400/453], Loss: 0.0644\n",
      "Epoch [81/200], Step [200/453], Loss: 0.1490\n",
      "Epoch [81/200], Step [400/453], Loss: 0.0916\n",
      "Epoch [82/200], Step [200/453], Loss: 0.0809\n",
      "Epoch [82/200], Step [400/453], Loss: 0.0361\n",
      "Epoch [83/200], Step [200/453], Loss: 0.1238\n",
      "Epoch [83/200], Step [400/453], Loss: 0.0770\n",
      "Epoch [84/200], Step [200/453], Loss: 0.1078\n",
      "Epoch [84/200], Step [400/453], Loss: 0.0874\n",
      "Epoch [85/200], Step [200/453], Loss: 0.0692\n",
      "Epoch [85/200], Step [400/453], Loss: 0.0544\n",
      "Epoch [86/200], Step [200/453], Loss: 0.1008\n",
      "Epoch [86/200], Step [400/453], Loss: 0.1097\n",
      "Epoch [87/200], Step [200/453], Loss: 0.1238\n",
      "Epoch [87/200], Step [400/453], Loss: 0.0779\n",
      "Epoch [88/200], Step [200/453], Loss: 0.0292\n",
      "Epoch [88/200], Step [400/453], Loss: 0.0616\n",
      "Epoch [89/200], Step [200/453], Loss: 0.0423\n",
      "Epoch [89/200], Step [400/453], Loss: 0.0707\n",
      "Epoch [90/200], Step [200/453], Loss: 0.0492\n",
      "Epoch [90/200], Step [400/453], Loss: 0.0474\n",
      "Epoch [91/200], Step [200/453], Loss: 0.0497\n",
      "Epoch [91/200], Step [400/453], Loss: 0.0857\n",
      "Epoch [92/200], Step [200/453], Loss: 0.0845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/200], Step [400/453], Loss: 0.1031\n",
      "Epoch [93/200], Step [200/453], Loss: 0.0630\n",
      "Epoch [93/200], Step [400/453], Loss: 0.0894\n",
      "Epoch [94/200], Step [200/453], Loss: 0.0992\n",
      "Epoch [94/200], Step [400/453], Loss: 0.0824\n",
      "Epoch [95/200], Step [200/453], Loss: 0.0332\n",
      "Epoch [95/200], Step [400/453], Loss: 0.0208\n",
      "Epoch [96/200], Step [200/453], Loss: 0.0656\n",
      "Epoch [96/200], Step [400/453], Loss: 0.1238\n",
      "Epoch [97/200], Step [200/453], Loss: 0.0443\n",
      "Epoch [97/200], Step [400/453], Loss: 0.1390\n",
      "Epoch [98/200], Step [200/453], Loss: 0.1718\n",
      "Epoch [98/200], Step [400/453], Loss: 0.1139\n",
      "Epoch [99/200], Step [200/453], Loss: 0.0200\n",
      "Epoch [99/200], Step [400/453], Loss: 0.1239\n",
      "Epoch [100/200], Step [200/453], Loss: 0.0494\n",
      "Epoch [100/200], Step [400/453], Loss: 0.1523\n",
      "Epoch [101/200], Step [200/453], Loss: 0.0374\n",
      "Epoch [101/200], Step [400/453], Loss: 0.0398\n",
      "Epoch [102/200], Step [200/453], Loss: 0.0813\n",
      "Epoch [102/200], Step [400/453], Loss: 0.1178\n",
      "Epoch [103/200], Step [200/453], Loss: 0.1009\n",
      "Epoch [103/200], Step [400/453], Loss: 0.0420\n",
      "Epoch [104/200], Step [200/453], Loss: 0.1178\n",
      "Epoch [104/200], Step [400/453], Loss: 0.0738\n",
      "Epoch [105/200], Step [200/453], Loss: 0.0580\n",
      "Epoch [105/200], Step [400/453], Loss: 0.0694\n",
      "Epoch [106/200], Step [200/453], Loss: 0.0780\n",
      "Epoch [106/200], Step [400/453], Loss: 0.0732\n",
      "Epoch [107/200], Step [200/453], Loss: 0.0497\n",
      "Epoch [107/200], Step [400/453], Loss: 0.0789\n",
      "Epoch [108/200], Step [200/453], Loss: 0.0605\n",
      "Epoch [108/200], Step [400/453], Loss: 0.1277\n",
      "Epoch [109/200], Step [200/453], Loss: 0.0224\n",
      "Epoch [109/200], Step [400/453], Loss: 0.0488\n",
      "Epoch [110/200], Step [200/453], Loss: 0.0527\n",
      "Epoch [110/200], Step [400/453], Loss: 0.0914\n",
      "Epoch [111/200], Step [200/453], Loss: 0.0531\n",
      "Epoch [111/200], Step [400/453], Loss: 0.0500\n",
      "Epoch [112/200], Step [200/453], Loss: 0.0734\n",
      "Epoch [112/200], Step [400/453], Loss: 0.0879\n",
      "Epoch [113/200], Step [200/453], Loss: 0.0918\n",
      "Epoch [113/200], Step [400/453], Loss: 0.0809\n",
      "Epoch [114/200], Step [200/453], Loss: 0.0601\n",
      "Epoch [114/200], Step [400/453], Loss: 0.0374\n",
      "Epoch [115/200], Step [200/453], Loss: 0.0392\n",
      "Epoch [115/200], Step [400/453], Loss: 0.0915\n",
      "Epoch [116/200], Step [200/453], Loss: 0.0720\n",
      "Epoch [116/200], Step [400/453], Loss: 0.0101\n",
      "Epoch [117/200], Step [200/453], Loss: 0.0378\n",
      "Epoch [117/200], Step [400/453], Loss: 0.1488\n",
      "Epoch [118/200], Step [200/453], Loss: 0.0665\n",
      "Epoch [118/200], Step [400/453], Loss: 0.0599\n",
      "Epoch [119/200], Step [200/453], Loss: 0.0245\n",
      "Epoch [119/200], Step [400/453], Loss: 0.0730\n",
      "Epoch [120/200], Step [200/453], Loss: 0.0880\n",
      "Epoch [120/200], Step [400/453], Loss: 0.0670\n",
      "Epoch [121/200], Step [200/453], Loss: 0.0735\n",
      "Epoch [121/200], Step [400/453], Loss: 0.1311\n",
      "Epoch [122/200], Step [200/453], Loss: 0.0288\n",
      "Epoch [122/200], Step [400/453], Loss: 0.0535\n",
      "Epoch [123/200], Step [200/453], Loss: 0.0798\n",
      "Epoch [123/200], Step [400/453], Loss: 0.0622\n",
      "Epoch [124/200], Step [200/453], Loss: 0.0704\n",
      "Epoch [124/200], Step [400/453], Loss: 0.0594\n",
      "Epoch [125/200], Step [200/453], Loss: 0.0455\n",
      "Epoch [125/200], Step [400/453], Loss: 0.0943\n",
      "Epoch [126/200], Step [200/453], Loss: 0.0989\n",
      "Epoch [126/200], Step [400/453], Loss: 0.1021\n",
      "Epoch [127/200], Step [200/453], Loss: 0.0918\n",
      "Epoch [127/200], Step [400/453], Loss: 0.0407\n",
      "Epoch [128/200], Step [200/453], Loss: 0.0336\n",
      "Epoch [128/200], Step [400/453], Loss: 0.0899\n",
      "Epoch [129/200], Step [200/453], Loss: 0.0469\n",
      "Epoch [129/200], Step [400/453], Loss: 0.0415\n",
      "Epoch [130/200], Step [200/453], Loss: 0.0671\n",
      "Epoch [130/200], Step [400/453], Loss: 0.0682\n",
      "Epoch [131/200], Step [200/453], Loss: 0.0176\n",
      "Epoch [131/200], Step [400/453], Loss: 0.0200\n",
      "Epoch [132/200], Step [200/453], Loss: 0.0299\n",
      "Epoch [132/200], Step [400/453], Loss: 0.0376\n",
      "Epoch [133/200], Step [200/453], Loss: 0.0635\n",
      "Epoch [133/200], Step [400/453], Loss: 0.0336\n",
      "Epoch [134/200], Step [200/453], Loss: 0.1415\n",
      "Epoch [134/200], Step [400/453], Loss: 0.0388\n",
      "Epoch [135/200], Step [200/453], Loss: 0.0478\n",
      "Epoch [135/200], Step [400/453], Loss: 0.1098\n",
      "Epoch [136/200], Step [200/453], Loss: 0.0913\n",
      "Epoch [136/200], Step [400/453], Loss: 0.0380\n",
      "Epoch [137/200], Step [200/453], Loss: 0.0374\n",
      "Epoch [137/200], Step [400/453], Loss: 0.0715\n",
      "Epoch [138/200], Step [200/453], Loss: 0.0461\n",
      "Epoch [138/200], Step [400/453], Loss: 0.0735\n",
      "Epoch [139/200], Step [200/453], Loss: 0.0585\n",
      "Epoch [139/200], Step [400/453], Loss: 0.0155\n",
      "Epoch [140/200], Step [200/453], Loss: 0.0376\n",
      "Epoch [140/200], Step [400/453], Loss: 0.0736\n",
      "Epoch [141/200], Step [200/453], Loss: 0.0332\n",
      "Epoch [141/200], Step [400/453], Loss: 0.0375\n",
      "Epoch [142/200], Step [200/453], Loss: 0.0991\n",
      "Epoch [142/200], Step [400/453], Loss: 0.0976\n",
      "Epoch [143/200], Step [200/453], Loss: 0.0241\n",
      "Epoch [143/200], Step [400/453], Loss: 0.0556\n",
      "Epoch [144/200], Step [200/453], Loss: 0.0381\n",
      "Epoch [144/200], Step [400/453], Loss: 0.0587\n",
      "Epoch [145/200], Step [200/453], Loss: 0.0760\n",
      "Epoch [145/200], Step [400/453], Loss: 0.0755\n",
      "Epoch [146/200], Step [200/453], Loss: 0.0165\n",
      "Epoch [146/200], Step [400/453], Loss: 0.0493\n",
      "Epoch [147/200], Step [200/453], Loss: 0.1625\n",
      "Epoch [147/200], Step [400/453], Loss: 0.0235\n",
      "Epoch [148/200], Step [200/453], Loss: 0.0388\n",
      "Epoch [148/200], Step [400/453], Loss: 0.0576\n",
      "Epoch [149/200], Step [200/453], Loss: 0.0451\n",
      "Epoch [149/200], Step [400/453], Loss: 0.0290\n",
      "Epoch [150/200], Step [200/453], Loss: 0.0458\n",
      "Epoch [150/200], Step [400/453], Loss: 0.0285\n",
      "Epoch [151/200], Step [200/453], Loss: 0.0630\n",
      "Epoch [151/200], Step [400/453], Loss: 0.0594\n",
      "Epoch [152/200], Step [200/453], Loss: 0.1004\n",
      "Epoch [152/200], Step [400/453], Loss: 0.0761\n",
      "Epoch [153/200], Step [200/453], Loss: 0.0700\n",
      "Epoch [153/200], Step [400/453], Loss: 0.0684\n",
      "Epoch [154/200], Step [200/453], Loss: 0.1627\n",
      "Epoch [154/200], Step [400/453], Loss: 0.1132\n",
      "Epoch [155/200], Step [200/453], Loss: 0.0736\n",
      "Epoch [155/200], Step [400/453], Loss: 0.0246\n",
      "Epoch [156/200], Step [200/453], Loss: 0.0264\n",
      "Epoch [156/200], Step [400/453], Loss: 0.0708\n",
      "Epoch [157/200], Step [200/453], Loss: 0.0301\n",
      "Epoch [157/200], Step [400/453], Loss: 0.0340\n",
      "Epoch [158/200], Step [200/453], Loss: 0.0332\n",
      "Epoch [158/200], Step [400/453], Loss: 0.1106\n",
      "Epoch [159/200], Step [200/453], Loss: 0.0304\n",
      "Epoch [159/200], Step [400/453], Loss: 0.0562\n",
      "Epoch [160/200], Step [200/453], Loss: 0.1222\n",
      "Epoch [160/200], Step [400/453], Loss: 0.0684\n",
      "Epoch [161/200], Step [200/453], Loss: 0.0187\n",
      "Epoch [161/200], Step [400/453], Loss: 0.0644\n",
      "Epoch [162/200], Step [200/453], Loss: 0.0391\n",
      "Epoch [162/200], Step [400/453], Loss: 0.0616\n",
      "Epoch [163/200], Step [200/453], Loss: 0.0357\n",
      "Epoch [163/200], Step [400/453], Loss: 0.0468\n",
      "Epoch [164/200], Step [200/453], Loss: 0.0149\n",
      "Epoch [164/200], Step [400/453], Loss: 0.0530\n",
      "Epoch [165/200], Step [200/453], Loss: 0.0618\n",
      "Epoch [165/200], Step [400/453], Loss: 0.0377\n",
      "Epoch [166/200], Step [200/453], Loss: 0.0635\n",
      "Epoch [166/200], Step [400/453], Loss: 0.0385\n",
      "Epoch [167/200], Step [200/453], Loss: 0.0228\n",
      "Epoch [167/200], Step [400/453], Loss: 0.0211\n",
      "Epoch [168/200], Step [200/453], Loss: 0.0376\n",
      "Epoch [168/200], Step [400/453], Loss: 0.0251\n",
      "Epoch [169/200], Step [200/453], Loss: 0.0534\n",
      "Epoch [169/200], Step [400/453], Loss: 0.0483\n",
      "Epoch [170/200], Step [200/453], Loss: 0.0674\n",
      "Epoch [170/200], Step [400/453], Loss: 0.0585\n",
      "Epoch [171/200], Step [200/453], Loss: 0.0195\n",
      "Epoch [171/200], Step [400/453], Loss: 0.0875\n",
      "Epoch [172/200], Step [200/453], Loss: 0.0185\n",
      "Epoch [172/200], Step [400/453], Loss: 0.0240\n",
      "Epoch [173/200], Step [200/453], Loss: 0.0376\n",
      "Epoch [173/200], Step [400/453], Loss: 0.0734\n",
      "Epoch [174/200], Step [200/453], Loss: 0.0920\n",
      "Epoch [174/200], Step [400/453], Loss: 0.0784\n",
      "Epoch [175/200], Step [200/453], Loss: 0.0226\n",
      "Epoch [175/200], Step [400/453], Loss: 0.0217\n",
      "Epoch [176/200], Step [200/453], Loss: 0.1239\n",
      "Epoch [176/200], Step [400/453], Loss: 0.0514\n",
      "Epoch [177/200], Step [200/453], Loss: 0.0184\n",
      "Epoch [177/200], Step [400/453], Loss: 0.0853\n",
      "Epoch [178/200], Step [200/453], Loss: 0.0755\n",
      "Epoch [178/200], Step [400/453], Loss: 0.0517\n",
      "Epoch [179/200], Step [200/453], Loss: 0.0224\n",
      "Epoch [179/200], Step [400/453], Loss: 0.0289\n",
      "Epoch [180/200], Step [200/453], Loss: 0.0843\n",
      "Epoch [180/200], Step [400/453], Loss: 0.0247\n",
      "Epoch [181/200], Step [200/453], Loss: 0.0733\n",
      "Epoch [181/200], Step [400/453], Loss: 0.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [182/200], Step [200/453], Loss: 0.0369\n",
      "Epoch [182/200], Step [400/453], Loss: 0.0179\n",
      "Epoch [183/200], Step [200/453], Loss: 0.0569\n",
      "Epoch [183/200], Step [400/453], Loss: 0.0811\n",
      "Epoch [184/200], Step [200/453], Loss: 0.0449\n",
      "Epoch [184/200], Step [400/453], Loss: 0.0263\n",
      "Epoch [185/200], Step [200/453], Loss: 0.0116\n",
      "Epoch [185/200], Step [400/453], Loss: 0.0287\n",
      "Epoch [186/200], Step [200/453], Loss: 0.1223\n",
      "Epoch [186/200], Step [400/453], Loss: 0.1260\n",
      "Epoch [187/200], Step [200/453], Loss: 0.0423\n",
      "Epoch [187/200], Step [400/453], Loss: 0.0091\n",
      "Epoch [188/200], Step [200/453], Loss: 0.0254\n",
      "Epoch [188/200], Step [400/453], Loss: 0.0518\n",
      "Epoch [189/200], Step [200/453], Loss: 0.0550\n",
      "Epoch [189/200], Step [400/453], Loss: 0.0384\n",
      "Epoch [190/200], Step [200/453], Loss: 0.0364\n",
      "Epoch [190/200], Step [400/453], Loss: 0.0597\n",
      "Epoch [191/200], Step [200/453], Loss: 0.0256\n",
      "Epoch [191/200], Step [400/453], Loss: 0.0147\n",
      "Epoch [192/200], Step [200/453], Loss: 0.0369\n",
      "Epoch [192/200], Step [400/453], Loss: 0.0134\n",
      "Epoch [193/200], Step [200/453], Loss: 0.0491\n",
      "Epoch [193/200], Step [400/453], Loss: 0.1231\n",
      "Epoch [194/200], Step [200/453], Loss: 0.0647\n",
      "Epoch [194/200], Step [400/453], Loss: 0.0124\n",
      "Epoch [195/200], Step [200/453], Loss: 0.0412\n",
      "Epoch [195/200], Step [400/453], Loss: 0.0904\n",
      "Epoch [196/200], Step [200/453], Loss: 0.0390\n",
      "Epoch [196/200], Step [400/453], Loss: 0.0423\n",
      "Epoch [197/200], Step [200/453], Loss: 0.0732\n",
      "Epoch [197/200], Step [400/453], Loss: 0.0582\n",
      "Epoch [198/200], Step [200/453], Loss: 0.0474\n",
      "Epoch [198/200], Step [400/453], Loss: 0.0299\n",
      "Epoch [199/200], Step [200/453], Loss: 0.0222\n",
      "Epoch [199/200], Step [400/453], Loss: 0.0491\n",
      "Epoch [200/200], Step [200/453], Loss: 0.0533\n",
      "Epoch [200/200], Step [400/453], Loss: 0.0360\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 200\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        X = sample_batched['X']\n",
    "        y = sample_batched['y']\n",
    "        # Move tensors to the configured device\n",
    "#         print(X)\n",
    "        embed = X.to(device)\n",
    "        labels = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embed)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backprpagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i_batch+1) % 200 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i_batch+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_dataset_test = ThermoDataset(X_test,y_test)\n",
    "test_loader = DataLoader(thermo_dataset_test, batch_size=100,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the  test for model_75 : 82.86901919165119 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In the test phase, don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        X = sample_batched['X'].to(device)\n",
    "        y = sample_batched['y'].to(device)\n",
    "        outputs = model(X)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print(predicted)\n",
    "#         print(y.size(0))\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the  test for model_75 : {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model_75.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model not using reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = thermo_sampled_embed_scaled\n",
    "y = thermo_sampled[\"is_thermophilic\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_dataset_train = ThermoDataset(X_train,y_train)\n",
    "train_loader = DataLoader(thermo_dataset_train, batch_size=100,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([100, 1280]) torch.Size([100])\n",
      "1 torch.Size([100, 1280]) torch.Size([100])\n",
      "2 torch.Size([100, 1280]) torch.Size([100])\n",
      "3 torch.Size([100, 1280]) torch.Size([100])\n",
      "4 torch.Size([100, 1280]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['X'].size(),\n",
    "          sample_batched['y'].size())\n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ThermoClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThermoClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(1280, 600)\n",
    "        self.fc2 = nn.Linear(600, 300)\n",
    "        self.fc3 = nn.Linear(300, 150)\n",
    "        self.fc4 = nn.Linear(150, 75)\n",
    "        self.fc5 = nn.Linear(75, 20)\n",
    "        self.fc6 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ThermoClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [200/453], Loss: 0.4224\n",
      "Epoch [1/200], Step [400/453], Loss: 0.3931\n",
      "Epoch [2/200], Step [200/453], Loss: 0.3599\n",
      "Epoch [2/200], Step [400/453], Loss: 0.3432\n",
      "Epoch [3/200], Step [200/453], Loss: 0.3850\n",
      "Epoch [3/200], Step [400/453], Loss: 0.3737\n",
      "Epoch [4/200], Step [200/453], Loss: 0.3569\n",
      "Epoch [4/200], Step [400/453], Loss: 0.3471\n",
      "Epoch [5/200], Step [200/453], Loss: 0.3272\n",
      "Epoch [5/200], Step [400/453], Loss: 0.4218\n",
      "Epoch [6/200], Step [200/453], Loss: 0.4622\n",
      "Epoch [6/200], Step [400/453], Loss: 0.3359\n",
      "Epoch [7/200], Step [200/453], Loss: 0.2659\n",
      "Epoch [7/200], Step [400/453], Loss: 0.3628\n",
      "Epoch [8/200], Step [200/453], Loss: 0.2549\n",
      "Epoch [8/200], Step [400/453], Loss: 0.2548\n",
      "Epoch [9/200], Step [200/453], Loss: 0.2514\n",
      "Epoch [9/200], Step [400/453], Loss: 0.3102\n",
      "Epoch [10/200], Step [200/453], Loss: 0.3142\n",
      "Epoch [10/200], Step [400/453], Loss: 0.3791\n",
      "Epoch [11/200], Step [200/453], Loss: 0.3997\n",
      "Epoch [11/200], Step [400/453], Loss: 0.2742\n",
      "Epoch [12/200], Step [200/453], Loss: 0.2471\n",
      "Epoch [12/200], Step [400/453], Loss: 0.2612\n",
      "Epoch [13/200], Step [200/453], Loss: 0.2574\n",
      "Epoch [13/200], Step [400/453], Loss: 0.1883\n",
      "Epoch [14/200], Step [200/453], Loss: 0.2183\n",
      "Epoch [14/200], Step [400/453], Loss: 0.2154\n",
      "Epoch [15/200], Step [200/453], Loss: 0.2363\n",
      "Epoch [15/200], Step [400/453], Loss: 0.2368\n",
      "Epoch [16/200], Step [200/453], Loss: 0.1880\n",
      "Epoch [16/200], Step [400/453], Loss: 0.1567\n",
      "Epoch [17/200], Step [200/453], Loss: 0.1511\n",
      "Epoch [17/200], Step [400/453], Loss: 0.2065\n",
      "Epoch [18/200], Step [200/453], Loss: 0.1548\n",
      "Epoch [18/200], Step [400/453], Loss: 0.2268\n",
      "Epoch [19/200], Step [200/453], Loss: 0.1128\n",
      "Epoch [19/200], Step [400/453], Loss: 0.1894\n",
      "Epoch [20/200], Step [200/453], Loss: 0.1442\n",
      "Epoch [20/200], Step [400/453], Loss: 0.1418\n",
      "Epoch [21/200], Step [200/453], Loss: 0.1600\n",
      "Epoch [21/200], Step [400/453], Loss: 0.1956\n",
      "Epoch [22/200], Step [200/453], Loss: 0.1150\n",
      "Epoch [22/200], Step [400/453], Loss: 0.1364\n",
      "Epoch [23/200], Step [200/453], Loss: 0.1026\n",
      "Epoch [23/200], Step [400/453], Loss: 0.1513\n",
      "Epoch [24/200], Step [200/453], Loss: 0.1652\n",
      "Epoch [24/200], Step [400/453], Loss: 0.1156\n",
      "Epoch [25/200], Step [200/453], Loss: 0.1368\n",
      "Epoch [25/200], Step [400/453], Loss: 0.1522\n",
      "Epoch [26/200], Step [200/453], Loss: 0.1747\n",
      "Epoch [26/200], Step [400/453], Loss: 0.1503\n",
      "Epoch [27/200], Step [200/453], Loss: 0.1237\n",
      "Epoch [27/200], Step [400/453], Loss: 0.1326\n",
      "Epoch [28/200], Step [200/453], Loss: 0.1122\n",
      "Epoch [28/200], Step [400/453], Loss: 0.1767\n",
      "Epoch [29/200], Step [200/453], Loss: 0.1280\n",
      "Epoch [29/200], Step [400/453], Loss: 0.1524\n",
      "Epoch [30/200], Step [200/453], Loss: 0.1201\n",
      "Epoch [30/200], Step [400/453], Loss: 0.1146\n",
      "Epoch [31/200], Step [200/453], Loss: 0.1342\n",
      "Epoch [31/200], Step [400/453], Loss: 0.1694\n",
      "Epoch [32/200], Step [200/453], Loss: 0.1342\n",
      "Epoch [32/200], Step [400/453], Loss: 0.1051\n",
      "Epoch [33/200], Step [200/453], Loss: 0.1335\n",
      "Epoch [33/200], Step [400/453], Loss: 0.2129\n",
      "Epoch [34/200], Step [200/453], Loss: 0.0885\n",
      "Epoch [34/200], Step [400/453], Loss: 0.1668\n",
      "Epoch [35/200], Step [200/453], Loss: 0.0431\n",
      "Epoch [35/200], Step [400/453], Loss: 0.0668\n",
      "Epoch [36/200], Step [200/453], Loss: 0.1204\n",
      "Epoch [36/200], Step [400/453], Loss: 0.1268\n",
      "Epoch [37/200], Step [200/453], Loss: 0.0524\n",
      "Epoch [37/200], Step [400/453], Loss: 0.0524\n",
      "Epoch [38/200], Step [200/453], Loss: 0.0813\n",
      "Epoch [38/200], Step [400/453], Loss: 0.1418\n",
      "Epoch [39/200], Step [200/453], Loss: 0.0499\n",
      "Epoch [39/200], Step [400/453], Loss: 0.0698\n",
      "Epoch [40/200], Step [200/453], Loss: 0.0893\n",
      "Epoch [40/200], Step [400/453], Loss: 0.0771\n",
      "Epoch [41/200], Step [200/453], Loss: 0.0379\n",
      "Epoch [41/200], Step [400/453], Loss: 0.0604\n",
      "Epoch [42/200], Step [200/453], Loss: 0.0338\n",
      "Epoch [42/200], Step [400/453], Loss: 0.1025\n",
      "Epoch [43/200], Step [200/453], Loss: 0.0616\n",
      "Epoch [43/200], Step [400/453], Loss: 0.0268\n",
      "Epoch [44/200], Step [200/453], Loss: 0.1417\n",
      "Epoch [44/200], Step [400/453], Loss: 0.0278\n",
      "Epoch [45/200], Step [200/453], Loss: 0.0677\n",
      "Epoch [45/200], Step [400/453], Loss: 0.0496\n",
      "Epoch [46/200], Step [200/453], Loss: 0.0785\n",
      "Epoch [46/200], Step [400/453], Loss: 0.0887\n",
      "Epoch [47/200], Step [200/453], Loss: 0.0599\n",
      "Epoch [47/200], Step [400/453], Loss: 0.0552\n",
      "Epoch [48/200], Step [200/453], Loss: 0.0732\n",
      "Epoch [48/200], Step [400/453], Loss: 0.0521\n",
      "Epoch [49/200], Step [200/453], Loss: 0.1053\n",
      "Epoch [49/200], Step [400/453], Loss: 0.0566\n",
      "Epoch [50/200], Step [200/453], Loss: 0.0285\n",
      "Epoch [50/200], Step [400/453], Loss: 0.1928\n",
      "Epoch [51/200], Step [200/453], Loss: 0.0819\n",
      "Epoch [51/200], Step [400/453], Loss: 0.0670\n",
      "Epoch [52/200], Step [200/453], Loss: 0.0734\n",
      "Epoch [52/200], Step [400/453], Loss: 0.0845\n",
      "Epoch [53/200], Step [200/453], Loss: 0.0386\n",
      "Epoch [53/200], Step [400/453], Loss: 0.0492\n",
      "Epoch [54/200], Step [200/453], Loss: 0.0332\n",
      "Epoch [54/200], Step [400/453], Loss: 0.0460\n",
      "Epoch [55/200], Step [200/453], Loss: 0.0678\n",
      "Epoch [55/200], Step [400/453], Loss: 0.0070\n",
      "Epoch [56/200], Step [200/453], Loss: 0.0573\n",
      "Epoch [56/200], Step [400/453], Loss: 0.0678\n",
      "Epoch [57/200], Step [200/453], Loss: 0.0866\n",
      "Epoch [57/200], Step [400/453], Loss: 0.0166\n",
      "Epoch [58/200], Step [200/453], Loss: 0.0464\n",
      "Epoch [58/200], Step [400/453], Loss: 0.1611\n",
      "Epoch [59/200], Step [200/453], Loss: 0.0355\n",
      "Epoch [59/200], Step [400/453], Loss: 0.0396\n",
      "Epoch [60/200], Step [200/453], Loss: 0.0712\n",
      "Epoch [60/200], Step [400/453], Loss: 0.0616\n",
      "Epoch [61/200], Step [200/453], Loss: 0.0903\n",
      "Epoch [61/200], Step [400/453], Loss: 0.0287\n",
      "Epoch [62/200], Step [200/453], Loss: 0.0348\n",
      "Epoch [62/200], Step [400/453], Loss: 0.0477\n",
      "Epoch [63/200], Step [200/453], Loss: 0.0281\n",
      "Epoch [63/200], Step [400/453], Loss: 0.0451\n",
      "Epoch [64/200], Step [200/453], Loss: 0.0281\n",
      "Epoch [64/200], Step [400/453], Loss: 0.0681\n",
      "Epoch [65/200], Step [200/453], Loss: 0.0292\n",
      "Epoch [65/200], Step [400/453], Loss: 0.0379\n",
      "Epoch [66/200], Step [200/453], Loss: 0.0360\n",
      "Epoch [66/200], Step [400/453], Loss: 0.0502\n",
      "Epoch [67/200], Step [200/453], Loss: 0.1075\n",
      "Epoch [67/200], Step [400/453], Loss: 0.0383\n",
      "Epoch [68/200], Step [200/453], Loss: 0.0442\n",
      "Epoch [68/200], Step [400/453], Loss: 0.0646\n",
      "Epoch [69/200], Step [200/453], Loss: 0.0182\n",
      "Epoch [69/200], Step [400/453], Loss: 0.0598\n",
      "Epoch [70/200], Step [200/453], Loss: 0.0060\n",
      "Epoch [70/200], Step [400/453], Loss: 0.0295\n",
      "Epoch [71/200], Step [200/453], Loss: 0.0403\n",
      "Epoch [71/200], Step [400/453], Loss: 0.0731\n",
      "Epoch [72/200], Step [200/453], Loss: 0.0219\n",
      "Epoch [72/200], Step [400/453], Loss: 0.0432\n",
      "Epoch [73/200], Step [200/453], Loss: 0.0293\n",
      "Epoch [73/200], Step [400/453], Loss: 0.0241\n",
      "Epoch [74/200], Step [200/453], Loss: 0.0152\n",
      "Epoch [74/200], Step [400/453], Loss: 0.0329\n",
      "Epoch [75/200], Step [200/453], Loss: 0.0381\n",
      "Epoch [75/200], Step [400/453], Loss: 0.0553\n",
      "Epoch [76/200], Step [200/453], Loss: 0.0378\n",
      "Epoch [76/200], Step [400/453], Loss: 0.0785\n",
      "Epoch [77/200], Step [200/453], Loss: 0.0469\n",
      "Epoch [77/200], Step [400/453], Loss: 0.0299\n",
      "Epoch [78/200], Step [200/453], Loss: 0.0202\n",
      "Epoch [78/200], Step [400/453], Loss: 0.0377\n",
      "Epoch [79/200], Step [200/453], Loss: 0.0350\n",
      "Epoch [79/200], Step [400/453], Loss: 0.0127\n",
      "Epoch [80/200], Step [200/453], Loss: 0.0725\n",
      "Epoch [80/200], Step [400/453], Loss: 0.0321\n",
      "Epoch [81/200], Step [200/453], Loss: 0.0168\n",
      "Epoch [81/200], Step [400/453], Loss: 0.0145\n",
      "Epoch [82/200], Step [200/453], Loss: 0.0628\n",
      "Epoch [82/200], Step [400/453], Loss: 0.0374\n",
      "Epoch [83/200], Step [200/453], Loss: 0.0087\n",
      "Epoch [83/200], Step [400/453], Loss: 0.0396\n",
      "Epoch [84/200], Step [200/453], Loss: 0.0730\n",
      "Epoch [84/200], Step [400/453], Loss: 0.0197\n",
      "Epoch [85/200], Step [200/453], Loss: 0.1266\n",
      "Epoch [85/200], Step [400/453], Loss: 0.0278\n",
      "Epoch [86/200], Step [200/453], Loss: 0.0330\n",
      "Epoch [86/200], Step [400/453], Loss: 0.0422\n",
      "Epoch [87/200], Step [200/453], Loss: 0.0097\n",
      "Epoch [87/200], Step [400/453], Loss: 0.0351\n",
      "Epoch [88/200], Step [200/453], Loss: 0.0458\n",
      "Epoch [88/200], Step [400/453], Loss: 0.0518\n",
      "Epoch [89/200], Step [200/453], Loss: 0.0768\n",
      "Epoch [89/200], Step [400/453], Loss: 0.0324\n",
      "Epoch [90/200], Step [200/453], Loss: 0.0122\n",
      "Epoch [90/200], Step [400/453], Loss: 0.0735\n",
      "Epoch [91/200], Step [200/453], Loss: 0.0162\n",
      "Epoch [91/200], Step [400/453], Loss: 0.0355\n",
      "Epoch [92/200], Step [200/453], Loss: 0.0195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/200], Step [400/453], Loss: 0.0429\n",
      "Epoch [93/200], Step [200/453], Loss: 0.0891\n",
      "Epoch [93/200], Step [400/453], Loss: 0.0745\n",
      "Epoch [94/200], Step [200/453], Loss: 0.0157\n",
      "Epoch [94/200], Step [400/453], Loss: 0.0440\n",
      "Epoch [95/200], Step [200/453], Loss: 0.0063\n",
      "Epoch [95/200], Step [400/453], Loss: 0.0604\n",
      "Epoch [96/200], Step [200/453], Loss: 0.0552\n",
      "Epoch [96/200], Step [400/453], Loss: 0.0260\n",
      "Epoch [97/200], Step [200/453], Loss: 0.0395\n",
      "Epoch [97/200], Step [400/453], Loss: 0.0491\n",
      "Epoch [98/200], Step [200/453], Loss: 0.0562\n",
      "Epoch [98/200], Step [400/453], Loss: 0.0085\n",
      "Epoch [99/200], Step [200/453], Loss: 0.0699\n",
      "Epoch [99/200], Step [400/453], Loss: 0.0190\n",
      "Epoch [100/200], Step [200/453], Loss: 0.0186\n",
      "Epoch [100/200], Step [400/453], Loss: 0.0267\n",
      "Epoch [101/200], Step [200/453], Loss: 0.0310\n",
      "Epoch [101/200], Step [400/453], Loss: 0.0178\n",
      "Epoch [102/200], Step [200/453], Loss: 0.0021\n",
      "Epoch [102/200], Step [400/453], Loss: 0.0390\n",
      "Epoch [103/200], Step [200/453], Loss: 0.0313\n",
      "Epoch [103/200], Step [400/453], Loss: 0.0132\n",
      "Epoch [104/200], Step [200/453], Loss: 0.0650\n",
      "Epoch [104/200], Step [400/453], Loss: 0.0370\n",
      "Epoch [105/200], Step [200/453], Loss: 0.0171\n",
      "Epoch [105/200], Step [400/453], Loss: 0.0591\n",
      "Epoch [106/200], Step [200/453], Loss: 0.0118\n",
      "Epoch [106/200], Step [400/453], Loss: 0.0290\n",
      "Epoch [107/200], Step [200/453], Loss: 0.0152\n",
      "Epoch [107/200], Step [400/453], Loss: 0.0086\n",
      "Epoch [108/200], Step [200/453], Loss: 0.0103\n",
      "Epoch [108/200], Step [400/453], Loss: 0.0466\n",
      "Epoch [109/200], Step [200/453], Loss: 0.0084\n",
      "Epoch [109/200], Step [400/453], Loss: 0.0254\n",
      "Epoch [110/200], Step [200/453], Loss: 0.0598\n",
      "Epoch [110/200], Step [400/453], Loss: 0.0361\n",
      "Epoch [111/200], Step [200/453], Loss: 0.0145\n",
      "Epoch [111/200], Step [400/453], Loss: 0.0153\n",
      "Epoch [112/200], Step [200/453], Loss: 0.0965\n",
      "Epoch [112/200], Step [400/453], Loss: 0.1018\n",
      "Epoch [113/200], Step [200/453], Loss: 0.0252\n",
      "Epoch [113/200], Step [400/453], Loss: 0.0050\n",
      "Epoch [114/200], Step [200/453], Loss: 0.0318\n",
      "Epoch [114/200], Step [400/453], Loss: 0.0251\n",
      "Epoch [115/200], Step [200/453], Loss: 0.0440\n",
      "Epoch [115/200], Step [400/453], Loss: 0.0145\n",
      "Epoch [116/200], Step [200/453], Loss: 0.1061\n",
      "Epoch [116/200], Step [400/453], Loss: 0.0594\n",
      "Epoch [117/200], Step [200/453], Loss: 0.0172\n",
      "Epoch [117/200], Step [400/453], Loss: 0.0469\n",
      "Epoch [118/200], Step [200/453], Loss: 0.0357\n",
      "Epoch [118/200], Step [400/453], Loss: 0.0094\n",
      "Epoch [119/200], Step [200/453], Loss: 0.0497\n",
      "Epoch [119/200], Step [400/453], Loss: 0.0509\n",
      "Epoch [120/200], Step [200/453], Loss: 0.0597\n",
      "Epoch [120/200], Step [400/453], Loss: 0.0094\n",
      "Epoch [121/200], Step [200/453], Loss: 0.0238\n",
      "Epoch [121/200], Step [400/453], Loss: 0.0195\n",
      "Epoch [122/200], Step [200/453], Loss: 0.0031\n",
      "Epoch [122/200], Step [400/453], Loss: 0.0219\n",
      "Epoch [123/200], Step [200/453], Loss: 0.0218\n",
      "Epoch [123/200], Step [400/453], Loss: 0.0263\n",
      "Epoch [124/200], Step [200/453], Loss: 0.0540\n",
      "Epoch [124/200], Step [400/453], Loss: 0.0223\n",
      "Epoch [125/200], Step [200/453], Loss: 0.0042\n",
      "Epoch [125/200], Step [400/453], Loss: 0.0950\n",
      "Epoch [126/200], Step [200/453], Loss: 0.0215\n",
      "Epoch [126/200], Step [400/453], Loss: 0.0115\n",
      "Epoch [127/200], Step [200/453], Loss: 0.0729\n",
      "Epoch [127/200], Step [400/453], Loss: 0.0052\n",
      "Epoch [128/200], Step [200/453], Loss: 0.0272\n",
      "Epoch [128/200], Step [400/453], Loss: 0.0442\n",
      "Epoch [129/200], Step [200/453], Loss: 0.0051\n",
      "Epoch [129/200], Step [400/453], Loss: 0.0521\n",
      "Epoch [130/200], Step [200/453], Loss: 0.0165\n",
      "Epoch [130/200], Step [400/453], Loss: 0.0219\n",
      "Epoch [131/200], Step [200/453], Loss: 0.0287\n",
      "Epoch [131/200], Step [400/453], Loss: 0.0123\n",
      "Epoch [132/200], Step [200/453], Loss: 0.0170\n",
      "Epoch [132/200], Step [400/453], Loss: 0.0118\n",
      "Epoch [133/200], Step [200/453], Loss: 0.0220\n",
      "Epoch [133/200], Step [400/453], Loss: 0.0194\n",
      "Epoch [134/200], Step [200/453], Loss: 0.0075\n",
      "Epoch [134/200], Step [400/453], Loss: 0.0322\n",
      "Epoch [135/200], Step [200/453], Loss: 0.0101\n",
      "Epoch [135/200], Step [400/453], Loss: 0.0271\n",
      "Epoch [136/200], Step [200/453], Loss: 0.0218\n",
      "Epoch [136/200], Step [400/453], Loss: 0.0115\n",
      "Epoch [137/200], Step [200/453], Loss: 0.0216\n",
      "Epoch [137/200], Step [400/453], Loss: 0.0251\n",
      "Epoch [138/200], Step [200/453], Loss: 0.0376\n",
      "Epoch [138/200], Step [400/453], Loss: 0.0126\n",
      "Epoch [139/200], Step [200/453], Loss: 0.0152\n",
      "Epoch [139/200], Step [400/453], Loss: 0.0171\n",
      "Epoch [140/200], Step [200/453], Loss: 0.0088\n",
      "Epoch [140/200], Step [400/453], Loss: 0.0069\n",
      "Epoch [141/200], Step [200/453], Loss: 0.0155\n",
      "Epoch [141/200], Step [400/453], Loss: 0.0044\n",
      "Epoch [142/200], Step [200/453], Loss: 0.0069\n",
      "Epoch [142/200], Step [400/453], Loss: 0.0344\n",
      "Epoch [143/200], Step [200/453], Loss: 0.0101\n",
      "Epoch [143/200], Step [400/453], Loss: 0.0172\n",
      "Epoch [144/200], Step [200/453], Loss: 0.0215\n",
      "Epoch [144/200], Step [400/453], Loss: 0.0095\n",
      "Epoch [145/200], Step [200/453], Loss: 0.0178\n",
      "Epoch [145/200], Step [400/453], Loss: 0.0229\n",
      "Epoch [146/200], Step [200/453], Loss: 0.0096\n",
      "Epoch [146/200], Step [400/453], Loss: 0.0070\n",
      "Epoch [147/200], Step [200/453], Loss: 0.0043\n",
      "Epoch [147/200], Step [400/453], Loss: 0.0282\n",
      "Epoch [148/200], Step [200/453], Loss: 0.0400\n",
      "Epoch [148/200], Step [400/453], Loss: 0.0104\n",
      "Epoch [149/200], Step [200/453], Loss: 0.0097\n",
      "Epoch [149/200], Step [400/453], Loss: 0.0056\n",
      "Epoch [150/200], Step [200/453], Loss: 0.0071\n",
      "Epoch [150/200], Step [400/453], Loss: 0.0121\n",
      "Epoch [151/200], Step [200/453], Loss: 0.0305\n",
      "Epoch [151/200], Step [400/453], Loss: 0.0034\n",
      "Epoch [152/200], Step [200/453], Loss: 0.0360\n",
      "Epoch [152/200], Step [400/453], Loss: 0.0291\n",
      "Epoch [153/200], Step [200/453], Loss: 0.0356\n",
      "Epoch [153/200], Step [400/453], Loss: 0.0047\n",
      "Epoch [154/200], Step [200/453], Loss: 0.0092\n",
      "Epoch [154/200], Step [400/453], Loss: 0.0028\n",
      "Epoch [155/200], Step [200/453], Loss: 0.0051\n",
      "Epoch [155/200], Step [400/453], Loss: 0.0265\n",
      "Epoch [156/200], Step [200/453], Loss: 0.0055\n",
      "Epoch [156/200], Step [400/453], Loss: 0.0205\n",
      "Epoch [157/200], Step [200/453], Loss: 0.0007\n",
      "Epoch [157/200], Step [400/453], Loss: 0.0595\n",
      "Epoch [158/200], Step [200/453], Loss: 0.0106\n",
      "Epoch [158/200], Step [400/453], Loss: 0.0186\n",
      "Epoch [159/200], Step [200/453], Loss: 0.0037\n",
      "Epoch [159/200], Step [400/453], Loss: 0.0048\n",
      "Epoch [160/200], Step [200/453], Loss: 0.0036\n",
      "Epoch [160/200], Step [400/453], Loss: 0.0201\n",
      "Epoch [161/200], Step [200/453], Loss: 0.0009\n",
      "Epoch [161/200], Step [400/453], Loss: 0.0474\n",
      "Epoch [162/200], Step [200/453], Loss: 0.0087\n",
      "Epoch [162/200], Step [400/453], Loss: 0.0032\n",
      "Epoch [163/200], Step [200/453], Loss: 0.0322\n",
      "Epoch [163/200], Step [400/453], Loss: 0.0077\n",
      "Epoch [164/200], Step [200/453], Loss: 0.0017\n",
      "Epoch [164/200], Step [400/453], Loss: 0.0153\n",
      "Epoch [165/200], Step [200/453], Loss: 0.0211\n",
      "Epoch [165/200], Step [400/453], Loss: 0.0098\n",
      "Epoch [166/200], Step [200/453], Loss: 0.0172\n",
      "Epoch [166/200], Step [400/453], Loss: 0.0037\n",
      "Epoch [167/200], Step [200/453], Loss: 0.0755\n",
      "Epoch [167/200], Step [400/453], Loss: 0.0272\n",
      "Epoch [168/200], Step [200/453], Loss: 0.0327\n",
      "Epoch [168/200], Step [400/453], Loss: 0.0229\n",
      "Epoch [169/200], Step [200/453], Loss: 0.0387\n",
      "Epoch [169/200], Step [400/453], Loss: 0.0138\n",
      "Epoch [170/200], Step [200/453], Loss: 0.0376\n",
      "Epoch [170/200], Step [400/453], Loss: 0.0459\n",
      "Epoch [171/200], Step [200/453], Loss: 0.0262\n",
      "Epoch [171/200], Step [400/453], Loss: 0.0066\n",
      "Epoch [172/200], Step [200/453], Loss: 0.0074\n",
      "Epoch [172/200], Step [400/453], Loss: 0.0149\n",
      "Epoch [173/200], Step [200/453], Loss: 0.0465\n",
      "Epoch [173/200], Step [400/453], Loss: 0.0770\n",
      "Epoch [174/200], Step [200/453], Loss: 0.0199\n",
      "Epoch [174/200], Step [400/453], Loss: 0.0012\n",
      "Epoch [175/200], Step [200/453], Loss: 0.0007\n",
      "Epoch [175/200], Step [400/453], Loss: 0.0217\n",
      "Epoch [176/200], Step [200/453], Loss: 0.0148\n",
      "Epoch [176/200], Step [400/453], Loss: 0.1034\n",
      "Epoch [177/200], Step [200/453], Loss: 0.0103\n",
      "Epoch [177/200], Step [400/453], Loss: 0.0131\n",
      "Epoch [178/200], Step [200/453], Loss: 0.0924\n",
      "Epoch [178/200], Step [400/453], Loss: 0.0043\n",
      "Epoch [179/200], Step [200/453], Loss: 0.0195\n",
      "Epoch [179/200], Step [400/453], Loss: 0.0113\n",
      "Epoch [180/200], Step [200/453], Loss: 0.0021\n",
      "Epoch [180/200], Step [400/453], Loss: 0.0244\n",
      "Epoch [181/200], Step [200/453], Loss: 0.0054\n",
      "Epoch [181/200], Step [400/453], Loss: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [182/200], Step [200/453], Loss: 0.0118\n",
      "Epoch [182/200], Step [400/453], Loss: 0.0072\n",
      "Epoch [183/200], Step [200/453], Loss: 0.0027\n",
      "Epoch [183/200], Step [400/453], Loss: 0.0125\n",
      "Epoch [184/200], Step [200/453], Loss: 0.0165\n",
      "Epoch [184/200], Step [400/453], Loss: 0.0124\n",
      "Epoch [185/200], Step [200/453], Loss: 0.0057\n",
      "Epoch [185/200], Step [400/453], Loss: 0.0089\n",
      "Epoch [186/200], Step [200/453], Loss: 0.0738\n",
      "Epoch [186/200], Step [400/453], Loss: 0.0081\n",
      "Epoch [187/200], Step [200/453], Loss: 0.0144\n",
      "Epoch [187/200], Step [400/453], Loss: 0.0025\n",
      "Epoch [188/200], Step [200/453], Loss: 0.0298\n",
      "Epoch [188/200], Step [400/453], Loss: 0.0373\n",
      "Epoch [189/200], Step [200/453], Loss: 0.0120\n",
      "Epoch [189/200], Step [400/453], Loss: 0.0104\n",
      "Epoch [190/200], Step [200/453], Loss: 0.0032\n",
      "Epoch [190/200], Step [400/453], Loss: 0.0020\n",
      "Epoch [191/200], Step [200/453], Loss: 0.0060\n",
      "Epoch [191/200], Step [400/453], Loss: 0.0023\n",
      "Epoch [192/200], Step [200/453], Loss: 0.0259\n",
      "Epoch [192/200], Step [400/453], Loss: 0.0060\n",
      "Epoch [193/200], Step [200/453], Loss: 0.0006\n",
      "Epoch [193/200], Step [400/453], Loss: 0.0028\n",
      "Epoch [194/200], Step [200/453], Loss: 0.0179\n",
      "Epoch [194/200], Step [400/453], Loss: 0.0002\n",
      "Epoch [195/200], Step [200/453], Loss: 0.0151\n",
      "Epoch [195/200], Step [400/453], Loss: 0.0179\n",
      "Epoch [196/200], Step [200/453], Loss: 0.0277\n",
      "Epoch [196/200], Step [400/453], Loss: 0.0030\n",
      "Epoch [197/200], Step [200/453], Loss: 0.0451\n",
      "Epoch [197/200], Step [400/453], Loss: 0.0248\n",
      "Epoch [198/200], Step [200/453], Loss: 0.0172\n",
      "Epoch [198/200], Step [400/453], Loss: 0.0122\n",
      "Epoch [199/200], Step [200/453], Loss: 0.0075\n",
      "Epoch [199/200], Step [400/453], Loss: 0.0423\n",
      "Epoch [200/200], Step [200/453], Loss: 0.0146\n",
      "Epoch [200/200], Step [400/453], Loss: 0.0269\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 200\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        X = sample_batched['X']\n",
    "        y = sample_batched['y']\n",
    "        # Move tensors to the configured device\n",
    "#         print(X)\n",
    "        embed = X.to(device)\n",
    "        labels = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embed)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backprpagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i_batch+1) % 200 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i_batch+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_dataset_test = ThermoDataset(X_test,y_test)\n",
    "test_loader = DataLoader(thermo_dataset_test, batch_size=100,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the  test for model_768 : 85.69912443618998 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In the test phase, don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        X = sample_batched['X'].to(device)\n",
    "        y = sample_batched['y'].to(device)\n",
    "        outputs = model(X)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print(predicted)\n",
    "#         print(y.size(0))\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the  test for model_768 : {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model_768.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
